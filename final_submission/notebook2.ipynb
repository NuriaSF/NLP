{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:09.447952Z",
     "start_time": "2020-03-14T14:45:09.441971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from CountVectorizer_BagOfWords import CountVectorizer as cv\n",
    "from TfIdfVectorizer import TfIdfVectorizer as tf\n",
    "#from Spelling_Correction_c  import Spelling_Correction_c \n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import json\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to solve the following problem: given a pair of different questions of Quora, decide if they are asking the same or not. In this notebook, we will discuss the process we have followed to solve the problem, the different models that we have used as well as the mistakes that each model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i get home tutor</td>\n",
       "      <td>how can i trust a home tutor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what be the difference between have be have be...</td>\n",
       "      <td>when should i use have be have be and have be</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if my atm card be block for online transaction...</td>\n",
       "      <td>my credit card be use for fraud transaction i ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how do i add usb 3.0 port in a laptop without ...</td>\n",
       "      <td>can i use a usb 3.0 device in a usb 2.0 port</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what be the best advantage of use Quora</td>\n",
       "      <td>what be the benefit to Quora</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where can i download the economist pdf</td>\n",
       "      <td>where can i download pdf of Gillian Glynn go girl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why do some people get everything</td>\n",
       "      <td>why be that some people get what they want ver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>be an all out nuclear war survivable</td>\n",
       "      <td>would all out nuclear war destroy all life on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what be the advantage of ashless transaction</td>\n",
       "      <td>what could happen to ashless transaction after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what be the main problem of India</td>\n",
       "      <td>what be the main problem face by India</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>what be good idea to help fall asleep quickly</td>\n",
       "      <td>what be some way to fall asleep faster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>what be the best place to visit in or near jun...</td>\n",
       "      <td>what be the best place to visit in Alaska and why</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what be physical mean of divergence</td>\n",
       "      <td>what be the physical mean of divergence curl a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what be the best phone to buy below 6k</td>\n",
       "      <td>what be the best phone to buy below 15k</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>how do people become beggar</td>\n",
       "      <td>can beg be drop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>who can help me improve my Anglish</td>\n",
       "      <td>who can help me in improve my Anglish speak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>after japan take it goal in ww2 pacific camapi...</td>\n",
       "      <td>what be an excusator</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>accord to you which be the best episode of com...</td>\n",
       "      <td>which be one of the best episode of comedy nig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>what be expect cutoff for 2016 ibps clerk prelim</td>\n",
       "      <td>what will be the expect cutoff of ibps clerk 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>why some macbooks keyboard have key with sign ...</td>\n",
       "      <td>what be cad command prompt i want to know it u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what be the strangest thing that ever happen t...</td>\n",
       "      <td>what be the strangest thing that have ever hap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>can i install a tablet in my car</td>\n",
       "      <td>how do you install a tablet into a car</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>why be manaphy a whiny pokemon</td>\n",
       "      <td>why be manaphy so whiny throughout the Rok mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>where do you sell forestry product</td>\n",
       "      <td>how do you sell forestry product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>can i hack any phone by just have his phone nu...</td>\n",
       "      <td>can someone hack into your phone just by know ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>how much wood would a woodchuck chuck if a woo...</td>\n",
       "      <td>why can not woodchuck chuck wood</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>have anyone successfully use the no contact ru...</td>\n",
       "      <td>do ex get back in contact</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>be deep learn overhyped</td>\n",
       "      <td>be machine learn overrate or overhyped</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>how can we find professor pad master student f...</td>\n",
       "      <td>how can we find professor pad master student f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>after demonetization of note government introd...</td>\n",
       "      <td>what be the utility behind remove r 1000 note ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291058</th>\n",
       "      <td>why do European drink everything at room tempe...</td>\n",
       "      <td>can i store keg of beer at room temperature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291059</th>\n",
       "      <td>who be some famous people who annoy you for so...</td>\n",
       "      <td>if harry potter parang be half blood how come ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291060</th>\n",
       "      <td>i be a u citizen can i take a gift from my bro...</td>\n",
       "      <td>sell property in India and pay tds in India do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291061</th>\n",
       "      <td>why do chrome struggle with wsj.com</td>\n",
       "      <td>how do i install goggle chrome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291062</th>\n",
       "      <td>do American regret their atomic attack against...</td>\n",
       "      <td>what be your biggest regret if you dian not ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291063</th>\n",
       "      <td>how can i make a home make pocket pussy which ...</td>\n",
       "      <td>how do it feel when a penis enter a vagina fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291064</th>\n",
       "      <td>how do you deal with your fear of go to the gy...</td>\n",
       "      <td>how do i get over my fear and resistance of go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291065</th>\n",
       "      <td>how can i become a business manei just love to...</td>\n",
       "      <td>i have a good idea but i don not know the tech...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291066</th>\n",
       "      <td>my mobile be steal be it possible for the thie...</td>\n",
       "      <td>i have lose my mobile how do i find my imei nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291067</th>\n",
       "      <td>how should i prepare for p block in board and ...</td>\n",
       "      <td>how do i prepare chemistry for board in one month</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291068</th>\n",
       "      <td>science be evolutionary theory falsifiable</td>\n",
       "      <td>be the theory of evolution unfalsifiable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291069</th>\n",
       "      <td>be we near world war 3</td>\n",
       "      <td>be world war 3 likely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291070</th>\n",
       "      <td>can anyone convert to Judaism</td>\n",
       "      <td>why do you convert to Judaism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291071</th>\n",
       "      <td>i always feel sleepy in my lecture what can i do</td>\n",
       "      <td>i always feel sleepy and lose in my own world....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291072</th>\n",
       "      <td>why be someone i block on snapchat on my add y...</td>\n",
       "      <td>if you unblock someone one snapchat will you r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291073</th>\n",
       "      <td>what be the best way to to make money</td>\n",
       "      <td>what the best way to make fast cash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291074</th>\n",
       "      <td>what be the reason behind the sudden end of th...</td>\n",
       "      <td>what can you do when your favorite tv show end</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291075</th>\n",
       "      <td>if the whole world have to speak one language ...</td>\n",
       "      <td>if the world have to speak one language what w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291076</th>\n",
       "      <td>when can i expect the merit list of te army co...</td>\n",
       "      <td>when will the merit list of te 36 be declare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291077</th>\n",
       "      <td>which be a suitable inpatient drug and alcohol...</td>\n",
       "      <td>which be a suitable inpatient drug and alcohol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291078</th>\n",
       "      <td>what be some font awesome icon that represent ...</td>\n",
       "      <td>what be some awesome argument make in house of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291079</th>\n",
       "      <td>i lose my original charger of the oneplus one ...</td>\n",
       "      <td>i have a Doto g Ind gen usually in the day i d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291080</th>\n",
       "      <td>how be football fan from pain react to la biga...</td>\n",
       "      <td>interrupt application in power electronics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291081</th>\n",
       "      <td>why be people with authenticity conscience kin...</td>\n",
       "      <td>why be people selfish towards people who be ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291082</th>\n",
       "      <td>be want to start you own tech startup and want...</td>\n",
       "      <td>be anybody in bandalore near marathalli look f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291083</th>\n",
       "      <td>which be the best Lapp for learn yoga</td>\n",
       "      <td>what be the best Lapp to learn yoga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291084</th>\n",
       "      <td>what the main reason behind 500 1000 r note ge...</td>\n",
       "      <td>what be the reason behind pm mode decision to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291085</th>\n",
       "      <td>how can i find out my driver license number us...</td>\n",
       "      <td>what the best way to get my driver s license n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291086</th>\n",
       "      <td>how have bill gate charity foundation help the...</td>\n",
       "      <td>how have bill gate affect u and the world</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291087</th>\n",
       "      <td>why do the un and almost all the country consi...</td>\n",
       "      <td>what be the best underground scene in Bel avid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291088 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "0                                 how do i get home tutor   \n",
       "1       what be the difference between have be have be...   \n",
       "2       if my atm card be block for online transaction...   \n",
       "3       how do i add usb 3.0 port in a laptop without ...   \n",
       "4                 what be the best advantage of use Quora   \n",
       "5                  where can i download the economist pdf   \n",
       "6                       why do some people get everything   \n",
       "7                    be an all out nuclear war survivable   \n",
       "8            what be the advantage of ashless transaction   \n",
       "9                       what be the main problem of India   \n",
       "10          what be good idea to help fall asleep quickly   \n",
       "11      what be the best place to visit in or near jun...   \n",
       "12                    what be physical mean of divergence   \n",
       "13                 what be the best phone to buy below 6k   \n",
       "14                            how do people become beggar   \n",
       "15                     who can help me improve my Anglish   \n",
       "16      after japan take it goal in ww2 pacific camapi...   \n",
       "17      accord to you which be the best episode of com...   \n",
       "18       what be expect cutoff for 2016 ibps clerk prelim   \n",
       "19      why some macbooks keyboard have key with sign ...   \n",
       "20      what be the strangest thing that ever happen t...   \n",
       "21                       can i install a tablet in my car   \n",
       "22                         why be manaphy a whiny pokemon   \n",
       "23                     where do you sell forestry product   \n",
       "24      can i hack any phone by just have his phone nu...   \n",
       "25      how much wood would a woodchuck chuck if a woo...   \n",
       "26      have anyone successfully use the no contact ru...   \n",
       "27                                be deep learn overhyped   \n",
       "28      how can we find professor pad master student f...   \n",
       "29      after demonetization of note government introd...   \n",
       "...                                                   ...   \n",
       "291058  why do European drink everything at room tempe...   \n",
       "291059  who be some famous people who annoy you for so...   \n",
       "291060  i be a u citizen can i take a gift from my bro...   \n",
       "291061                why do chrome struggle with wsj.com   \n",
       "291062  do American regret their atomic attack against...   \n",
       "291063  how can i make a home make pocket pussy which ...   \n",
       "291064  how do you deal with your fear of go to the gy...   \n",
       "291065  how can i become a business manei just love to...   \n",
       "291066  my mobile be steal be it possible for the thie...   \n",
       "291067  how should i prepare for p block in board and ...   \n",
       "291068         science be evolutionary theory falsifiable   \n",
       "291069                             be we near world war 3   \n",
       "291070                      can anyone convert to Judaism   \n",
       "291071   i always feel sleepy in my lecture what can i do   \n",
       "291072  why be someone i block on snapchat on my add y...   \n",
       "291073              what be the best way to to make money   \n",
       "291074  what be the reason behind the sudden end of th...   \n",
       "291075  if the whole world have to speak one language ...   \n",
       "291076  when can i expect the merit list of te army co...   \n",
       "291077  which be a suitable inpatient drug and alcohol...   \n",
       "291078  what be some font awesome icon that represent ...   \n",
       "291079  i lose my original charger of the oneplus one ...   \n",
       "291080  how be football fan from pain react to la biga...   \n",
       "291081  why be people with authenticity conscience kin...   \n",
       "291082  be want to start you own tech startup and want...   \n",
       "291083              which be the best Lapp for learn yoga   \n",
       "291084  what the main reason behind 500 1000 r note ge...   \n",
       "291085  how can i find out my driver license number us...   \n",
       "291086  how have bill gate charity foundation help the...   \n",
       "291087  why do the un and almost all the country consi...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0                            how can i trust a home tutor             0  \n",
       "1           when should i use have be have be and have be             0  \n",
       "2       my credit card be use for fraud transaction i ...             0  \n",
       "3            can i use a usb 3.0 device in a usb 2.0 port             0  \n",
       "4                            what be the benefit to Quora             1  \n",
       "5       where can i download pdf of Gillian Glynn go girl             0  \n",
       "6       why be that some people get what they want ver...             1  \n",
       "7       would all out nuclear war destroy all life on ...             1  \n",
       "8       what could happen to ashless transaction after...             0  \n",
       "9                  what be the main problem face by India             1  \n",
       "10                 what be some way to fall asleep faster             1  \n",
       "11      what be the best place to visit in Alaska and why             0  \n",
       "12      what be the physical mean of divergence curl a...             1  \n",
       "13                what be the best phone to buy below 15k             0  \n",
       "14                                        can beg be drop             0  \n",
       "15            who can help me in improve my Anglish speak             1  \n",
       "16                                   what be an excusator             0  \n",
       "17      which be one of the best episode of comedy nig...             1  \n",
       "18      what will be the expect cutoff of ibps clerk 2016             0  \n",
       "19      what be cad command prompt i want to know it u...             0  \n",
       "20      what be the strangest thing that have ever hap...             1  \n",
       "21                 how do you install a tablet into a car             1  \n",
       "22      why be manaphy so whiny throughout the Rok mon...             1  \n",
       "23                       how do you sell forestry product             0  \n",
       "24      can someone hack into your phone just by know ...             1  \n",
       "25                       why can not woodchuck chuck wood             0  \n",
       "26                              do ex get back in contact             0  \n",
       "27                 be machine learn overrate or overhyped             0  \n",
       "28      how can we find professor pad master student f...             0  \n",
       "29      what be the utility behind remove r 1000 note ...             1  \n",
       "...                                                   ...           ...  \n",
       "291058        can i store keg of beer at room temperature             0  \n",
       "291059  if harry potter parang be half blood how come ...             0  \n",
       "291060  sell property in India and pay tds in India do...             0  \n",
       "291061                     how do i install goggle chrome             0  \n",
       "291062  what be your biggest regret if you dian not ge...             0  \n",
       "291063  how do it feel when a penis enter a vagina fro...             0  \n",
       "291064  how do i get over my fear and resistance of go...             1  \n",
       "291065  i have a good idea but i don not know the tech...             0  \n",
       "291066  i have lose my mobile how do i find my imei nu...             0  \n",
       "291067  how do i prepare chemistry for board in one month             0  \n",
       "291068           be the theory of evolution unfalsifiable             0  \n",
       "291069                              be world war 3 likely             1  \n",
       "291070                      why do you convert to Judaism             0  \n",
       "291071  i always feel sleepy and lose in my own world....             1  \n",
       "291072  if you unblock someone one snapchat will you r...             0  \n",
       "291073                what the best way to make fast cash             1  \n",
       "291074     what can you do when your favorite tv show end             0  \n",
       "291075  if the world have to speak one language what w...             1  \n",
       "291076       when will the merit list of te 36 be declare             1  \n",
       "291077  which be a suitable inpatient drug and alcohol...             0  \n",
       "291078  what be some awesome argument make in house of...             0  \n",
       "291079  i have a Doto g Ind gen usually in the day i d...             0  \n",
       "291080         interrupt application in power electronics             0  \n",
       "291081  why be people selfish towards people who be ki...             0  \n",
       "291082  be anybody in bandalore near marathalli look f...             0  \n",
       "291083                what be the best Lapp to learn yoga             1  \n",
       "291084  what be the reason behind pm mode decision to ...             1  \n",
       "291085  what the best way to get my driver s license n...             0  \n",
       "291086          how have bill gate affect u and the world             0  \n",
       "291087  what be the best underground scene in Bel avid...             0  \n",
       "\n",
       "[291088 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "train_df =pd.read_csv('train_df.csv')\n",
    "test_df =pd.read_csv('test_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions for some of the models.The first functions are meant to extract, given a vectorizer, the matrix of features for the classifier. The two last functions are used to identify the errors that a classifier is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_list(q1,q2,count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1)\n",
    "    q2_mat = count_vectorizer.transform(q2)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "    \n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "\n",
    "def get_mistakes(clf, X_q1q2, y):\n",
    "    \"\"\"\n",
    "    Returns two lists: one containing the indices of the predictions that are not correct\n",
    "    and another one containing the predictions\n",
    "    \"\"\"\n",
    "    predictions        = clf.predict(X_q1q2).round(0).astype(int)\n",
    "    incorrect_preds    = predictions != y\n",
    "    incorrect_indices, = np.where(incorrect_preds)\n",
    "    incorrect_indices2 = [x for x in  range(len(incorrect_preds)) if incorrect_preds[x] ==True]\n",
    "    incorrect_indices3 = np.arange(len(incorrect_preds))[incorrect_preds]        \n",
    "    \n",
    "    if np.sum(incorrect_preds)==0:\n",
    "        print(\"no mistakes in this df\")\n",
    "    else:\n",
    "        return incorrect_indices, predictions\n",
    "    \n",
    "def print_mistake_k(k, dataset, mistake_indices, predictions):\n",
    "    \"\"\"\n",
    "    Auxiliar function to print the k-th mistake made in the prediction\n",
    "    \"\"\"\n",
    "    print(\"Original q1: \", train_df.iloc[mistake_indices[k]].question1, \" Treated q1: \", dataset[mistake_indices[k]])\n",
    "    print(\"Original q2: \", train_df.iloc[mistake_indices[k]].question2, \" Treated q2: \", dataset[mistake_indices[k]+train_df.shape[0]])\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])  \n",
    "    \n",
    "    \n",
    "def load_logistic(filename):\n",
    "    # Load the logistic parameters\n",
    "    with open(filename, 'rb') as fp:\n",
    "        logistic_params = json.load(fp)\n",
    "        \n",
    "    #Create logistic object\n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "    logistic.coef_ = np.array(logistic_params['coef_'])\n",
    "    logistic.classes_ = np.array(logistic_params['classes_'])\n",
    "    logistic.intercept_ = np.array(logistic_params['intercept_'])\n",
    "    \n",
    "    return logistic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first naive model was proposed in class: pass the text through the vectorizers and use the returned matrix as the matrix of features. We saw that the classifier wrongly classified some questions with spelling mistakes. For example, the classifiera would identify as different questions those who were written like \"whats\" from those who were written like \"what's\". \n",
    "\n",
    "We thought that this problem may be common with any model that we try to train, so the first thing we propose to do is correcting the spelling mistakes. We propose to remove \"'s\", change the negatives \"'t\" for \"not\" as well as the plurals \"'re\" for \"are\", remove symbols and points. Then, we implemented a spell checking function using the edit distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells that transform the raw questions into cleaned questions can be found in notebook 1. Since the computations are very long, we load the already cleaned questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all elements of the documents into strings \n",
    "q1_train_raw =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train_raw =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_val_raw  =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
    "q2_val_raw  =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
    "q1_test_raw  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test_raw  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "\n",
    "all_questions_raw = q1_train_raw + q2_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions= q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/train_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "train_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/val_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "val_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/test_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "test_labels = [int(x.strip()) for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST MODEL: NAIVE CLASSIFIER WITH SPELL CHECKING\n",
    "For the first model, we just wanted to see what difference did the spellchecking do. So, did we improve the results? Did we improve the results as expected? If so, what mistakes is our model doing now?\n",
    "\n",
    "We will do this checking for both the CountVectorizer and the TfIdfVectorizer. The classifier will be a sklearn Logistic Regression. This is a simple classifier that will serve as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:21.407870Z",
     "start_time": "2020-03-14T14:45:21.401885Z"
    }
   },
   "outputs": [],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:46:44.793780Z",
     "start_time": "2020-03-14T14:45:42.528916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)\", document_cleaner_func=None,\n",
       "        dtype=<class 'numpy.float32'>, max_df=0.99, min_df=5,\n",
       "        min_word_counts=1, ngram_range=(1, 3),\n",
       "        stop_words={\"won't\", 'for', 'but', 'over', 'aren', \"aren't\", 'how', 'between', \"needn't\", 'other', 'has', 'under', 'or', 'all', 'our', \"didn't\", 'its', 'ma', 'hasn', 'through', 'during', 'a', 'll', 'yourself', 'up', \"wouldn't\", 'shan', 'his', 'after', \"mightn't\", 'above', 'as', 'why', 'wasn', \"hadn'...', 'about', 'the', 'any', 'both', 'very', 'were', 'against', 'your', 'itself', 'my', 'them', 'that'},\n",
       "        token_cleaner_func=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer_func=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.load(\"models/CountVectorizer.pkl\")\n",
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words={'for', \"won't\", 'but', 'over', 'aren', \"aren't\", 'between', 'how', \"needn't\", 'other', 'has', 'or', 'under', 'all', 'our', \"didn't\", 'its', 'ma', 'hasn', 'through', 'during', 'a', 'll', 'yourself', 'up', 'shan', \"wouldn't\", 'his', 'after', \"mightn't\", 'above', 'as', 'why', 'wasn', \"hadn'...', 'about', 'the', 'any', 'both', 'very', 'were', 'against', 'your', 'itself', 'my', 'them', 'that'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the result obtained with our implementation of the CountVectorizer with the result obtained using the sklearn version of the CountVectorizer. One of the objectives of this deliverable was to implement and understend how the vectorizers worked, so we set as an objective to obtain the same as the sklearn vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "\n",
    "X_tr_q1q2_sk = get_features_from_list(q1_train, q2_train,countvect_sk)\n",
    "\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, CountVectorizer)\n",
    "\n",
    "X_val_q1q2_sk  = get_features_from_list(q1_val, q2_val, countvect_sk)\n",
    "\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)\n",
    "\n",
    "X_te_q1q2_sk  = get_features_from_list(q1_test, q2_test, countvect_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the logistic regression using our implementation of CountVectorizer. We load the logistic regression model from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873892706438838\n",
      "Result on validation:  0.7564062563763118\n",
      "Result on test:  0.7533668105876712\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873667596649119\n",
      "Result on validation:  0.7563817405636126\n",
      "Result on test:  0.7533304796390545\n"
     ]
    }
   ],
   "source": [
    "logistic_sk = load_logistic('models/logistic_sk.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "We wanted to identify the mistakes that the classifier was doing in this case. We saw that the classifier was making mistages mainly for the following reasons:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.9074094431924367\n",
      "Accuracy on validation:  0.788647044274054\n",
      "Accuracy on test:  0.7851665883400529\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels==logistic.predict(X_tr_q1q2))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  why do men like woman foot  Treated q1:  why do men like woman foot\n",
      "Original q2:  why do men like woman foot  Treated q2:  why do men like woman foot\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "        stop_words={'for', \"won't\", 'but', 'over', 'aren', \"aren't\", 'between', 'how', \"needn't\", 'other', 'has', 'or', 'under', 'all', 'our', \"didn't\", 'its', 'ma', 'hasn', 'through', 'during', 'a', 'll', 'yourself', 'up', 'shan', \"wouldn't\", 'his', 'after', \"mightn't\", 'above', 'as', 'why', 'wasn', \"hadn'...', 'about', 'the', 'any', 'both', 'very', 'were', 'against', 'your', 'itself', 'my', 'them', 'that'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=False,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tf()\n",
    "tfidf_vectorizer.load(\"models/TfIdfVectorizer.pkl\")\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that additionally, we want to compare our result with that given by the implementation of sklearn of the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our TfIdf Vectorizer: (291088, 285364) (291088, 3)\n",
      "With sklearn TfIdf Vectorizer: (291088, 285364) (291088, 3)\n"
     ]
    }
   ],
   "source": [
    "X_tr_q1q2_tfidf = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_tr_q1q2_sk_tfidf = get_features_from_list(q1_train, q2_train, tfidf_sk)\n",
    "X_val_q1q2_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_vectorizer)\n",
    "X_val_q1q2_sk_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_sk)\n",
    "X_te_q1q2_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)\n",
    "X_te_q1q2_sk_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_sk)\n",
    "\n",
    "print(\"With our TfIdf Vectorizer:\", X_tr_q1q2_tfidf.shape, train_df.shape)\n",
    "print(\"With sklearn TfIdf Vectorizer:\", X_tr_q1q2_sk_tfidf.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using our implementation of TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8303725534934316\n",
      "Result on validation:  0.7532320054109183\n",
      "Result on test:  0.7486764630961492\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of TfIdf Vectorizer. Note that the result is different because the formula that sklearn uses is different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8598447779977646\n",
      "Result on validation:  0.7478109106794504\n",
      "Result on test:  0.7448132038351072\n"
     ]
    }
   ],
   "source": [
    "logisitc_sk = load_logistic('models/logistic_tfidf_sk.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "So, in this case, the mistakes are practically the same, hence:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8615642005166823\n",
      "Accuracy on validation:  0.7928827603264902\n",
      "Accuracy on test:  0.7880729179549334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels ==logistic.predict(X_tr_q1q2_tfidf))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2_tfidf))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2_tfidf))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  be Persian consider Caucasian  Treated q1:  be Persian consider Caucasian\n",
      "Original q2:  be Persian white  Treated q2:  be Persian white\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2_tfidf, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND MODEL: NAIVE CLASSIFIER WITH EXTRA FEATURES\n",
    "\n",
    "Given the mistakes encountered in the previous model, we tried to code some extra features to tackle with those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to obtain the extra features.\n",
    "\n",
    "Here we give a list of extra features that we could add to the feature vector.\n",
    "\n",
    "1. Lenght of the question\n",
    "\n",
    "2. Is there a [math] tag? \n",
    "\n",
    "3. Is there a number in the question?\n",
    "\n",
    "4. Is it the same number in both questions? \n",
    "\n",
    "5. % of intersection words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qlength(questions):\n",
    "    qlen = []\n",
    "    for quest in questions:\n",
    "        clean_doc_pattern = re.compile( r\"('\\w)|([^a-zA-Z0-9.])\") #Find words containing alphanumeric or points\n",
    "        q = re.sub('\\'s', '', quest) #Remove 's\n",
    "        q = re.sub('\\'t', ' not', q) #Change 't for not'\n",
    "        q = re.sub('\\'re', ' are', q) #Change 're for are'\n",
    "        q = re.sub('[?%!@#$\\'\\\"\"]', '', q)#Remove symbols\n",
    "        q = re.sub('\\.\\s', ' ', q)#Remove points with a space afterwards\n",
    "        clean_q = clean_doc_pattern.sub(\" \", q)\n",
    "        qlen.append(len(re.findall(r\"(?u)\\b[\\w.,]+\\b\",q)))\n",
    "        \n",
    "    return np.array(qlen).reshape(-1,1)\n",
    "\n",
    "def is_math(questions):\n",
    "    math=[]\n",
    "    for quest in questions:\n",
    "        if '[math]' in quest:\n",
    "            math.append(1)\n",
    "        else:\n",
    "            math.append(0)\n",
    "    return np.array(math).reshape(-1,1)\n",
    "    \n",
    "def is_number(word):\n",
    "    try :  \n",
    "        w = float(word) \n",
    "        if(np.isnan(w)):\n",
    "            return 0\n",
    "        if(np.isinf(w)):\n",
    "            return 0\n",
    "        res = 1\n",
    "    except : \n",
    "        res = 0\n",
    "    return res    \n",
    "\n",
    "def has_numbers(questions):\n",
    "    num=np.zeros((len(questions)))\n",
    "    which_num = np.zeros((len(questions)))\n",
    "    i=0\n",
    "    for quest in questions:\n",
    "        for w in re.findall(r\"(?u)\\b[\\w.,]+\\b\",quest):\n",
    "            is_num = is_number(w)\n",
    "            if is_num==1:\n",
    "                num[i]=1\n",
    "                which_num[i]=float(w)\n",
    "                if(np.isnan(which_num[i])):\n",
    "                    print(which_num[i])\n",
    "                    print(float(w))\n",
    "                break\n",
    "        i+=1\n",
    "    return num.reshape(-1,1), which_num.reshape(-1,1)\n",
    "\n",
    "\n",
    "def is_different_number(which_num1, which_num2):\n",
    "    dif = which_num1 - which_num2\n",
    "    dif[dif>0]=1\n",
    "    return np.array(dif).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row, q1, q2, q_dict):\n",
    "    set1 = set(q_dict[q1[row]])\n",
    "    set2 = set(q_dict[q2[row]])\n",
    "    return(len(set1.intersection(set2))/len(set1.union(set2)))\n",
    "\n",
    "\n",
    "def intersection(q1_train, q2_train,q1_val,q2_val, q1_test, q2_test):\n",
    "    q1 = q1_train + q1_val +  q1_test\n",
    "    q2 = q2_train + q1_val + q2_test\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(len(q1)):\n",
    "            q_dict[q1[i]].add(q2[i])\n",
    "            q_dict[q2[i]].add(q1[i])\n",
    "\n",
    "    intersect_train = []\n",
    "    intersect_test = []\n",
    "    intersect_val = []\n",
    "    for row in range(len(q1_train)):\n",
    "        intersect_train.append(q1_q2_intersect(row, q1_train, q2_train, q_dict))\n",
    "    \n",
    "    for row in range(len(q1_val)):\n",
    "        intersect_val.append(q1_q2_intersect(row, q1_val, q2_val, q_dict))\n",
    "        \n",
    "    for row in range(len(q1_test)):\n",
    "        intersect_test.append(q1_q2_intersect(row, q1_test, q2_test, q_dict))\n",
    "    \n",
    "    intersect_train = np.array(intersect_train).reshape(-1,1)\n",
    "    intersect_val = np.array(intersect_val).reshape(-1,1)\n",
    "    intersect_test = np.array(intersect_test).reshape(-1,1)\n",
    "    return intersect_train, intersect_val, intersect_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_train, intersect_val, intersect_test = intersection(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1_train, which_num1_train=  has_numbers(q1_train_raw)\n",
    "num2_train, which_num2_train =  has_numbers(q2_train_raw)\n",
    "dif_number_train = is_different_number(which_num1_train,which_num2_train)\n",
    "\n",
    "num1_val, which_num1_val=  has_numbers(q1_val_raw)\n",
    "num2_val, which_num2_val =  has_numbers(q2_val_raw)\n",
    "dif_number_val = is_different_number(which_num1_val,which_num2_val)\n",
    "\n",
    "num1_test, which_num1_test=  has_numbers(q1_test_raw)\n",
    "num2_test, which_num2_test =  has_numbers(q2_test_raw)\n",
    "dif_number_test = is_different_number(which_num1_test,which_num2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "math1_train = is_math(q1_train_raw)\n",
    "math2_train = is_math(q2_train_raw)\n",
    "\n",
    "math1_val = is_math(q1_val_raw)\n",
    "math2_val = is_math(q2_val_raw)\n",
    "\n",
    "math1_test = is_math(q1_test_raw)\n",
    "math2_test = is_math(q2_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1_train = get_qlength(q1_train_raw)\n",
    "len2_train = get_qlength(q2_train_raw)\n",
    "\n",
    "len1_val = get_qlength(q1_val_raw)\n",
    "len2_val = get_qlength(q2_val_raw)\n",
    "\n",
    "len1_test = get_qlength(q1_test_raw)\n",
    "len2_test = get_qlength(q2_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we have already check that our CountVectorizer yields the same result as the sklearn one, we will only use ours. We already loaded the count vectorizer, so we use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result. We see that the AUC has dropped a lot. We think that this may be due to the imbalance of the values of the different features, i.e., we are not normalizing the values of any of the features. We thought that it would be necessary to change the model, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.48450264973360235\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.4840274908285725\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2)))\n",
    "\n",
    "#val roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already loaded the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will only run the code for our TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2_tfidf = sparse.hstack((X_tr_q1q2_tfidf,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2_tfidf = sparse.hstack((X_val_q1q2_tfidf,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2_tfidf = sparse.hstack((X_te_q1q2_tfidf,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar thing happens with the tfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.48450264973360235\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.4840274908285725\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD MODEL: XGBOOST\n",
    "\n",
    "Given all the previous results, a thing was clear: we needed to change the classifier. So our take was: combine everything we have done until now (text with the spell checking and the extra features) but with a more sophisticated model. We chose the XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_count = xgb.Booster()\n",
    "\n",
    "xgb_count.load_model('models/xgb_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8415324575386137\n",
      "Accuracy on validation:  0.8431548355181796\n",
      "Accuracy on test:  0.8305795344925672\n",
      "AUC on train:  0.9160198613889634\n",
      "AUC on validation:  0.8989376768402263\n",
      "AUC on test:  0.8941733477487699\n",
      "Original q1:  what be 10 thing you would tell your 19 year old self  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  what be some of the most important thing you would tell your 19 year old self  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "pred_test = xgb_count.predict(d_test)\n",
    "pred_train = xgb_count.predict(d_train)\n",
    "pred_val = xgb_count.predict(d_val)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_count, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tfidf = xgb.Booster()\n",
    "\n",
    "xgb_tfidf.load_model('models/xgb_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8403712966525587\n",
      "Accuracy on validation:  0.8428765767994064\n",
      "Accuracy on test:  0.8289965124044621\n",
      "AUC on train:  0.9168537518780854\n",
      "AUC on validation:  0.8942311188490448\n",
      "AUC on test:  0.8891366320817327\n",
      "Original q1:  what be 10 thing you would tell your 19 year old self  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  what be some of the most important thing you would tell your 19 year old self  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2_tfidf, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2_tfidf, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2_tfidf, label=val_labels)\n",
    "\n",
    "pred_test = xgb_tfidf.predict(d_test)\n",
    "pred_val = xgb_tfidf.predict(d_val)\n",
    "pred_train = xgb_tfidf.predict(d_train)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_tfidf, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOURTH MODEL: DIFFERENT APPROACH WITH DEEP LEARNING\n",
    "\n",
    "Our main objective for this deliverable was to work with a more classic approach for natural language processing, mainly to implement and understand the CountVectorizer and TfIdfVectorizer. Additionally, we tried to work on the mistakes and limitations that this approach had, hence having to do a bit of feature engeenireing to tackle those problems.\n",
    "\n",
    "However, nowadays deep learning is used practically to solve anything, so, how well could it work to solve this problem? In this section we explore a completely different approach using deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from RNN import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and prepare fields\n",
    "question1 = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm')\n",
    "question2 = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm')\n",
    "label = data.LabelField(dtype=torch.float)\n",
    "\n",
    "fields = [('question1', question1), ('question2', question2), ('is_duplicate', label)]\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path='.', train='train_df.csv', test='test_df.csv', format='csv', fields=fields, skip_header=True)\n",
    "\n",
    "# build vocabularies (same for question1 and question2)\n",
    "MAX_VOCAB_SIZE = 60000\n",
    "\n",
    "# train data\n",
    "question1.build_vocab(train_data.question1, train_data.question2,\n",
    "                      max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "question2.build_vocab(train_data.question1, train_data.question2,\n",
    "                      max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "label.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "# test data\n",
    "question1.build_vocab(test_data.question1, test_data.question2,\n",
    "                      max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "question2.build_vocab(test_data.question1, test_data.question2,\n",
    "                      max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "label.build_vocab(test_data, max_size=MAX_VOCAB_SIZE)\n",
    "\n",
    "# prepare iterators\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    sort=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813070212342212\n"
     ]
    }
   ],
   "source": [
    "softmax = torch.nn.Softmax()\n",
    "output_values = []\n",
    "labels_values = []\n",
    "\n",
    "# model to device\n",
    "model = LSTM(vocab_dim=len(question1.vocab), text=question1)\n",
    "model.load_state_dict(torch.load('models/RNN'))\n",
    "model.to(device)\n",
    "\n",
    "# final test with the best model\n",
    "for batch in test_iterator:\n",
    "    output = model(batch.question1.to(device),\n",
    "                   batch.question2.to(device))\n",
    "    output = torch.argmax(softmax(output), dim=1)\n",
    "    output_values = np.concatenate(\n",
    "        (output_values, output.cpu().detach().numpy()))\n",
    "    labels_values = np.concatenate(\n",
    "        (labels_values, batch.is_duplicate.cpu().detach().numpy()))\n",
    "auc = metrics.roc_auc_score(output_values, labels_values)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST BUT NOT LEAST: LET'S DO PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We lose some accuracy with respect to the XGBoost alone because we are not doing the features exactly as the model above. We are adding extra features, yes, but from the treated data, instead of the raw data. We didn't know how to solve this issue (passing one set of data to one model and another different set to another model), so we opted for this solution. We are sure that it is possible to do so,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "import extra_features\n",
    "import xgboost as xgb\n",
    "from pipeline_classes import CountVectorizerTransformer, XGBModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Notice that what should have been done is to serialize the whole pipeline and load it as an object. We have a problem with serializing the CountVectorizer class, since the preprocessor has inline lambdas and these objects are not serializable (as fas as we know). The solution we found is to not save and recreate the preprocessor each time, saving the parameters. This takes no time, since the Preprocessor does not need to train. So we opted to serialize each object separatedly. \n",
    "\n",
    "In our opinion, the Pipeline should have two parent functions, `dump` and `load` that call the dump(s) and load(s) functions respectively for each model. Of course, sklearn does not work that way, but serializing object-wise instead of serializing the whole object seems like a much better approach, since strange things can happen for each object (and more so, when custom Transformers/Classifiers are defined!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVT = CountVectorizerTransformer()\n",
    "CVT.load(\"models/Pipeline_CountVectorizer.pkl\")\n",
    "xgb_mod = XGBModel()\n",
    "xgb_mod.load(\"models/Pipeline_XGBoost.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = Pipeline([\n",
    "    ('countVectorizer', CVT),\n",
    "    ('model', xgb_mod)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_test = q1_test+q2_test\n",
    "all_questions_val = q1_val+q2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883453008209628"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(val_labels, model_cv.predict(all_questions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906295230962548"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(test_labels, model_cv.predict(all_questions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
