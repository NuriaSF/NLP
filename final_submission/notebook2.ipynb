{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:09.447952Z",
     "start_time": "2020-03-14T14:45:09.441971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/laia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from CountVectorizer_BagOfWords import CountVectorizer as cv\n",
    "from TfIdfVectorizer import TfIdfVectorizer as tf\n",
    "from Spelling_Correction_c  import Spelling_Correction_c \n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import json\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to solve the following problem: given a pair of different questions of Quora, decide if they are asking the same or not. In this notebook, we will discuss the process we have followed to solve the problem, the different models that we have used as well as the mistakes that each model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311380</td>\n",
       "      <td>370141</td>\n",
       "      <td>108248</td>\n",
       "      <td>500659</td>\n",
       "      <td>How do I get home tutors?</td>\n",
       "      <td>How can I trust a home tutor?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62646</td>\n",
       "      <td>400219</td>\n",
       "      <td>349589</td>\n",
       "      <td>66001</td>\n",
       "      <td>What is the difference between 'had been', 'ha...</td>\n",
       "      <td>When should I use \"has been\", \"have been\" and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98055</td>\n",
       "      <td>15247</td>\n",
       "      <td>29146</td>\n",
       "      <td>29147</td>\n",
       "      <td>If my ATM card is blocked for online transacti...</td>\n",
       "      <td>My credit card was used for fraud transactions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127437</td>\n",
       "      <td>124101</td>\n",
       "      <td>200474</td>\n",
       "      <td>42953</td>\n",
       "      <td>How do I add USB 3.0 port in a laptop without ...</td>\n",
       "      <td>Can I use a USB 3.0 device in a USB 2.0 port?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111569</td>\n",
       "      <td>1333</td>\n",
       "      <td>2657</td>\n",
       "      <td>2658</td>\n",
       "      <td>What is the best Advantage of using Quora?</td>\n",
       "      <td>What is the benefit to Quora?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291083</th>\n",
       "      <td>192476</td>\n",
       "      <td>217697</td>\n",
       "      <td>306239</td>\n",
       "      <td>324116</td>\n",
       "      <td>Which is the best app for learning Yoga?</td>\n",
       "      <td>What is the best app to learn yoga?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291084</th>\n",
       "      <td>17730</td>\n",
       "      <td>81327</td>\n",
       "      <td>49754</td>\n",
       "      <td>89884</td>\n",
       "      <td>What's the main reason behind 500 &amp; 1000 rs no...</td>\n",
       "      <td>What is the reason behind PM Modi's decision t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291085</th>\n",
       "      <td>28030</td>\n",
       "      <td>401928</td>\n",
       "      <td>179454</td>\n",
       "      <td>143531</td>\n",
       "      <td>How can I find out my drivers license number u...</td>\n",
       "      <td>What's the best way to get my driver’s license...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291086</th>\n",
       "      <td>277869</td>\n",
       "      <td>231706</td>\n",
       "      <td>341577</td>\n",
       "      <td>341578</td>\n",
       "      <td>How has Bill Gates charity foundation helped t...</td>\n",
       "      <td>How has Bill Gates affected us and the world?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291087</th>\n",
       "      <td>249342</td>\n",
       "      <td>134794</td>\n",
       "      <td>215378</td>\n",
       "      <td>215379</td>\n",
       "      <td>Why do the UN and almost all the countries con...</td>\n",
       "      <td>What is the best underground scene in Tel Aviv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291088 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id    qid1    qid2  \\\n",
       "0           311380  370141  108248  500659   \n",
       "1            62646  400219  349589   66001   \n",
       "2            98055   15247   29146   29147   \n",
       "3           127437  124101  200474   42953   \n",
       "4           111569    1333    2657    2658   \n",
       "...            ...     ...     ...     ...   \n",
       "291083      192476  217697  306239  324116   \n",
       "291084       17730   81327   49754   89884   \n",
       "291085       28030  401928  179454  143531   \n",
       "291086      277869  231706  341577  341578   \n",
       "291087      249342  134794  215378  215379   \n",
       "\n",
       "                                                question1  \\\n",
       "0                               How do I get home tutors?   \n",
       "1       What is the difference between 'had been', 'ha...   \n",
       "2       If my ATM card is blocked for online transacti...   \n",
       "3       How do I add USB 3.0 port in a laptop without ...   \n",
       "4              What is the best Advantage of using Quora?   \n",
       "...                                                   ...   \n",
       "291083           Which is the best app for learning Yoga?   \n",
       "291084  What's the main reason behind 500 & 1000 rs no...   \n",
       "291085  How can I find out my drivers license number u...   \n",
       "291086  How has Bill Gates charity foundation helped t...   \n",
       "291087  Why do the UN and almost all the countries con...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0                           How can I trust a home tutor?             0  \n",
       "1       When should I use \"has been\", \"have been\" and ...             0  \n",
       "2       My credit card was used for fraud transactions...             0  \n",
       "3           Can I use a USB 3.0 device in a USB 2.0 port?             0  \n",
       "4                           What is the benefit to Quora?             1  \n",
       "...                                                   ...           ...  \n",
       "291083                What is the best app to learn yoga?             1  \n",
       "291084  What is the reason behind PM Modi's decision t...             1  \n",
       "291085  What's the best way to get my driver’s license...             0  \n",
       "291086      How has Bill Gates affected us and the world?             0  \n",
       "291087  What is the best underground scene in Tel Aviv...             0  \n",
       "\n",
       "[291088 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "train_df =pd.read_csv('train_df.csv')\n",
    "test_df =pd.read_csv('test_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions for some of the models.The first functions are meant to extract, given a vectorizer, the matrix of features for the classifier. The two last functions are used to identify the errors that a classifier is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_list(q1,q2,count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1)\n",
    "    q2_mat = count_vectorizer.transform(q2)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "    \n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "\n",
    "def get_mistakes(clf, X_q1q2, y):\n",
    "    \"\"\"\n",
    "    Returns two lists: one containing the indices of the predictions that are not correct\n",
    "    and another one containing the predictions\n",
    "    \"\"\"\n",
    "    predictions        = clf.predict(X_q1q2).round(0).astype(int)\n",
    "    incorrect_preds    = predictions != y\n",
    "    incorrect_indices, = np.where(incorrect_preds)\n",
    "    incorrect_indices2 = [x for x in  range(len(incorrect_preds)) if incorrect_preds[x] ==True]\n",
    "    incorrect_indices3 = np.arange(len(incorrect_preds))[incorrect_preds]        \n",
    "    \n",
    "    if np.sum(incorrect_preds)==0:\n",
    "        print(\"no mistakes in this df\")\n",
    "    else:\n",
    "        return incorrect_indices, predictions\n",
    "    \n",
    "def print_mistake_k(k, dataset, mistake_indices, predictions):\n",
    "    \"\"\"\n",
    "    Auxiliar function to print the k-th mistake made in the prediction\n",
    "    \"\"\"\n",
    "    print(\"Original q1: \", train_df.iloc[mistake_indices[k]].question1, \" Treated q1: \", dataset[mistake_indices[k]])\n",
    "    print(\"Original q2: \", train_df.iloc[mistake_indices[k]].question2, \" Treated q2: \", dataset[mistake_indices[k]+train_df.shape[0]])\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])  \n",
    "    \n",
    "    \n",
    "def load_logistic(filename):\n",
    "    # Load the logistic parameters\n",
    "    with open(filename, 'rb') as fp:\n",
    "        logistic_params = json.load(fp)\n",
    "        \n",
    "    #Create logistic object\n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "    logistic.coef_ = np.array(logistic_params['coef_'])\n",
    "    logistic.classes_ = np.array(logistic_params['classes_'])\n",
    "    logistic.intercept_ = np.array(logistic_params['intercept_'])\n",
    "    \n",
    "    return logistic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first naive model was proposed in class: pass the text through the vectorizers and use the returned matrix as the matrix of features. We saw that the classifier wrongly classified some questions with spelling mistakes. For example, the classifiera would identify as different questions those who were written like \"whats\" from those who were written like \"what's\". \n",
    "\n",
    "We thought that this problem may be common with any model that we try to train, so the first thing we propose to do is correcting the spelling mistakes. We propose to remove \"'s\", change the negatives \"'t\" for \"not\" as well as the plurals \"'re\" for \"are\", remove symbols and points. Then, we implemented a spell checking function using the edit distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells that transform the raw questions into cleaned questions can be found in notebook 1. Since the computations are very long, we load the already cleaned questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all elements of the documents into strings \n",
    "q1_train_raw =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train_raw =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_val_raw  =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
    "q2_val_raw  =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
    "q1_test_raw  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test_raw  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "\n",
    "all_questions_raw = q1_train_raw + q2_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions= q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/train_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "train_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/val_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "val_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/test_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "test_labels = [int(x.strip()) for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST MODEL: NAIVE CLASSIFIER WITH SPELL CHECKING\n",
    "For the first model, we just wanted to see what difference did the spellchecking do. So, did we improve the results? Did we improve the results as expected? If so, what mistakes is our model doing now?\n",
    "\n",
    "We will do this checking for both the CountVectorizer and the TfIdfVectorizer. The classifier will be a sklearn Logistic Regression. This is a simple classifier that will serve as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:21.407870Z",
     "start_time": "2020-03-14T14:45:21.401885Z"
    }
   },
   "outputs": [],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3), max_df = 0.99, min_df = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace fit by loading the model here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:46:44.793780Z",
     "start_time": "2020-03-14T14:45:42.528916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)\", document_cleaner_func=None,\n",
       "                dtype=<class 'numpy.float32'>, max_df=0.99, min_df=5,\n",
       "                min_word_counts=1, ngram_range=(1, 3),\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                token_cleaner_func=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer_func=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "                ngram_range=(1, 3), preprocessor=None,\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the result obtained with our implementation of the CountVectorizer with the result obtained using the sklearn version of the CountVectorizer. One of the objectives of this deliverable was to implement and understend how the vectorizers worked, so we set as an objective to obtain the same as the sklearn vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "\n",
    "X_tr_q1q2_sk = get_features_from_list(q1_train, q2_train,countvect_sk)\n",
    "\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, CountVectorizer)\n",
    "\n",
    "X_val_q1q2_sk  = get_features_from_list(q1_val, q2_val, countvect_sk)\n",
    "\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)\n",
    "\n",
    "X_te_q1q2_sk  = get_features_from_list(q1_test, q2_test, countvect_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the logistic regression using our implementation of CountVectorizer. We load the logistic regression model from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873714184551172\n",
      "Result on validation:  0.756423585069429\n",
      "Result on test:  0.7533599677258676\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8874152854760406\n",
      "Result on validation:  0.7564062563763118\n",
      "Result on test:  0.7533834828117459\n"
     ]
    }
   ],
   "source": [
    "logistic_sk = load_logistic('models/logistic_sk.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "We wanted to identify the mistakes that the classifier was doing in this case. We saw that the classifier was making mistages mainly for the following reasons:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.907399137030726\n",
      "Accuracy on validation:  0.788647044274054\n",
      "Accuracy on test:  0.7851665883400529\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels==logistic.predict(X_tr_q1q2))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Why do men like women's feet?  Treated q1:  why do men like woman foot\n",
      "Original q2:  Why do men like womens feet?  Treated q2:  why do men like woman foot\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace fit by loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.99, max_features=None,\n",
       "                min_df=5, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=False,\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "                use_idf=False, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tf(stop_words = set(stopwords.words('english')), ngram_range=(1,3), max_df = 0.4, min_df = 5)\n",
    "tfidf_vectorizer.fit(all_questions)\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that additionally, we want to compare our result with that given by the implementation of sklearn of the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our TfIdf Vectorizer: (291088, 285364) (291088, 7)\n",
      "With sklearn TfIdf Vectorizer: (291088, 285364) (291088, 7)\n"
     ]
    }
   ],
   "source": [
    "X_tr_q1q2_tfidf = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_tr_q1q2_sk_tfidf = get_features_from_list(q1_train, q2_train, tfidf_sk)\n",
    "X_val_q1q2_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_vectorizer)\n",
    "X_val_q1q2_sk_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_sk)\n",
    "X_te_q1q2_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)\n",
    "X_te_q1q2_sk_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_sk)\n",
    "\n",
    "print(\"With our TfIdf Vectorizer:\", X_tr_q1q2_tfidf.shape, train_df.shape)\n",
    "print(\"With sklearn TfIdf Vectorizer:\", X_tr_q1q2_sk_tfidf.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using our implementation of TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8303725534934316\n",
      "Result on validation:  0.7532320054109183\n",
      "Result on test:  0.7486764630961492\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of TfIdf Vectorizer. Note that the result is different because the formula that sklearn uses is different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8598897999557084\n",
      "Result on validation:  0.7480028045154151\n",
      "Result on test:  0.7448268895587145\n"
     ]
    }
   ],
   "source": [
    "logisitc_sk = load_logistic('models/logistic_tfidf_sk.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "So, in this case, the mistakes are practically the same, hence:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8615642005166823\n",
      "Accuracy on validation:  0.7928827603264902\n",
      "Accuracy on test:  0.7880729179549334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels ==logistic.predict(X_tr_q1q2_tfidf))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2_tfidf))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2_tfidf))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Are Persians considered Caucasian?  Treated q1:  be Persian consider Caucasian\n",
      "Original q2:  Are Persians White?  Treated q2:  be Persian white\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2_tfidf, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND MODEL: NAIVE CLASSIFIER WITH EXTRA FEATURES\n",
    "\n",
    "Given the mistakes encountered in the previous model, we tried to code some extra features to tackle with those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to obtain the extra features.\n",
    "\n",
    "Here we give a list of extra features that we could add to the feature vector.\n",
    "\n",
    "1. Lenght of the question\n",
    "\n",
    "2. Is there a [math] tag? \n",
    "\n",
    "3. Is there a number in the question?\n",
    "\n",
    "4. Is it the same number in both questions? \n",
    "\n",
    "5. % of intersection words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qlength(questions):\n",
    "    qlen = []\n",
    "    for quest in questions:\n",
    "        clean_doc_pattern = re.compile( r\"('\\w)|([^a-zA-Z0-9.])\") #Find words containing alphanumeric or points\n",
    "        q = re.sub('\\'s', '', quest) #Remove 's\n",
    "        q = re.sub('\\'t', ' not', q) #Change 't for not'\n",
    "        q = re.sub('\\'re', ' are', q) #Change 're for are'\n",
    "        q = re.sub('[?%!@#$\\'\\\"\"]', '', q)#Remove symbols\n",
    "        q = re.sub('\\.\\s', ' ', q)#Remove points with a space afterwards\n",
    "        clean_q = clean_doc_pattern.sub(\" \", q)\n",
    "        qlen.append(len(re.findall(r\"(?u)\\b[\\w.,]+\\b\",q)))\n",
    "        \n",
    "    return np.array(qlen).reshape(-1,1)\n",
    "\n",
    "def is_math(questions):\n",
    "    math=[]\n",
    "    for quest in questions:\n",
    "        if '[math]' in quest:\n",
    "            math.append(1)\n",
    "        else:\n",
    "            math.append(0)\n",
    "    return np.array(math).reshape(-1,1)\n",
    "    \n",
    "def is_number(word):\n",
    "    try :  \n",
    "        w = float(word) \n",
    "        if(np.isnan(w)):\n",
    "            return 0\n",
    "        if(np.isinf(w)):\n",
    "            return 0\n",
    "        res = 1\n",
    "    except : \n",
    "        res = 0\n",
    "    return res    \n",
    "\n",
    "def has_numbers(questions):\n",
    "    num=np.zeros((len(questions)))\n",
    "    which_num = np.zeros((len(questions)))\n",
    "    i=0\n",
    "    for quest in questions:\n",
    "        for w in re.findall(r\"(?u)\\b[\\w.,]+\\b\",quest):\n",
    "            is_num = is_number(w)\n",
    "            if is_num==1:\n",
    "                num[i]=1\n",
    "                which_num[i]=float(w)\n",
    "                if(np.isnan(which_num[i])):\n",
    "                    print(which_num[i])\n",
    "                    print(float(w))\n",
    "                break\n",
    "        i+=1\n",
    "    return num.reshape(-1,1), which_num.reshape(-1,1)\n",
    "\n",
    "\n",
    "def is_different_number(which_num1, which_num2):\n",
    "    dif = which_num1 - which_num2\n",
    "    dif[dif>0]=1\n",
    "    return np.array(dif).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row, q1, q2, q_dict):\n",
    "    set1 = set(q_dict[q1[row]])\n",
    "    set2 = set(q_dict[q2[row]])\n",
    "    return(len(set1.intersection(set2))/len(set1.union(set2)))\n",
    "\n",
    "\n",
    "def intersection(q1_train, q2_train,q1_val,q2_val, q1_test, q2_test):\n",
    "    q1 = q1_train + q1_val +  q1_test\n",
    "    q2 = q2_train + q1_val + q2_test\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(len(q1)):\n",
    "            q_dict[q1[i]].add(q2[i])\n",
    "            q_dict[q2[i]].add(q1[i])\n",
    "\n",
    "    intersect_train = []\n",
    "    intersect_test = []\n",
    "    intersect_val = []\n",
    "    for row in range(len(q1_train)):\n",
    "        intersect_train.append(q1_q2_intersect(row, q1_train, q2_train, q_dict))\n",
    "    \n",
    "    for row in range(len(q1_val)):\n",
    "        intersect_val.append(q1_q2_intersect(row, q1_val, q2_val, q_dict))\n",
    "        \n",
    "    for row in range(len(q1_test)):\n",
    "        intersect_test.append(q1_q2_intersect(row, q1_test, q2_test, q_dict))\n",
    "    \n",
    "    intersect_train = np.array(intersect_train).reshape(-1,1)\n",
    "    intersect_val = np.array(intersect_val).reshape(-1,1)\n",
    "    intersect_test = np.array(intersect_test).reshape(-1,1)\n",
    "    return intersect_train, intersect_val, intersect_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_train, intersect_val, intersect_test = intersection(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1_train, which_num1_train=  has_numbers(q1_train_raw)\n",
    "num2_train, which_num2_train =  has_numbers(q2_train_raw)\n",
    "dif_number_train = is_different_number(which_num1_train,which_num2_train)\n",
    "\n",
    "num1_val, which_num1_val=  has_numbers(q1_val_raw)\n",
    "num2_val, which_num2_val =  has_numbers(q2_val_raw)\n",
    "dif_number_val = is_different_number(which_num1_val,which_num2_val)\n",
    "\n",
    "num1_test, which_num1_test=  has_numbers(q1_test_raw)\n",
    "num2_test, which_num2_test =  has_numbers(q2_test_raw)\n",
    "dif_number_test = is_different_number(which_num1_test,which_num2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "math1_train = is_math(q1_train_raw)\n",
    "math2_train = is_math(q2_train_raw)\n",
    "\n",
    "math1_val = is_math(q1_val_raw)\n",
    "math2_val = is_math(q2_val_raw)\n",
    "\n",
    "math1_test = is_math(q1_test_raw)\n",
    "math2_test = is_math(q2_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1_train = get_qlength(q1_train_raw)\n",
    "len2_train = get_qlength(q2_train_raw)\n",
    "\n",
    "len1_val = get_qlength(q1_val_raw)\n",
    "len2_val = get_qlength(q2_val_raw)\n",
    "\n",
    "len1_test = get_qlength(q1_test_raw)\n",
    "len2_test = get_qlength(q2_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we have already check that our CountVectorizer yields the same result as the sklearn one, we will only use ours. We already loaded the count vectorizer, so we use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result. We see that the AUC has dropped a lot. We think that this may be due to the imbalance of the values of the different features, i.e., we are not normalizing the values of any of the features. We thought that it would be necessary to change the model, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2)))\n",
    "\n",
    "#val roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already loaded the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will only run the code for our TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2_tfidf = sparse.hstack((X_tr_q1q2_tfidf,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2_tfidf = sparse.hstack((X_val_q1q2_tfidf,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2_tfidf = sparse.hstack((X_te_q1q2_tfidf,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar thing happens with the tfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD MODEL: XGBOOST\n",
    "\n",
    "Given all the previous results, a thing was clear: we needed to change the classifier. So our take was: combine everything we have done until now (text with the spell checking and the extra features) but with a more sophisticated model. We chose the XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_count = xgb.Booster()\n",
    "\n",
    "xgb_count.load_model('models/xgb_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8415084098279558\n",
      "Accuracy on validation:  0.8431548355181796\n",
      "Accuracy on test:  0.8305547997724406\n",
      "AUC on train:  0.9159446541394345\n",
      "AUC on validation:  0.8989376768402263\n",
      "AUC on test:  0.8939774284673486\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "pred_test = xgb_count.predict(d_test)\n",
    "pred_train = xgb_count.predict(d_train)\n",
    "pred_val = xgb_count.predict(d_val)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_count, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tfidf = xgb.Booster()\n",
    "\n",
    "xgb_tfidf.load_model('models/xgb_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8403609904908481\n",
      "Accuracy on validation:  0.8428765767994064\n",
      "Accuracy on test:  0.8288975735239555\n",
      "AUC on train:  0.9168234628206468\n",
      "AUC on validation:  0.8942311188490448\n",
      "AUC on test:  0.8889539442559593\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2_tfidf, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2_tfidf, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2_tfidf, label=val_labels)\n",
    "\n",
    "pred_test = xgb_tfidf.predict(d_test)\n",
    "pred_val = xgb_tfidf.predict(d_val)\n",
    "pred_train = xgb_tfidf.predict(d_train)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_tfidf, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOURTH MODEL: DIFFERENT APPROACH WITH DEEP LEARNING\n",
    "\n",
    "Our main objective for this deliverable was to work with a more classic approach for natural language processing, mainly to implement and understand the CountVectorizer and TfIdfVectorizer. Additionally, we tried to work on the mistakes and limitations that this approach had, hence having to do a bit of feature engeenireing to tackle those problems.\n",
    "\n",
    "However, nowadays deep learning is used practically to solve anything, so, how well could it work to solve this problem? In this section we explore a completely different approach using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST BUT NOT LEAST: LET'S DO PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data from CountVectorizer to model automatically, we need to transform the data first. This is just a hacky way to do it. \n",
    "\n",
    "Note that the correct way is to modify the CountVectorized that we implemented so that the output of the  transform is already the desired matrix (i.e. doing the hstack inside). We tried to implement the most general CountVectorized that we could, so we have to resort to this way. It is in no way slower in terms of speed. It is just ugly in the sense that is doing something hack-ish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiddleTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        nexamples, nvars = X.shape\n",
    "        split = (int)(nexamples/2)\n",
    "        XX1 = X[:split,:]\n",
    "        XX2 = X[split:,:]\n",
    "        XX = sparse.hstack([XX1, XX2], format='csr')\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = Pipeline([\n",
    "    ('countVectorizer', cv(#token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n",
    "                     stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3))),\n",
    "    ('middleTransformer', MiddleTransformer()),\n",
    "    ('model', sklearn.linear_model.LogisticRegression(solver=\"liblinear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countVectorizer', CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)\", document_cleaner_func=None,\n",
       "        dtype=<class 'numpy.float32'>, max_df=1.0, min_df=1,\n",
       "        min_word_counts=1, ngram_range=(1, 3),\n",
       "        stop_words={'them', 'an', 'herself', 'but', 'd', 'some', 'had', 'yours', \"isn't\", ...ty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.fit(all_questions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_test = q1_test+q2_test\n",
    "all_questions_val = q1_val+q2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(val_labels, model_cv.predict(all_questions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7778368046218165"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(test_labels, model_cv.predict(all_questions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipeline Great Again: GridSearch Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select samples to cross validate, we need the samples to be in pair with the labels. Right now we dont have that, so we are going to (once again) hack our way through it.\n",
    "\n",
    "Again note that the correct way to do it is to modify the CountVectorizer so that it automatically does this process inside the fit and transform functions. But following our filosophy of having a general CountVectorizer, this is a good way to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndocs = (int)(len(all_questions)/2)\n",
    "all_questions_tuples = [(q1_train[i], q2_train[i]) for i in range(ndocs)]\n",
    "\n",
    "class CrossValidationTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        XX = [x[0] for x in X]\n",
    "        for x in X:\n",
    "            XX.append(x[1])\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "model_cv = Pipeline([\n",
    "    ('crossValidationTransformer', CrossValidationTransformer()),\n",
    "    ('countVectorizer', tf(#token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n",
    "                     stop_words = set(stopwords.words('english')))),\n",
    "    ('middleTransformer', MiddleTransformer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {'countVectorizer__ngram_range':[(1,3)], \n",
    "         'countVectorizer__min_df':[1,5,10,15,],\n",
    "         'countVectorizer__max_df':[.4,.3],\n",
    "         }\n",
    "\n",
    "gs = GridSearchCV(model_cv, params, scoring='roc_auc', cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 12.1min finished\n"
     ]
    }
   ],
   "source": [
    "results = gs.fit(all_questions_tuples, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countVectorizer__max_df': 0.4,\n",
       " 'countVectorizer__min_df': 1,\n",
       " 'countVectorizer__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436795654397367"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
