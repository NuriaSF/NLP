{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:09.447952Z",
     "start_time": "2020-03-14T14:45:09.441971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programs\\Anaconda\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "E:\\Programs\\Anaconda\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from CountVectorizer_BagOfWords import CountVectorizer as cv\n",
    "from TfIdfVectorizer import TfIdfVectorizer as tf\n",
    "#from Spelling_Correction_c  import Spelling_Correction_c \n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import json\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to solve the following problem: given a pair of different questions of Quora, decide if they are asking the same or not. In this notebook, we will discuss the process we have followed to solve the problem, the different models that we have used as well as the mistakes that each model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311380</td>\n",
       "      <td>370141</td>\n",
       "      <td>108248</td>\n",
       "      <td>500659</td>\n",
       "      <td>How do I get home tutors?</td>\n",
       "      <td>How can I trust a home tutor?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62646</td>\n",
       "      <td>400219</td>\n",
       "      <td>349589</td>\n",
       "      <td>66001</td>\n",
       "      <td>What is the difference between 'had been', 'ha...</td>\n",
       "      <td>When should I use \"has been\", \"have been\" and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98055</td>\n",
       "      <td>15247</td>\n",
       "      <td>29146</td>\n",
       "      <td>29147</td>\n",
       "      <td>If my ATM card is blocked for online transacti...</td>\n",
       "      <td>My credit card was used for fraud transactions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127437</td>\n",
       "      <td>124101</td>\n",
       "      <td>200474</td>\n",
       "      <td>42953</td>\n",
       "      <td>How do I add USB 3.0 port in a laptop without ...</td>\n",
       "      <td>Can I use a USB 3.0 device in a USB 2.0 port?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111569</td>\n",
       "      <td>1333</td>\n",
       "      <td>2657</td>\n",
       "      <td>2658</td>\n",
       "      <td>What is the best Advantage of using Quora?</td>\n",
       "      <td>What is the benefit to Quora?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>277562</td>\n",
       "      <td>334891</td>\n",
       "      <td>223475</td>\n",
       "      <td>462116</td>\n",
       "      <td>Where can I download The Economist PDF?</td>\n",
       "      <td>Where can I download pdf of Gillian Flynn's Go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86744</td>\n",
       "      <td>95115</td>\n",
       "      <td>158705</td>\n",
       "      <td>129101</td>\n",
       "      <td>Why do some people get everything?</td>\n",
       "      <td>Why is that some people get what they want ver...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15344</td>\n",
       "      <td>375063</td>\n",
       "      <td>193288</td>\n",
       "      <td>210697</td>\n",
       "      <td>Is an all-out nuclear war survivable?</td>\n",
       "      <td>Would all out nuclear war destroy all life on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>163359</td>\n",
       "      <td>156960</td>\n",
       "      <td>245527</td>\n",
       "      <td>245528</td>\n",
       "      <td>What are the advantages of cashless transaction?</td>\n",
       "      <td>What could happen to cashless transaction afte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>188422</td>\n",
       "      <td>369725</td>\n",
       "      <td>224469</td>\n",
       "      <td>347611</td>\n",
       "      <td>What are the main problems of India?</td>\n",
       "      <td>What is the main problem faced by India?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>279962</td>\n",
       "      <td>192258</td>\n",
       "      <td>156907</td>\n",
       "      <td>235979</td>\n",
       "      <td>What are good ideas to help fall asleep quickly?</td>\n",
       "      <td>What are some ways to fall asleep faster?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>216557</td>\n",
       "      <td>331043</td>\n",
       "      <td>402448</td>\n",
       "      <td>457880</td>\n",
       "      <td>What are the best places to visit in or near J...</td>\n",
       "      <td>What are the best places to visit in Alaska an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>268330</td>\n",
       "      <td>384831</td>\n",
       "      <td>516903</td>\n",
       "      <td>138358</td>\n",
       "      <td>What is physical meaning of divergence?</td>\n",
       "      <td>What is the physical meaning of divergence, cu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>106336</td>\n",
       "      <td>93933</td>\n",
       "      <td>156950</td>\n",
       "      <td>691</td>\n",
       "      <td>What is the best phone to buy below 6K?</td>\n",
       "      <td>What is the best phone to buy below 15k?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>259771</td>\n",
       "      <td>167473</td>\n",
       "      <td>259631</td>\n",
       "      <td>10263</td>\n",
       "      <td>How do people become beggars?</td>\n",
       "      <td>Can begging be dropped?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>178323</td>\n",
       "      <td>222181</td>\n",
       "      <td>329685</td>\n",
       "      <td>306168</td>\n",
       "      <td>Who can help me improve my English?</td>\n",
       "      <td>Who can help me in improving my English speaking?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>256338</td>\n",
       "      <td>64456</td>\n",
       "      <td>112012</td>\n",
       "      <td>112013</td>\n",
       "      <td>After Japan took its goals in WW2 Pacific cama...</td>\n",
       "      <td>What is an excubator?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>142892</td>\n",
       "      <td>401526</td>\n",
       "      <td>266913</td>\n",
       "      <td>135200</td>\n",
       "      <td>According to you, which is the best episode of...</td>\n",
       "      <td>Which is one of the best episodes of Comedy Ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>124049</td>\n",
       "      <td>398604</td>\n",
       "      <td>500314</td>\n",
       "      <td>32424</td>\n",
       "      <td>What is expected cutoff for 2016 IBPS clerk pr...</td>\n",
       "      <td>What will be the expected cutoff of IBPS clerk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41266</td>\n",
       "      <td>363707</td>\n",
       "      <td>493708</td>\n",
       "      <td>493709</td>\n",
       "      <td>Why some MacBooks' keyboards have keys with si...</td>\n",
       "      <td>What is cmd (command prompt)? I want to know i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>309297</td>\n",
       "      <td>101812</td>\n",
       "      <td>52164</td>\n",
       "      <td>168583</td>\n",
       "      <td>What is the strangest thing that ever happened...</td>\n",
       "      <td>What is the strangest thing that has ever happ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>228527</td>\n",
       "      <td>192360</td>\n",
       "      <td>291987</td>\n",
       "      <td>291988</td>\n",
       "      <td>Can I install a tablet in my car?</td>\n",
       "      <td>How do you install a tablet into a car?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>158251</td>\n",
       "      <td>55245</td>\n",
       "      <td>51931</td>\n",
       "      <td>97415</td>\n",
       "      <td>Why is Manaphy a whiny Pokemon?</td>\n",
       "      <td>Why is Manaphy so whiny throughout the Pokémon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>131897</td>\n",
       "      <td>131766</td>\n",
       "      <td>211157</td>\n",
       "      <td>211158</td>\n",
       "      <td>Where do you sell forestry products?</td>\n",
       "      <td>How do you sell forestry products?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>53575</td>\n",
       "      <td>361835</td>\n",
       "      <td>136730</td>\n",
       "      <td>168336</td>\n",
       "      <td>Can I hack any phone by just having his phone ...</td>\n",
       "      <td>Can someone hack into your iPhone just by know...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59604</td>\n",
       "      <td>53681</td>\n",
       "      <td>32690</td>\n",
       "      <td>94866</td>\n",
       "      <td>How much wood would a woodchuck chuck if a woo...</td>\n",
       "      <td>Why can't woodchucks chuck wood?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>141412</td>\n",
       "      <td>179927</td>\n",
       "      <td>275973</td>\n",
       "      <td>275974</td>\n",
       "      <td>Has anyone successfully used the \"no-contact r...</td>\n",
       "      <td>Do ex's get back in contact?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>217166</td>\n",
       "      <td>108126</td>\n",
       "      <td>177724</td>\n",
       "      <td>177725</td>\n",
       "      <td>Is deep learning overhyped?</td>\n",
       "      <td>Is machine learning overrated or overhyped?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>229895</td>\n",
       "      <td>235703</td>\n",
       "      <td>346466</td>\n",
       "      <td>84865</td>\n",
       "      <td>How can we find professors, PhD &amp; masters stud...</td>\n",
       "      <td>How can we find professors, PhD &amp; masters stud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11583</td>\n",
       "      <td>61194</td>\n",
       "      <td>106887</td>\n",
       "      <td>106888</td>\n",
       "      <td>After demonetisation of notes government intro...</td>\n",
       "      <td>What is the utility behind removing Rs 1000 no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291058</th>\n",
       "      <td>83079</td>\n",
       "      <td>244863</td>\n",
       "      <td>357575</td>\n",
       "      <td>357576</td>\n",
       "      <td>Why do Europeans drink everything at room temp...</td>\n",
       "      <td>Can I store kegs of beer at room temperature?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291059</th>\n",
       "      <td>285872</td>\n",
       "      <td>69674</td>\n",
       "      <td>25702</td>\n",
       "      <td>120198</td>\n",
       "      <td>Who are some famous people who annoy you for s...</td>\n",
       "      <td>If Harry Potter parant were Half-blood how com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291060</th>\n",
       "      <td>219069</td>\n",
       "      <td>192387</td>\n",
       "      <td>292020</td>\n",
       "      <td>292021</td>\n",
       "      <td>I am a US citizen. Can I take a gift from my b...</td>\n",
       "      <td>Selling property in India and paying TDS in In...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291061</th>\n",
       "      <td>6648</td>\n",
       "      <td>306997</td>\n",
       "      <td>430614</td>\n",
       "      <td>430615</td>\n",
       "      <td>Why does Chrome struggle with WSJ.com?</td>\n",
       "      <td>How do I install Google Chrome?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291062</th>\n",
       "      <td>30255</td>\n",
       "      <td>282161</td>\n",
       "      <td>42696</td>\n",
       "      <td>402061</td>\n",
       "      <td>Do Americans regret their atomic attack agains...</td>\n",
       "      <td>What is your biggest regret if you didn't get ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291063</th>\n",
       "      <td>214338</td>\n",
       "      <td>330011</td>\n",
       "      <td>210443</td>\n",
       "      <td>98683</td>\n",
       "      <td>How can I make a home made pocket pussy which ...</td>\n",
       "      <td>How does it feel when a penis enters a vagina,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291064</th>\n",
       "      <td>245799</td>\n",
       "      <td>130848</td>\n",
       "      <td>209912</td>\n",
       "      <td>126496</td>\n",
       "      <td>How do you deal with your fear of going to the...</td>\n",
       "      <td>How do I get over my fear and resistance of go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291065</th>\n",
       "      <td>157698</td>\n",
       "      <td>67239</td>\n",
       "      <td>116403</td>\n",
       "      <td>116404</td>\n",
       "      <td>How can I become a business man.I just love to...</td>\n",
       "      <td>I have a good idea, but I don't know the techn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291066</th>\n",
       "      <td>307651</td>\n",
       "      <td>318595</td>\n",
       "      <td>389204</td>\n",
       "      <td>376001</td>\n",
       "      <td>My mobile was stolen. Is it possible for the t...</td>\n",
       "      <td>I have lost my mobile. How do I find my IMEI n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291067</th>\n",
       "      <td>130256</td>\n",
       "      <td>49825</td>\n",
       "      <td>88632</td>\n",
       "      <td>88633</td>\n",
       "      <td>How should I prepare for p-block in boards, an...</td>\n",
       "      <td>How do I prepare chemistry for boards in one m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291068</th>\n",
       "      <td>227748</td>\n",
       "      <td>369886</td>\n",
       "      <td>368203</td>\n",
       "      <td>98239</td>\n",
       "      <td>Science: Is evolutionary theory falsifiable?</td>\n",
       "      <td>Is the theory of evolution unfalsifiable?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291069</th>\n",
       "      <td>35662</td>\n",
       "      <td>177712</td>\n",
       "      <td>61254</td>\n",
       "      <td>7059</td>\n",
       "      <td>Are we near World War 3?</td>\n",
       "      <td>Is world war 3 likely?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291070</th>\n",
       "      <td>232857</td>\n",
       "      <td>185821</td>\n",
       "      <td>283590</td>\n",
       "      <td>283591</td>\n",
       "      <td>Can anyone convert to judaism?</td>\n",
       "      <td>Why did you convert to Judaism?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291071</th>\n",
       "      <td>196719</td>\n",
       "      <td>33087</td>\n",
       "      <td>60836</td>\n",
       "      <td>60837</td>\n",
       "      <td>I always feel sleepy in my lectures. What can ...</td>\n",
       "      <td>I always feel sleepy and lost in my own world....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291072</th>\n",
       "      <td>164782</td>\n",
       "      <td>190177</td>\n",
       "      <td>289172</td>\n",
       "      <td>289173</td>\n",
       "      <td>Why is someone I blocked on snapchat on my \"ad...</td>\n",
       "      <td>If you unblock someone one snapchat will you r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291073</th>\n",
       "      <td>71200</td>\n",
       "      <td>263278</td>\n",
       "      <td>77023</td>\n",
       "      <td>18893</td>\n",
       "      <td>What is the best way to to make money?</td>\n",
       "      <td>What's the best way to make fast cash?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291074</th>\n",
       "      <td>118857</td>\n",
       "      <td>37520</td>\n",
       "      <td>68257</td>\n",
       "      <td>68258</td>\n",
       "      <td>What was the reason behind the sudden end of t...</td>\n",
       "      <td>What can you do when your favorite TV show ends?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291075</th>\n",
       "      <td>249903</td>\n",
       "      <td>191293</td>\n",
       "      <td>290590</td>\n",
       "      <td>290591</td>\n",
       "      <td>If the whole world had to speak one language, ...</td>\n",
       "      <td>If the world had to speak one language what wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291076</th>\n",
       "      <td>65632</td>\n",
       "      <td>132616</td>\n",
       "      <td>212314</td>\n",
       "      <td>212315</td>\n",
       "      <td>When can I expect the merit list of TES army c...</td>\n",
       "      <td>When will the merit list of tes 36 be declared?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291077</th>\n",
       "      <td>153313</td>\n",
       "      <td>382998</td>\n",
       "      <td>342654</td>\n",
       "      <td>245040</td>\n",
       "      <td>Which is a suitable inpatient drug and alcohol...</td>\n",
       "      <td>Which is a suitable inpatient drug and alcohol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291078</th>\n",
       "      <td>220374</td>\n",
       "      <td>235050</td>\n",
       "      <td>84373</td>\n",
       "      <td>345673</td>\n",
       "      <td>What are some Font Awesome icons that represen...</td>\n",
       "      <td>What are some awesome arguments made in House ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291079</th>\n",
       "      <td>46203</td>\n",
       "      <td>60766</td>\n",
       "      <td>106219</td>\n",
       "      <td>106220</td>\n",
       "      <td>I lost my original charger of the OnePlus One....</td>\n",
       "      <td>I have a Moto G 2nd gen. Usually in the day I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291080</th>\n",
       "      <td>129130</td>\n",
       "      <td>104174</td>\n",
       "      <td>172030</td>\n",
       "      <td>172031</td>\n",
       "      <td>How are football fans from Spain reacting to L...</td>\n",
       "      <td>Interrupt applications in power electronics?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291081</th>\n",
       "      <td>146449</td>\n",
       "      <td>220297</td>\n",
       "      <td>327370</td>\n",
       "      <td>327371</td>\n",
       "      <td>Why are people with authenticity, conscience, ...</td>\n",
       "      <td>Why are people selfish towards people who are ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291082</th>\n",
       "      <td>194278</td>\n",
       "      <td>314547</td>\n",
       "      <td>439280</td>\n",
       "      <td>439281</td>\n",
       "      <td>Is wanted to start you own tech startup and wa...</td>\n",
       "      <td>Is anybody in Bangalore (near marathalli) look...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291083</th>\n",
       "      <td>192476</td>\n",
       "      <td>217697</td>\n",
       "      <td>306239</td>\n",
       "      <td>324116</td>\n",
       "      <td>Which is the best app for learning Yoga?</td>\n",
       "      <td>What is the best app to learn yoga?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291084</th>\n",
       "      <td>17730</td>\n",
       "      <td>81327</td>\n",
       "      <td>49754</td>\n",
       "      <td>89884</td>\n",
       "      <td>What's the main reason behind 500 &amp; 1000 rs no...</td>\n",
       "      <td>What is the reason behind PM Modi's decision t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291085</th>\n",
       "      <td>28030</td>\n",
       "      <td>401928</td>\n",
       "      <td>179454</td>\n",
       "      <td>143531</td>\n",
       "      <td>How can I find out my drivers license number u...</td>\n",
       "      <td>What's the best way to get my driver’s license...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291086</th>\n",
       "      <td>277869</td>\n",
       "      <td>231706</td>\n",
       "      <td>341577</td>\n",
       "      <td>341578</td>\n",
       "      <td>How has Bill Gates charity foundation helped t...</td>\n",
       "      <td>How has Bill Gates affected us and the world?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291087</th>\n",
       "      <td>249342</td>\n",
       "      <td>134794</td>\n",
       "      <td>215378</td>\n",
       "      <td>215379</td>\n",
       "      <td>Why do the UN and almost all the countries con...</td>\n",
       "      <td>What is the best underground scene in Tel Aviv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291088 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      id    qid1    qid2  \\\n",
       "0           311380  370141  108248  500659   \n",
       "1            62646  400219  349589   66001   \n",
       "2            98055   15247   29146   29147   \n",
       "3           127437  124101  200474   42953   \n",
       "4           111569    1333    2657    2658   \n",
       "5           277562  334891  223475  462116   \n",
       "6            86744   95115  158705  129101   \n",
       "7            15344  375063  193288  210697   \n",
       "8           163359  156960  245527  245528   \n",
       "9           188422  369725  224469  347611   \n",
       "10          279962  192258  156907  235979   \n",
       "11          216557  331043  402448  457880   \n",
       "12          268330  384831  516903  138358   \n",
       "13          106336   93933  156950     691   \n",
       "14          259771  167473  259631   10263   \n",
       "15          178323  222181  329685  306168   \n",
       "16          256338   64456  112012  112013   \n",
       "17          142892  401526  266913  135200   \n",
       "18          124049  398604  500314   32424   \n",
       "19           41266  363707  493708  493709   \n",
       "20          309297  101812   52164  168583   \n",
       "21          228527  192360  291987  291988   \n",
       "22          158251   55245   51931   97415   \n",
       "23          131897  131766  211157  211158   \n",
       "24           53575  361835  136730  168336   \n",
       "25           59604   53681   32690   94866   \n",
       "26          141412  179927  275973  275974   \n",
       "27          217166  108126  177724  177725   \n",
       "28          229895  235703  346466   84865   \n",
       "29           11583   61194  106887  106888   \n",
       "...            ...     ...     ...     ...   \n",
       "291058       83079  244863  357575  357576   \n",
       "291059      285872   69674   25702  120198   \n",
       "291060      219069  192387  292020  292021   \n",
       "291061        6648  306997  430614  430615   \n",
       "291062       30255  282161   42696  402061   \n",
       "291063      214338  330011  210443   98683   \n",
       "291064      245799  130848  209912  126496   \n",
       "291065      157698   67239  116403  116404   \n",
       "291066      307651  318595  389204  376001   \n",
       "291067      130256   49825   88632   88633   \n",
       "291068      227748  369886  368203   98239   \n",
       "291069       35662  177712   61254    7059   \n",
       "291070      232857  185821  283590  283591   \n",
       "291071      196719   33087   60836   60837   \n",
       "291072      164782  190177  289172  289173   \n",
       "291073       71200  263278   77023   18893   \n",
       "291074      118857   37520   68257   68258   \n",
       "291075      249903  191293  290590  290591   \n",
       "291076       65632  132616  212314  212315   \n",
       "291077      153313  382998  342654  245040   \n",
       "291078      220374  235050   84373  345673   \n",
       "291079       46203   60766  106219  106220   \n",
       "291080      129130  104174  172030  172031   \n",
       "291081      146449  220297  327370  327371   \n",
       "291082      194278  314547  439280  439281   \n",
       "291083      192476  217697  306239  324116   \n",
       "291084       17730   81327   49754   89884   \n",
       "291085       28030  401928  179454  143531   \n",
       "291086      277869  231706  341577  341578   \n",
       "291087      249342  134794  215378  215379   \n",
       "\n",
       "                                                question1  \\\n",
       "0                               How do I get home tutors?   \n",
       "1       What is the difference between 'had been', 'ha...   \n",
       "2       If my ATM card is blocked for online transacti...   \n",
       "3       How do I add USB 3.0 port in a laptop without ...   \n",
       "4              What is the best Advantage of using Quora?   \n",
       "5                 Where can I download The Economist PDF?   \n",
       "6                      Why do some people get everything?   \n",
       "7                   Is an all-out nuclear war survivable?   \n",
       "8        What are the advantages of cashless transaction?   \n",
       "9                    What are the main problems of India?   \n",
       "10       What are good ideas to help fall asleep quickly?   \n",
       "11      What are the best places to visit in or near J...   \n",
       "12                What is physical meaning of divergence?   \n",
       "13                What is the best phone to buy below 6K?   \n",
       "14                          How do people become beggars?   \n",
       "15                    Who can help me improve my English?   \n",
       "16      After Japan took its goals in WW2 Pacific cama...   \n",
       "17      According to you, which is the best episode of...   \n",
       "18      What is expected cutoff for 2016 IBPS clerk pr...   \n",
       "19      Why some MacBooks' keyboards have keys with si...   \n",
       "20      What is the strangest thing that ever happened...   \n",
       "21                      Can I install a tablet in my car?   \n",
       "22                        Why is Manaphy a whiny Pokemon?   \n",
       "23                   Where do you sell forestry products?   \n",
       "24      Can I hack any phone by just having his phone ...   \n",
       "25      How much wood would a woodchuck chuck if a woo...   \n",
       "26      Has anyone successfully used the \"no-contact r...   \n",
       "27                            Is deep learning overhyped?   \n",
       "28      How can we find professors, PhD & masters stud...   \n",
       "29      After demonetisation of notes government intro...   \n",
       "...                                                   ...   \n",
       "291058  Why do Europeans drink everything at room temp...   \n",
       "291059  Who are some famous people who annoy you for s...   \n",
       "291060  I am a US citizen. Can I take a gift from my b...   \n",
       "291061             Why does Chrome struggle with WSJ.com?   \n",
       "291062  Do Americans regret their atomic attack agains...   \n",
       "291063  How can I make a home made pocket pussy which ...   \n",
       "291064  How do you deal with your fear of going to the...   \n",
       "291065  How can I become a business man.I just love to...   \n",
       "291066  My mobile was stolen. Is it possible for the t...   \n",
       "291067  How should I prepare for p-block in boards, an...   \n",
       "291068      Science: Is evolutionary theory falsifiable?    \n",
       "291069                           Are we near World War 3?   \n",
       "291070                     Can anyone convert to judaism?   \n",
       "291071  I always feel sleepy in my lectures. What can ...   \n",
       "291072  Why is someone I blocked on snapchat on my \"ad...   \n",
       "291073             What is the best way to to make money?   \n",
       "291074  What was the reason behind the sudden end of t...   \n",
       "291075  If the whole world had to speak one language, ...   \n",
       "291076  When can I expect the merit list of TES army c...   \n",
       "291077  Which is a suitable inpatient drug and alcohol...   \n",
       "291078  What are some Font Awesome icons that represen...   \n",
       "291079  I lost my original charger of the OnePlus One....   \n",
       "291080  How are football fans from Spain reacting to L...   \n",
       "291081  Why are people with authenticity, conscience, ...   \n",
       "291082  Is wanted to start you own tech startup and wa...   \n",
       "291083           Which is the best app for learning Yoga?   \n",
       "291084  What's the main reason behind 500 & 1000 rs no...   \n",
       "291085  How can I find out my drivers license number u...   \n",
       "291086  How has Bill Gates charity foundation helped t...   \n",
       "291087  Why do the UN and almost all the countries con...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0                           How can I trust a home tutor?             0  \n",
       "1       When should I use \"has been\", \"have been\" and ...             0  \n",
       "2       My credit card was used for fraud transactions...             0  \n",
       "3           Can I use a USB 3.0 device in a USB 2.0 port?             0  \n",
       "4                           What is the benefit to Quora?             1  \n",
       "5       Where can I download pdf of Gillian Flynn's Go...             0  \n",
       "6       Why is that some people get what they want ver...             1  \n",
       "7       Would all out nuclear war destroy all life on ...             1  \n",
       "8       What could happen to cashless transaction afte...             0  \n",
       "9                What is the main problem faced by India?             1  \n",
       "10              What are some ways to fall asleep faster?             1  \n",
       "11      What are the best places to visit in Alaska an...             0  \n",
       "12      What is the physical meaning of divergence, cu...             1  \n",
       "13               What is the best phone to buy below 15k?             0  \n",
       "14                                Can begging be dropped?             0  \n",
       "15      Who can help me in improving my English speaking?             1  \n",
       "16                                  What is an excubator?             0  \n",
       "17      Which is one of the best episodes of Comedy Ni...             1  \n",
       "18      What will be the expected cutoff of IBPS clerk...             0  \n",
       "19      What is cmd (command prompt)? I want to know i...             0  \n",
       "20      What is the strangest thing that has ever happ...             1  \n",
       "21                How do you install a tablet into a car?             1  \n",
       "22      Why is Manaphy so whiny throughout the Pokémon...             1  \n",
       "23                     How do you sell forestry products?             0  \n",
       "24      Can someone hack into your iPhone just by know...             1  \n",
       "25                       Why can't woodchucks chuck wood?             0  \n",
       "26                           Do ex's get back in contact?             0  \n",
       "27            Is machine learning overrated or overhyped?             0  \n",
       "28      How can we find professors, PhD & masters stud...             0  \n",
       "29      What is the utility behind removing Rs 1000 no...             1  \n",
       "...                                                   ...           ...  \n",
       "291058      Can I store kegs of beer at room temperature?             0  \n",
       "291059  If Harry Potter parant were Half-blood how com...             0  \n",
       "291060  Selling property in India and paying TDS in In...             0  \n",
       "291061                    How do I install Google Chrome?             0  \n",
       "291062  What is your biggest regret if you didn't get ...             0  \n",
       "291063  How does it feel when a penis enters a vagina,...             0  \n",
       "291064  How do I get over my fear and resistance of go...             1  \n",
       "291065  I have a good idea, but I don't know the techn...             0  \n",
       "291066  I have lost my mobile. How do I find my IMEI n...             0  \n",
       "291067  How do I prepare chemistry for boards in one m...             0  \n",
       "291068          Is the theory of evolution unfalsifiable?             0  \n",
       "291069                             Is world war 3 likely?             1  \n",
       "291070                    Why did you convert to Judaism?             0  \n",
       "291071  I always feel sleepy and lost in my own world....             1  \n",
       "291072  If you unblock someone one snapchat will you r...             0  \n",
       "291073             What's the best way to make fast cash?             1  \n",
       "291074   What can you do when your favorite TV show ends?             0  \n",
       "291075  If the world had to speak one language what wo...             1  \n",
       "291076    When will the merit list of tes 36 be declared?             1  \n",
       "291077  Which is a suitable inpatient drug and alcohol...             0  \n",
       "291078  What are some awesome arguments made in House ...             0  \n",
       "291079  I have a Moto G 2nd gen. Usually in the day I ...             0  \n",
       "291080       Interrupt applications in power electronics?             0  \n",
       "291081  Why are people selfish towards people who are ...             0  \n",
       "291082  Is anybody in Bangalore (near marathalli) look...             0  \n",
       "291083                What is the best app to learn yoga?             1  \n",
       "291084  What is the reason behind PM Modi's decision t...             1  \n",
       "291085  What's the best way to get my driver’s license...             0  \n",
       "291086      How has Bill Gates affected us and the world?             0  \n",
       "291087  What is the best underground scene in Tel Aviv...             0  \n",
       "\n",
       "[291088 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "train_df =pd.read_csv('train_df.csv')\n",
    "test_df =pd.read_csv('test_df.csv')\n",
    "val_df = pd.read_csv('val_df.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions for some of the models.The first functions are meant to extract, given a vectorizer, the matrix of features for the classifier. The two last functions are used to identify the errors that a classifier is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_list(q1,q2,count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1)\n",
    "    q2_mat = count_vectorizer.transform(q2)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "    \n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "\n",
    "def get_mistakes(clf, X_q1q2, y):\n",
    "    \"\"\"\n",
    "    Returns two lists: one containing the indices of the predictions that are not correct\n",
    "    and another one containing the predictions\n",
    "    \"\"\"\n",
    "    predictions        = clf.predict(X_q1q2).round(0).astype(int)\n",
    "    incorrect_preds    = predictions != y\n",
    "    incorrect_indices, = np.where(incorrect_preds)\n",
    "    incorrect_indices2 = [x for x in  range(len(incorrect_preds)) if incorrect_preds[x] ==True]\n",
    "    incorrect_indices3 = np.arange(len(incorrect_preds))[incorrect_preds]        \n",
    "    \n",
    "    if np.sum(incorrect_preds)==0:\n",
    "        print(\"no mistakes in this df\")\n",
    "    else:\n",
    "        return incorrect_indices, predictions\n",
    "    \n",
    "def print_mistake_k(k, dataset, mistake_indices, predictions):\n",
    "    \"\"\"\n",
    "    Auxiliar function to print the k-th mistake made in the prediction\n",
    "    \"\"\"\n",
    "    print(\"Original q1: \", train_df.iloc[mistake_indices[k]].question1, \" Treated q1: \", dataset[mistake_indices[k]])\n",
    "    print(\"Original q2: \", train_df.iloc[mistake_indices[k]].question2, \" Treated q2: \", dataset[mistake_indices[k]+train_df.shape[0]])\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])  \n",
    "    \n",
    "    \n",
    "def load_logistic(filename):\n",
    "    # Load the logistic parameters\n",
    "    with open(filename, 'rb') as fp:\n",
    "        logistic_params = json.load(fp)\n",
    "        \n",
    "    #Create logistic object\n",
    "    logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "    logistic.coef_ = np.array(logistic_params['coef_'])\n",
    "    logistic.classes_ = np.array(logistic_params['classes_'])\n",
    "    logistic.intercept_ = np.array(logistic_params['intercept_'])\n",
    "    \n",
    "    return logistic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first naive model was proposed in class: pass the text through the vectorizers and use the returned matrix as the matrix of features. We saw that the classifier wrongly classified some questions with spelling mistakes. For example, the classifiera would identify as different questions those who were written like \"whats\" from those who were written like \"what's\". \n",
    "\n",
    "We thought that this problem may be common with any model that we try to train, so the first thing we propose to do is correcting the spelling mistakes. We propose to remove \"'s\", change the negatives \"'t\" for \"not\" as well as the plurals \"'re\" for \"are\", remove symbols and points. Then, we implemented a spell checking function using the edit distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells that transform the raw questions into cleaned questions can be found in notebook 1. Since the computations are very long, we load the already cleaned questions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all elements of the documents into strings \n",
    "q1_train_raw =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train_raw =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_val_raw  =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
    "q2_val_raw  =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
    "q1_test_raw  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test_raw  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "\n",
    "all_questions_raw = q1_train_raw + q2_train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions= q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/train_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "train_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/val_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "val_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/test_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "test_labels = [int(x.strip()) for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST MODEL: NAIVE CLASSIFIER WITH SPELL CHECKING\n",
    "For the first model, we just wanted to see what difference did the spellchecking do. So, did we improve the results? Did we improve the results as expected? If so, what mistakes is our model doing now?\n",
    "\n",
    "We will do this checking for both the CountVectorizer and the TfIdfVectorizer. The classifier will be a sklearn Logistic Regression. This is a simple classifier that will serve as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:21.407870Z",
     "start_time": "2020-03-14T14:45:21.401885Z"
    }
   },
   "outputs": [],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:46:44.793780Z",
     "start_time": "2020-03-14T14:45:42.528916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)\", document_cleaner_func=None,\n",
       "        dtype=<class 'numpy.float32'>, max_df=0.99, min_df=5,\n",
       "        min_word_counts=1, ngram_range=(1, 3),\n",
       "        stop_words={'her', 'won', 'through', 'on', 'can', 'didn', 'so', \"haven't\", 'was', 'his', 'i', 'if', 'not', 'then', 're', 'hadn', 'yourself', 'hers', 'this', 'and', 'do', 'mightn', 'about', 'ma', 'or', 'why', 'yours', 'for', 'each', 'your', 'himself', 'before', 'such', \"you'd\", \"weren't\", 'have', 'ot...lf', 'has', \"mightn't\", 'into', 'where', 'mustn', 'having', \"needn't\", 'be', 'our', \"hasn't\", 'too'},\n",
       "        token_cleaner_func=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer_func=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.load(\"models/CountVectorizer.pkl\")\n",
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words={'her', 'won', 'through', 'on', 'can', 'so', 'didn', \"haven't\", 'was', 'i', 'his', 'if', 'then', 'not', 're', 'hadn', 'yourself', 'hers', 'this', 'and', 'do', 'about', 'mightn', 'ma', 'or', 'yours', 'for', 'why', 'each', 'your', 'himself', 'before', 'such', \"you'd\", \"weren't\", 'have', 'ot...lf', 'has', \"mightn't\", 'into', 'where', 'mustn', 'having', \"needn't\", 'be', 'our', \"hasn't\", 'too'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the result obtained with our implementation of the CountVectorizer with the result obtained using the sklearn version of the CountVectorizer. One of the objectives of this deliverable was to implement and understend how the vectorizers worked, so we set as an objective to obtain the same as the sklearn vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "\n",
    "X_tr_q1q2_sk = get_features_from_list(q1_train, q2_train,countvect_sk)\n",
    "\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, CountVectorizer)\n",
    "\n",
    "X_val_q1q2_sk  = get_features_from_list(q1_val, q2_val, countvect_sk)\n",
    "\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)\n",
    "\n",
    "X_te_q1q2_sk  = get_features_from_list(q1_test, q2_test, countvect_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the logistic regression using our implementation of CountVectorizer. We load the logistic regression model from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873892706438838\n",
      "Result on validation:  0.7564062563763118\n",
      "Result on test:  0.7533668105876712\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873667596649119\n",
      "Result on validation:  0.7563817405636126\n",
      "Result on test:  0.7533304796390545\n"
     ]
    }
   ],
   "source": [
    "logistic_sk = load_logistic('models/logistic_sk.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "We wanted to identify the mistakes that the classifier was doing in this case. We saw that the classifier was making mistages mainly for the following reasons:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.9074094431924367\n",
      "Accuracy on validation:  0.788647044274054\n",
      "Accuracy on test:  0.7851665883400529\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels==logistic.predict(X_tr_q1q2))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Why do men like women's feet?  Treated q1:  why do men like woman foot\n",
      "Original q2:  Why do men like womens feet?  Treated q2:  why do men like woman foot\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "        stop_words={'her', 'won', 'through', 'on', 'can', 'so', 'didn', \"haven't\", 'was', 'i', 'his', 'if', 'then', 'not', 're', 'hadn', 'yourself', 'hers', 'this', 'and', 'do', 'about', 'mightn', 'ma', 'or', 'yours', 'for', 'why', 'each', 'your', 'himself', 'before', 'such', \"you'd\", \"weren't\", 'have', 'ot...lf', 'has', \"mightn't\", 'into', 'where', 'mustn', 'having', \"needn't\", 'be', 'our', \"hasn't\", 'too'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=False,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tf()\n",
    "tfidf_vectorizer.load(\"models/TfIdfVectorizer.pkl\")\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that additionally, we want to compare our result with that given by the implementation of sklearn of the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our TfIdf Vectorizer: (291088, 285364) (291088, 7)\n",
      "With sklearn TfIdf Vectorizer: (291088, 285364) (291088, 7)\n"
     ]
    }
   ],
   "source": [
    "X_tr_q1q2_tfidf = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_tr_q1q2_sk_tfidf = get_features_from_list(q1_train, q2_train, tfidf_sk)\n",
    "X_val_q1q2_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_vectorizer)\n",
    "X_val_q1q2_sk_tfidf  = get_features_from_list(q1_val, q2_val, tfidf_sk)\n",
    "X_te_q1q2_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)\n",
    "X_te_q1q2_sk_tfidf  = get_features_from_list(q1_test, q2_test, tfidf_sk)\n",
    "\n",
    "print(\"With our TfIdf Vectorizer:\", X_tr_q1q2_tfidf.shape, train_df.shape)\n",
    "print(\"With sklearn TfIdf Vectorizer:\", X_tr_q1q2_sk_tfidf.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using our implementation of TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8303725534934316\n",
      "Result on validation:  0.7532320054109183\n",
      "Result on test:  0.7486764630961492\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of TfIdf Vectorizer. Note that the result is different because the formula that sklearn uses is different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8598447779977646\n",
      "Result on validation:  0.7478109106794504\n",
      "Result on test:  0.7448132038351072\n"
     ]
    }
   ],
   "source": [
    "logisitc_sk = load_logistic('models/logistic_tfidf_sk.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "So, in this case, the mistakes are practically the same, hence:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8615642005166823\n",
      "Accuracy on validation:  0.7928827603264902\n",
      "Accuracy on test:  0.7880729179549334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels ==logistic.predict(X_tr_q1q2_tfidf))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2_tfidf))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2_tfidf))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Are Persians considered Caucasian?  Treated q1:  be Persian consider Caucasian\n",
      "Original q2:  Are Persians White?  Treated q2:  be Persian white\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2_tfidf, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND MODEL: NAIVE CLASSIFIER WITH EXTRA FEATURES\n",
    "\n",
    "Given the mistakes encountered in the previous model, we tried to code some extra features to tackle with those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to obtain the extra features.\n",
    "\n",
    "Here we give a list of extra features that we could add to the feature vector.\n",
    "\n",
    "1. Lenght of the question\n",
    "\n",
    "2. Is there a [math] tag? \n",
    "\n",
    "3. Is there a number in the question?\n",
    "\n",
    "4. Is it the same number in both questions? \n",
    "\n",
    "5. % of intersection words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qlength(questions):\n",
    "    qlen = []\n",
    "    for quest in questions:\n",
    "        clean_doc_pattern = re.compile( r\"('\\w)|([^a-zA-Z0-9.])\") #Find words containing alphanumeric or points\n",
    "        q = re.sub('\\'s', '', quest) #Remove 's\n",
    "        q = re.sub('\\'t', ' not', q) #Change 't for not'\n",
    "        q = re.sub('\\'re', ' are', q) #Change 're for are'\n",
    "        q = re.sub('[?%!@#$\\'\\\"\"]', '', q)#Remove symbols\n",
    "        q = re.sub('\\.\\s', ' ', q)#Remove points with a space afterwards\n",
    "        clean_q = clean_doc_pattern.sub(\" \", q)\n",
    "        qlen.append(len(re.findall(r\"(?u)\\b[\\w.,]+\\b\",q)))\n",
    "        \n",
    "    return np.array(qlen).reshape(-1,1)\n",
    "\n",
    "def is_math(questions):\n",
    "    math=[]\n",
    "    for quest in questions:\n",
    "        if '[math]' in quest:\n",
    "            math.append(1)\n",
    "        else:\n",
    "            math.append(0)\n",
    "    return np.array(math).reshape(-1,1)\n",
    "    \n",
    "def is_number(word):\n",
    "    try :  \n",
    "        w = float(word) \n",
    "        if(np.isnan(w)):\n",
    "            return 0\n",
    "        if(np.isinf(w)):\n",
    "            return 0\n",
    "        res = 1\n",
    "    except : \n",
    "        res = 0\n",
    "    return res    \n",
    "\n",
    "def has_numbers(questions):\n",
    "    num=np.zeros((len(questions)))\n",
    "    which_num = np.zeros((len(questions)))\n",
    "    i=0\n",
    "    for quest in questions:\n",
    "        for w in re.findall(r\"(?u)\\b[\\w.,]+\\b\",quest):\n",
    "            is_num = is_number(w)\n",
    "            if is_num==1:\n",
    "                num[i]=1\n",
    "                which_num[i]=float(w)\n",
    "                if(np.isnan(which_num[i])):\n",
    "                    print(which_num[i])\n",
    "                    print(float(w))\n",
    "                break\n",
    "        i+=1\n",
    "    return num.reshape(-1,1), which_num.reshape(-1,1)\n",
    "\n",
    "\n",
    "def is_different_number(which_num1, which_num2):\n",
    "    dif = which_num1 - which_num2\n",
    "    dif[dif>0]=1\n",
    "    return np.array(dif).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row, q1, q2, q_dict):\n",
    "    set1 = set(q_dict[q1[row]])\n",
    "    set2 = set(q_dict[q2[row]])\n",
    "    return(len(set1.intersection(set2))/len(set1.union(set2)))\n",
    "\n",
    "\n",
    "def intersection(q1_train, q2_train,q1_val,q2_val, q1_test, q2_test):\n",
    "    q1 = q1_train + q1_val +  q1_test\n",
    "    q2 = q2_train + q1_val + q2_test\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(len(q1)):\n",
    "            q_dict[q1[i]].add(q2[i])\n",
    "            q_dict[q2[i]].add(q1[i])\n",
    "\n",
    "    intersect_train = []\n",
    "    intersect_test = []\n",
    "    intersect_val = []\n",
    "    for row in range(len(q1_train)):\n",
    "        intersect_train.append(q1_q2_intersect(row, q1_train, q2_train, q_dict))\n",
    "    \n",
    "    for row in range(len(q1_val)):\n",
    "        intersect_val.append(q1_q2_intersect(row, q1_val, q2_val, q_dict))\n",
    "        \n",
    "    for row in range(len(q1_test)):\n",
    "        intersect_test.append(q1_q2_intersect(row, q1_test, q2_test, q_dict))\n",
    "    \n",
    "    intersect_train = np.array(intersect_train).reshape(-1,1)\n",
    "    intersect_val = np.array(intersect_val).reshape(-1,1)\n",
    "    intersect_test = np.array(intersect_test).reshape(-1,1)\n",
    "    return intersect_train, intersect_val, intersect_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_train, intersect_val, intersect_test = intersection(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1_train, which_num1_train=  has_numbers(q1_train_raw)\n",
    "num2_train, which_num2_train =  has_numbers(q2_train_raw)\n",
    "dif_number_train = is_different_number(which_num1_train,which_num2_train)\n",
    "\n",
    "num1_val, which_num1_val=  has_numbers(q1_val_raw)\n",
    "num2_val, which_num2_val =  has_numbers(q2_val_raw)\n",
    "dif_number_val = is_different_number(which_num1_val,which_num2_val)\n",
    "\n",
    "num1_test, which_num1_test=  has_numbers(q1_test_raw)\n",
    "num2_test, which_num2_test =  has_numbers(q2_test_raw)\n",
    "dif_number_test = is_different_number(which_num1_test,which_num2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "math1_train = is_math(q1_train_raw)\n",
    "math2_train = is_math(q2_train_raw)\n",
    "\n",
    "math1_val = is_math(q1_val_raw)\n",
    "math2_val = is_math(q2_val_raw)\n",
    "\n",
    "math1_test = is_math(q1_test_raw)\n",
    "math2_test = is_math(q2_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1_train = get_qlength(q1_train_raw)\n",
    "len2_train = get_qlength(q2_train_raw)\n",
    "\n",
    "len1_val = get_qlength(q1_val_raw)\n",
    "len2_val = get_qlength(q2_val_raw)\n",
    "\n",
    "len1_test = get_qlength(q1_test_raw)\n",
    "len2_test = get_qlength(q2_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we have already check that our CountVectorizer yields the same result as the sklearn one, we will only use ours. We already loaded the count vectorizer, so we use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result. We see that the AUC has dropped a lot. We think that this may be due to the imbalance of the values of the different features, i.e., we are not normalizing the values of any of the features. We thought that it would be necessary to change the model, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features.json')\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2)))\n",
    "\n",
    "#val roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already loaded the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will only run the code for our TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2_tfidf = sparse.hstack((X_tr_q1q2_tfidf,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2_tfidf = sparse.hstack((X_val_q1q2_tfidf,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2_tfidf = sparse.hstack((X_te_q1q2_tfidf,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar thing happens with the tfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = load_logistic('models/logistic_extra_features_tfidf.json')\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2_tfidf)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2_tfidf)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD MODEL: XGBOOST\n",
    "\n",
    "Given all the previous results, a thing was clear: we needed to change the classifier. So our take was: combine everything we have done until now (text with the spell checking and the extra features) but with a more sophisticated model. We chose the XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_count = xgb.Booster()\n",
    "\n",
    "xgb_count.load_model('models/xgb_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8415084098279558\n",
      "Accuracy on validation:  0.8431548355181796\n",
      "Accuracy on test:  0.8305547997724406\n",
      "AUC on train:  0.9159446541394345\n",
      "AUC on validation:  0.8989376768402263\n",
      "AUC on test:  0.8939774284673486\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "pred_test = xgb_count.predict(d_test)\n",
    "pred_train = xgb_count.predict(d_train)\n",
    "pred_val = xgb_count.predict(d_val)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_count, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tfidf = xgb.Booster()\n",
    "\n",
    "xgb_tfidf.load_model('models/xgb_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8403609904908481\n",
      "Accuracy on validation:  0.8428765767994064\n",
      "Accuracy on test:  0.8288975735239555\n",
      "AUC on train:  0.9168234628206468\n",
      "AUC on validation:  0.8942311188490448\n",
      "AUC on test:  0.8889539442559593\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2_tfidf, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2_tfidf, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2_tfidf, label=val_labels)\n",
    "\n",
    "pred_test = xgb_tfidf.predict(d_test)\n",
    "pred_val = xgb_tfidf.predict(d_val)\n",
    "pred_train = xgb_tfidf.predict(d_train)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_tfidf, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOURTH MODEL: DIFFERENT APPROACH WITH DEEP LEARNING\n",
    "\n",
    "Our main objective for this deliverable was to work with a more classic approach for natural language processing, mainly to implement and understand the CountVectorizer and TfIdfVectorizer. Additionally, we tried to work on the mistakes and limitations that this approach had, hence having to do a bit of feature engeenireing to tackle those problems.\n",
    "\n",
    "However, nowadays deep learning is used practically to solve anything, so, how well could it work to solve this problem? In this section we explore a completely different approach using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST BUT NOT LEAST: LET'S DO PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We lose some accuracy with respect to the XGBoost alone because we are not doing the features exactly as the model above. We are adding extra features, yes, but from the treated data, instead of the raw data. We didn't know how to solve this issue (passing one set of data to one model and another different set to another model), so we opted for this solution. We are sure that it is possible to do so,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "import extra_features\n",
    "import xgboost as xgb\n",
    "from pipeline_classes import CountVectorizerTransformer, XGBModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Notice that what should have been done is to serialize the whole pipeline and load it as an object. We have a problem with serializing the CountVectorizer class, since the preprocessor has inline lambdas and these objects are not serializable (as fas as we know). The solution we found is to not save and recreate the preprocessor each time, saving the parameters. This takes no time, since the Preprocessor does not need to train. So we opted to serialize each object separatedly. \n",
    "\n",
    "In our opinion, the Pipeline should have two parent functions, `dump` and `load` that call the dump(s) and load(s) functions respectively for each model. Of course, sklearn does not work that way, but serializing object-wise instead of serializing the whole object seems like a much better approach, since strange things can happen for each object (and more so, when custom Transformers/Classifiers are defined!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVT = CountVectorizerTransformer()\n",
    "CVT.load(\"models/Pipeline_CountVectorizer.pkl\")\n",
    "xgb_mod = XGBModel()\n",
    "xgb_mod.load(\"models/Pipeline_XGBoost.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = Pipeline([\n",
    "    ('countVectorizer', CVT),\n",
    "    ('model', xgb_mod)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_test = q1_test+q2_test\n",
    "all_questions_val = q1_val+q2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883453008209628"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(val_labels, model_cv.predict(all_questions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7906295230962548"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(test_labels, model_cv.predict(all_questions_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
