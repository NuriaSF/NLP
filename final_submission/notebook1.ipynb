{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:09.447952Z",
     "start_time": "2020-03-14T14:45:09.441971Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from CountVectorizer_BagOfWords import CountVectorizer as cv\n",
    "from TfIdfVectorizer import TfIdfVectorizer as tf\n",
    "#from Spelling_Correction_c  import Spelling_Correction_c \n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#How to call such stemmers and lemmatizer in the CountVectorizer object:\n",
    "#PorterStemmer(): token_cleaner_func = PorterStemmer().stem\n",
    "#LancasterStemmer(): token_cleaner_func = LancasterStemmer().stem\n",
    "#SnowballStemmer(language='english'): token_cleaner_func = SnowballStemmer(language='english').stem\n",
    "#WordNetLemmatizer(): token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to solve the following problem: given a pair of different questions of Quora, decide if they are asking the same or not. In this notebook, we will discuss the process we have followed to solve the problem, the different models that we have used as well as the mistakes that each model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60040</td>\n",
       "      <td>105079</td>\n",
       "      <td>19068</td>\n",
       "      <td>What is the chance that i damaged my brain smo...</td>\n",
       "      <td>Has anyone ever died from smoking marijuana?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134252</td>\n",
       "      <td>14287</td>\n",
       "      <td>5883</td>\n",
       "      <td>Do Quora users still see questions that are ma...</td>\n",
       "      <td>What happens to questions marked as needing im...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>176265</td>\n",
       "      <td>271238</td>\n",
       "      <td>271239</td>\n",
       "      <td>Which politcal party would you vote for in 201...</td>\n",
       "      <td>How many minimum marks required for getting ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>129602</td>\n",
       "      <td>62620</td>\n",
       "      <td>197112</td>\n",
       "      <td>How much does it cost to get a personal traine...</td>\n",
       "      <td>How much does a day pass for LA Fitness cost?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202449</td>\n",
       "      <td>304753</td>\n",
       "      <td>200833</td>\n",
       "      <td>What is the evolutionary significance of meter...</td>\n",
       "      <td>What is the evolutionary significance of gover...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>377576</td>\n",
       "      <td>508889</td>\n",
       "      <td>508890</td>\n",
       "      <td>Where I download Bollywood movies?</td>\n",
       "      <td>From where to download Bollywood movies?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>288362</td>\n",
       "      <td>264458</td>\n",
       "      <td>409304</td>\n",
       "      <td>What are the essential things to carry while t...</td>\n",
       "      <td>What are all things I should not carry to Dubai?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>222625</td>\n",
       "      <td>330234</td>\n",
       "      <td>330235</td>\n",
       "      <td>What is signed char and unsigned char exactly?</td>\n",
       "      <td>How do you check if the char is signed or unsi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>201499</td>\n",
       "      <td>303523</td>\n",
       "      <td>303524</td>\n",
       "      <td>How do I write Muhammed Shamnu in Arabic?</td>\n",
       "      <td>How can I write \"Let it be\" in arabic?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>262197</td>\n",
       "      <td>201705</td>\n",
       "      <td>378466</td>\n",
       "      <td>What are the highest and average packages offe...</td>\n",
       "      <td>Are the PSUs better than MNCs?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>352414</td>\n",
       "      <td>17944</td>\n",
       "      <td>283109</td>\n",
       "      <td>Is hypnotism real?</td>\n",
       "      <td>Is Hypnotism real? If so, How to hypnotise som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47663</td>\n",
       "      <td>85069</td>\n",
       "      <td>85070</td>\n",
       "      <td>Why do some people say vaccines cause autism?</td>\n",
       "      <td>Do vaccines really cause autism? If so,which o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43457</td>\n",
       "      <td>78162</td>\n",
       "      <td>78163</td>\n",
       "      <td>Do rich people worry about anything?</td>\n",
       "      <td>What do wealthy people worry about?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>362478</td>\n",
       "      <td>64154</td>\n",
       "      <td>46266</td>\n",
       "      <td>How is it possible that whenever you call some...</td>\n",
       "      <td>Can someone who blocked my number see the text...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>352026</td>\n",
       "      <td>480909</td>\n",
       "      <td>480910</td>\n",
       "      <td>What are the current issues impacting on opera...</td>\n",
       "      <td>What are current issues in operations management?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23951</td>\n",
       "      <td>44818</td>\n",
       "      <td>44819</td>\n",
       "      <td>What is the salary of Uday Shankar, Star Tv In...</td>\n",
       "      <td>How much is the salary of STAR TV INDIA ceo Ud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>104288</td>\n",
       "      <td>90646</td>\n",
       "      <td>172208</td>\n",
       "      <td>What can you eat on a 500 calories per day diet?</td>\n",
       "      <td>How dangerous is the 500 calories-a-day diet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>210737</td>\n",
       "      <td>315375</td>\n",
       "      <td>76438</td>\n",
       "      <td>If Apple has the age of everyone with an Apple...</td>\n",
       "      <td>Does Apple allow app makers to pay users to do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67805</td>\n",
       "      <td>117285</td>\n",
       "      <td>117286</td>\n",
       "      <td>Which are the best colleges for mechanical eng...</td>\n",
       "      <td>Which is the best college for mechanical engin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>328428</td>\n",
       "      <td>86511</td>\n",
       "      <td>454958</td>\n",
       "      <td>Can I lose weight by masturbating?</td>\n",
       "      <td>Why am I not losing weight?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>275753</td>\n",
       "      <td>198715</td>\n",
       "      <td>132359</td>\n",
       "      <td>How do I convince parents to take me to the do...</td>\n",
       "      <td>How can I convince my parents to take me to th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>101871</td>\n",
       "      <td>168672</td>\n",
       "      <td>168673</td>\n",
       "      <td>How do I start a social network?</td>\n",
       "      <td>How do you start a social network?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>218894</td>\n",
       "      <td>48319</td>\n",
       "      <td>98181</td>\n",
       "      <td>What are some tips on how to increase rank in ...</td>\n",
       "      <td>How can I optimize my website to rank at the t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23015</td>\n",
       "      <td>43147</td>\n",
       "      <td>43148</td>\n",
       "      <td>Which is the best GATE coaching for CSE in ind...</td>\n",
       "      <td>Which one is the best GATE coaching for CSE?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>123974</td>\n",
       "      <td>200298</td>\n",
       "      <td>200299</td>\n",
       "      <td>Who is your favourite person in your family an...</td>\n",
       "      <td>Who is your favorite person in family? And why?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323402</th>\n",
       "      <td>30255</td>\n",
       "      <td>55902</td>\n",
       "      <td>55903</td>\n",
       "      <td>Why is Merlot a burgundy wine?</td>\n",
       "      <td>Where can I get a music system fitted in my ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323403</th>\n",
       "      <td>383572</td>\n",
       "      <td>515527</td>\n",
       "      <td>515528</td>\n",
       "      <td>What are some best suited working model of phy...</td>\n",
       "      <td>What are some suggestions on the class 12th ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323404</th>\n",
       "      <td>214338</td>\n",
       "      <td>319890</td>\n",
       "      <td>319891</td>\n",
       "      <td>Is there a way to remove a stick and poke tattoo?</td>\n",
       "      <td>Should I remove my tattoo for my boyfriend?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323405</th>\n",
       "      <td>245799</td>\n",
       "      <td>172896</td>\n",
       "      <td>358716</td>\n",
       "      <td>How would you compare the United States' eutha...</td>\n",
       "      <td>How would you compare the United States' eutha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323406</th>\n",
       "      <td>157698</td>\n",
       "      <td>37961</td>\n",
       "      <td>19286</td>\n",
       "      <td>How can I improve my English writing skills?</td>\n",
       "      <td>How can I improve my English grammar?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323407</th>\n",
       "      <td>307651</td>\n",
       "      <td>431358</td>\n",
       "      <td>431359</td>\n",
       "      <td>What will be the effect of BREXIT on EU countr...</td>\n",
       "      <td>What effect will Brexit have on the EU?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323408</th>\n",
       "      <td>335665</td>\n",
       "      <td>462935</td>\n",
       "      <td>462936</td>\n",
       "      <td>How do I live a peaceful life?</td>\n",
       "      <td>What is the easiest way to live a life?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323409</th>\n",
       "      <td>345156</td>\n",
       "      <td>104333</td>\n",
       "      <td>473446</td>\n",
       "      <td>Why can't you delete your own questions on Quora?</td>\n",
       "      <td>Why did Quora deleted my answer that I posted ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323410</th>\n",
       "      <td>130256</td>\n",
       "      <td>209079</td>\n",
       "      <td>84700</td>\n",
       "      <td>Can you get pregnant 9 days before your period...</td>\n",
       "      <td>When can women get pregnant in the menstrual c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323411</th>\n",
       "      <td>227748</td>\n",
       "      <td>27341</td>\n",
       "      <td>201197</td>\n",
       "      <td>What does Hillary Clinton plan on doing during...</td>\n",
       "      <td>What will Hillary prioritize in her first hund...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323412</th>\n",
       "      <td>35662</td>\n",
       "      <td>65135</td>\n",
       "      <td>65136</td>\n",
       "      <td>What is the difference between oxidation and r...</td>\n",
       "      <td>What is difference between oxidation and reduc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323413</th>\n",
       "      <td>232857</td>\n",
       "      <td>342961</td>\n",
       "      <td>342962</td>\n",
       "      <td>What are project ideas for information technol...</td>\n",
       "      <td>What are some project ideas for information te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323414</th>\n",
       "      <td>196719</td>\n",
       "      <td>297478</td>\n",
       "      <td>297479</td>\n",
       "      <td>What are some creative butter recipes for popc...</td>\n",
       "      <td>Is there any way or chance I can make a guy in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323415</th>\n",
       "      <td>164782</td>\n",
       "      <td>2929</td>\n",
       "      <td>49460</td>\n",
       "      <td>What happens to a question on Quora if it is m...</td>\n",
       "      <td>What did I do wrong when my question is marked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323416</th>\n",
       "      <td>71200</td>\n",
       "      <td>27000</td>\n",
       "      <td>122569</td>\n",
       "      <td>Does the president have to pay for any utility...</td>\n",
       "      <td>Does Congress pay the rent and the utilities f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323417</th>\n",
       "      <td>118857</td>\n",
       "      <td>4306</td>\n",
       "      <td>39500</td>\n",
       "      <td>How should I start learning Python?</td>\n",
       "      <td>How do I start learning python web programming...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323418</th>\n",
       "      <td>249903</td>\n",
       "      <td>339589</td>\n",
       "      <td>363647</td>\n",
       "      <td>How many wives should I have?</td>\n",
       "      <td>How many wives spank their husband? to make re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323419</th>\n",
       "      <td>65632</td>\n",
       "      <td>113871</td>\n",
       "      <td>113872</td>\n",
       "      <td>Who invented the numbers 0-9?</td>\n",
       "      <td>What is the Mean deviation of the numbers 0.5,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323420</th>\n",
       "      <td>153313</td>\n",
       "      <td>240676</td>\n",
       "      <td>240677</td>\n",
       "      <td>Which instrument is in demand for college?</td>\n",
       "      <td>Which Ivy League School's economics or MBA pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323421</th>\n",
       "      <td>220374</td>\n",
       "      <td>327458</td>\n",
       "      <td>327459</td>\n",
       "      <td>What is the best country or countries to be a ...</td>\n",
       "      <td>Why does Thailand seem to have a disproportion...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323422</th>\n",
       "      <td>46203</td>\n",
       "      <td>82673</td>\n",
       "      <td>82674</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Why do I scratch/cut myself?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323423</th>\n",
       "      <td>129130</td>\n",
       "      <td>164741</td>\n",
       "      <td>207517</td>\n",
       "      <td>What are some synonyms for \"looking forward to\"?</td>\n",
       "      <td>I am looking forward to meet you is it correct?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323424</th>\n",
       "      <td>146449</td>\n",
       "      <td>72165</td>\n",
       "      <td>206317</td>\n",
       "      <td>Should I go to the gym in the morning or evening?</td>\n",
       "      <td>Which is the best time to go to the gym, morni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323425</th>\n",
       "      <td>194278</td>\n",
       "      <td>187966</td>\n",
       "      <td>294443</td>\n",
       "      <td>How long would it take to crack the WWII enigm...</td>\n",
       "      <td>How fast can the enigma code be cracked with t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323426</th>\n",
       "      <td>382050</td>\n",
       "      <td>89997</td>\n",
       "      <td>54533</td>\n",
       "      <td>Which are the top places to visit in Kerala?</td>\n",
       "      <td>Which places should be visited while travellin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323432 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "5        60040  105079   19068   \n",
       "6       134252   14287    5883   \n",
       "7       176265  271238  271239   \n",
       "8       129602   62620  197112   \n",
       "9       202449  304753  200833   \n",
       "10      377576  508889  508890   \n",
       "11      288362  264458  409304   \n",
       "12      222625  330234  330235   \n",
       "13      201499  303523  303524   \n",
       "14      262197  201705  378466   \n",
       "15      352414   17944  283109   \n",
       "16       47663   85069   85070   \n",
       "17       43457   78162   78163   \n",
       "18      362478   64154   46266   \n",
       "19      352026  480909  480910   \n",
       "20       23951   44818   44819   \n",
       "21      104288   90646  172208   \n",
       "22      210737  315375   76438   \n",
       "23       67805  117285  117286   \n",
       "24      328428   86511  454958   \n",
       "25      275753  198715  132359   \n",
       "26      101871  168672  168673   \n",
       "27      218894   48319   98181   \n",
       "28       23015   43147   43148   \n",
       "29      123974  200298  200299   \n",
       "...        ...     ...     ...   \n",
       "323402   30255   55902   55903   \n",
       "323403  383572  515527  515528   \n",
       "323404  214338  319890  319891   \n",
       "323405  245799  172896  358716   \n",
       "323406  157698   37961   19286   \n",
       "323407  307651  431358  431359   \n",
       "323408  335665  462935  462936   \n",
       "323409  345156  104333  473446   \n",
       "323410  130256  209079   84700   \n",
       "323411  227748   27341  201197   \n",
       "323412   35662   65135   65136   \n",
       "323413  232857  342961  342962   \n",
       "323414  196719  297478  297479   \n",
       "323415  164782    2929   49460   \n",
       "323416   71200   27000  122569   \n",
       "323417  118857    4306   39500   \n",
       "323418  249903  339589  363647   \n",
       "323419   65632  113871  113872   \n",
       "323420  153313  240676  240677   \n",
       "323421  220374  327458  327459   \n",
       "323422   46203   82673   82674   \n",
       "323423  129130  164741  207517   \n",
       "323424  146449   72165  206317   \n",
       "323425  194278  187966  294443   \n",
       "323426  382050   89997   54533   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "5       What is the chance that i damaged my brain smo...   \n",
       "6       Do Quora users still see questions that are ma...   \n",
       "7       Which politcal party would you vote for in 201...   \n",
       "8       How much does it cost to get a personal traine...   \n",
       "9       What is the evolutionary significance of meter...   \n",
       "10                     Where I download Bollywood movies?   \n",
       "11      What are the essential things to carry while t...   \n",
       "12         What is signed char and unsigned char exactly?   \n",
       "13              How do I write Muhammed Shamnu in Arabic?   \n",
       "14      What are the highest and average packages offe...   \n",
       "15                                     Is hypnotism real?   \n",
       "16          Why do some people say vaccines cause autism?   \n",
       "17                   Do rich people worry about anything?   \n",
       "18      How is it possible that whenever you call some...   \n",
       "19      What are the current issues impacting on opera...   \n",
       "20      What is the salary of Uday Shankar, Star Tv In...   \n",
       "21       What can you eat on a 500 calories per day diet?   \n",
       "22      If Apple has the age of everyone with an Apple...   \n",
       "23      Which are the best colleges for mechanical eng...   \n",
       "24                     Can I lose weight by masturbating?   \n",
       "25      How do I convince parents to take me to the do...   \n",
       "26                       How do I start a social network?   \n",
       "27      What are some tips on how to increase rank in ...   \n",
       "28      Which is the best GATE coaching for CSE in ind...   \n",
       "29      Who is your favourite person in your family an...   \n",
       "...                                                   ...   \n",
       "323402                     Why is Merlot a burgundy wine?   \n",
       "323403  What are some best suited working model of phy...   \n",
       "323404  Is there a way to remove a stick and poke tattoo?   \n",
       "323405  How would you compare the United States' eutha...   \n",
       "323406       How can I improve my English writing skills?   \n",
       "323407  What will be the effect of BREXIT on EU countr...   \n",
       "323408                     How do I live a peaceful life?   \n",
       "323409  Why can't you delete your own questions on Quora?   \n",
       "323410  Can you get pregnant 9 days before your period...   \n",
       "323411  What does Hillary Clinton plan on doing during...   \n",
       "323412  What is the difference between oxidation and r...   \n",
       "323413  What are project ideas for information technol...   \n",
       "323414  What are some creative butter recipes for popc...   \n",
       "323415  What happens to a question on Quora if it is m...   \n",
       "323416  Does the president have to pay for any utility...   \n",
       "323417                How should I start learning Python?   \n",
       "323418                      How many wives should I have?   \n",
       "323419                      Who invented the numbers 0-9?   \n",
       "323420         Which instrument is in demand for college?   \n",
       "323421  What is the best country or countries to be a ...   \n",
       "323422                      How can I smartly cut myself?   \n",
       "323423   What are some synonyms for \"looking forward to\"?   \n",
       "323424  Should I go to the gym in the morning or evening?   \n",
       "323425  How long would it take to crack the WWII enigm...   \n",
       "323426       Which are the top places to visit in Kerala?   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       Why do I get bored with things so quickly and ...             1  \n",
       "1       How do I study for Honeywell company recruitme...             1  \n",
       "2          Why is Quora not using reliable search engine?             0  \n",
       "3       Can someone who thinks about suicide for 7 yea...             0  \n",
       "4            Can one tell who viewed my Instagram videos?             1  \n",
       "5            Has anyone ever died from smoking marijuana?             0  \n",
       "6       What happens to questions marked as needing im...             1  \n",
       "7       How many minimum marks required for getting ad...             0  \n",
       "8           How much does a day pass for LA Fitness cost?             0  \n",
       "9       What is the evolutionary significance of gover...             0  \n",
       "10               From where to download Bollywood movies?             1  \n",
       "11       What are all things I should not carry to Dubai?             0  \n",
       "12      How do you check if the char is signed or unsi...             0  \n",
       "13                 How can I write \"Let it be\" in arabic?             0  \n",
       "14                         Are the PSUs better than MNCs?             0  \n",
       "15      Is Hypnotism real? If so, How to hypnotise som...             1  \n",
       "16      Do vaccines really cause autism? If so,which o...             1  \n",
       "17                    What do wealthy people worry about?             1  \n",
       "18      Can someone who blocked my number see the text...             0  \n",
       "19      What are current issues in operations management?             1  \n",
       "20      How much is the salary of STAR TV INDIA ceo Ud...             0  \n",
       "21          How dangerous is the 500 calories-a-day diet?             0  \n",
       "22      Does Apple allow app makers to pay users to do...             0  \n",
       "23      Which is the best college for mechanical engin...             0  \n",
       "24                            Why am I not losing weight?             0  \n",
       "25      How can I convince my parents to take me to th...             1  \n",
       "26                     How do you start a social network?             0  \n",
       "27      How can I optimize my website to rank at the t...             1  \n",
       "28           Which one is the best GATE coaching for CSE?             0  \n",
       "29        Who is your favorite person in family? And why?             1  \n",
       "...                                                   ...           ...  \n",
       "323402  Where can I get a music system fitted in my ba...             0  \n",
       "323403  What are some suggestions on the class 12th ph...             0  \n",
       "323404        Should I remove my tattoo for my boyfriend?             0  \n",
       "323405  How would you compare the United States' eutha...             0  \n",
       "323406              How can I improve my English grammar?             0  \n",
       "323407            What effect will Brexit have on the EU?             1  \n",
       "323408            What is the easiest way to live a life?             0  \n",
       "323409  Why did Quora deleted my answer that I posted ...             0  \n",
       "323410  When can women get pregnant in the menstrual c...             0  \n",
       "323411  What will Hillary prioritize in her first hund...             1  \n",
       "323412  What is difference between oxidation and reduc...             1  \n",
       "323413  What are some project ideas for information te...             0  \n",
       "323414  Is there any way or chance I can make a guy in...             0  \n",
       "323415  What did I do wrong when my question is marked...             1  \n",
       "323416  Does Congress pay the rent and the utilities f...             0  \n",
       "323417  How do I start learning python web programming...             1  \n",
       "323418  How many wives spank their husband? to make re...             0  \n",
       "323419  What is the Mean deviation of the numbers 0.5,...             0  \n",
       "323420  Which Ivy League School's economics or MBA pro...             0  \n",
       "323421  Why does Thailand seem to have a disproportion...             0  \n",
       "323422                       Why do I scratch/cut myself?             0  \n",
       "323423    I am looking forward to meet you is it correct?             0  \n",
       "323424  Which is the best time to go to the gym, morni...             1  \n",
       "323425  How fast can the enigma code be cracked with t...             1  \n",
       "323426  Which places should be visited while travellin...             1  \n",
       "323427         Is it OK to use your phone while charging?             0  \n",
       "323428            Can dogs understand the human language?             0  \n",
       "323429                 What's your favourite skin lotion?             1  \n",
       "323430   What should I do to become a hedge fund manager?             1  \n",
       "323431  We've thought about evil geniuses ruling the w...             0  \n",
       "\n",
       "[323432 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "available_data = pd.read_csv(\"quora_train_data.csv\")\n",
    "test_df = pd.read_csv(\"quora_test_data.csv\")\n",
    "available_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:23.479283Z",
     "start_time": "2020-03-14T14:44:23.388271Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "train_df, val_df = sklearn.model_selection.train_test_split(available_data, test_size=0.1, random_state=123)\n",
    "\n",
    "train_df.to_csv('train_df.csv')\n",
    "test_df.to_csv('test_df.csv')\n",
    "val_df.to_csv('val_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions for some of the models.The first functions are meant to extract, given a vectorizer, the matrix of features for the classifier. The two last functions are used to identify the errors that a classifier is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_list(q1,q2,count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1)\n",
    "    q2_mat = count_vectorizer.transform(q2)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "    \n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat], format=\"csr\")\n",
    "            \n",
    "    return X_q1q2\n",
    "\n",
    "def get_mistakes(clf, X_q1q2, y):\n",
    "    \"\"\"\n",
    "    Returns two lists: one containing the indices of the predictions that are not correct\n",
    "    and another one containing the predictions\n",
    "    \"\"\"\n",
    "    predictions        = clf.predict(X_q1q2).round(0).astype(int)\n",
    "    incorrect_preds    = predictions != y\n",
    "    incorrect_indices, = np.where(incorrect_preds)\n",
    "    incorrect_indices2 = [x for x in  range(len(incorrect_preds)) if incorrect_preds[x] ==True]\n",
    "    incorrect_indices3 = np.arange(len(incorrect_preds))[incorrect_preds]        \n",
    "    \n",
    "    if np.sum(incorrect_preds)==0:\n",
    "        print(\"no mistakes in this df\")\n",
    "    else:\n",
    "        return incorrect_indices, predictions\n",
    "    \n",
    "def print_mistake_k(k, dataset, mistake_indices, predictions):\n",
    "    \"\"\"\n",
    "    Auxiliar function to print the k-th mistake made in the prediction\n",
    "    \"\"\"\n",
    "    print(\"Original q1: \", train_df.iloc[mistake_indices[k]].question1, \" Treated q1: \", dataset[mistake_indices[k]])\n",
    "    print(\"Original q2: \", train_df.iloc[mistake_indices[k]].question2, \" Treated q2: \", dataset[mistake_indices[k]+train_df.shape[0]])\n",
    "    print(\"true class:\", train_df.iloc[mistake_indices[k]].is_duplicate)\n",
    "    print(\"prediction:\", predictions[mistake_indices[k]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first naive model was proposed in class: pass the text through the vectorizers and use the returned matrix as the matrix of features. We saw that the classifier wrongly classified some questions with spelling mistakes. For example, the classifiera would identify as different questions those who were written like \"whats\" from those who were written like \"what's\". \n",
    "\n",
    "We thought that this problem may be common with any model that we try to train, so the first thing we propose to do is correcting the spelling mistakes. We propose to remove \"'s\", change the negatives \"'t\" for \"not\" as well as the plurals \"'re\" for \"are\", remove symbols and points. Then, we implemented a spell checking function using the edit distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is necessary in order to obtain a list of documents. This is the structure we usually want, at least for the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all elements of the documents into strings \n",
    "q1_train_raw =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train_raw =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_val_raw  =  cast_list_as_strings(list(val_df[\"question1\"]))\n",
    "q2_val_raw  =  cast_list_as_strings(list(val_df[\"question2\"]))\n",
    "q1_test_raw  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test_raw  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "\n",
    "all_questions_raw = q1_train_raw + q2_train_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes a while to compute, so we write the result in a text file. We DON'T NEED TO RUN THE FOLLOWING CELLS, they are here to illustrate the process that we have done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Load the words of our corpus\n",
    "nltk.download('words')\n",
    "words = nltk.corpus.words.words()\n",
    "words.extend(['online', 'Quora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the spelling correction object (it will create the BK tree)\n",
    "spelling_c = Spelling_Correction_c( words, tol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why do sasuke have only two car in ex girlfriend the movie'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def document_cleaner_spelling(spelling, text):\n",
    "    clean_doc_pattern = re.compile( r\"('\\w)|([^a-zA-Z0-9.])\") #Find words containing alphanumeric or points\n",
    "    q = re.sub('\\'s', '', text) #Remove 's\n",
    "    q = re.sub('\\'t', ' not', q) #Change 't for not'\n",
    "    q = re.sub('\\'re', ' are', q) #Change 're for are'\n",
    "    q = re.sub('[?%!@#$\\'\\\"\"]', '', q)#Remove symbols\n",
    "    q = re.sub('\\.\\s', ' ', q)#Remove points with a space afterwards\n",
    "    clean_q = clean_doc_pattern.sub(\" \", q)\n",
    "    correct_q = spelling_c.correct_text(clean_q)#Clean spelling mistakes\n",
    "    return correct_q\n",
    "\n",
    "document_cleaner_spelling(spelling_c, \"Why does Sasuke have only two cars in ex-girlfriend The Movie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q1_train))\n",
    "q1_train_cleaned = []\n",
    "for quest in q1_train:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q1_train_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q1_train_cleaned.txt', 'w') as f:\n",
    "    for item in q1_train_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q2_train))\n",
    "q2_train_cleaned = []\n",
    "for quest in q2_train:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q2_train_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q2_train_cleaned.txt', 'w') as f:\n",
    "    for item in q2_train_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q1_val_raw))\n",
    "q1_val_cleaned = []\n",
    "for quest in q1_val_raw:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q1_val_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q1_val_cleaned.txt', 'w') as f:\n",
    "    for item in q1_val_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q2_val_raw))\n",
    "q2_val_cleaned = []\n",
    "for quest in q2_val_raw:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q2_val_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q2_val_cleaned.txt', 'w') as f:\n",
    "    for item in q2_val_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q1_test))\n",
    "q1_test_cleaned = []\n",
    "for quest in q1_test:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q1_test_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q1_test_cleaned.txt', 'w') as f:\n",
    "    for item in q1_test_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "print(len(q2_test))\n",
    "q2_test_cleaned = []\n",
    "for quest in q2_test:\n",
    "    #print(i)\n",
    "    quest_cl = document_cleaner_spelling(spelling_c,quest)\n",
    "    q2_test_cleaned.append(quest_cl)\n",
    "    i+=1\n",
    "\n",
    "with open('cleaned_data/q2_test_cleaned.txt', 'w') as f:\n",
    "    for item in q2_test_cleaned:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/train_labels.txt', 'w') as f:\n",
    "    for item in train_df['is_duplicate'].values:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('cleaned_data/val_labels.txt', 'w') as f:\n",
    "    for item in val_df['is_duplicate'].values:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open('cleaned_data/test_labels.txt', 'w') as f:\n",
    "    for item in test_df['is_duplicate'].values:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run from here\n",
    "\n",
    "We run these cells to obtain the results of the  cleaned text fromthe txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_train_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_train = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_val_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_val = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q1_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q1_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/q2_test_cleaned.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "q2_test = [x.strip() for x in content] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions= q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data/train_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "train_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/val_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "val_labels = [int(x.strip()) for x in content] \n",
    "\n",
    "with open('cleaned_data/test_labels.txt') as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "test_labels = [int(x.strip()) for x in content] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST MODEL: NAIVE CLASSIFIER WITH SPELL CHECKING\n",
    "For the first model, we just wanted to see what difference did the spellchecking do. So, did we improve the results? Did we improve the results as expected? If so, what mistakes is our model doing now?\n",
    "\n",
    "We will do this checking for bot the CountVectorizer and the TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:21.407870Z",
     "start_time": "2020-03-14T14:45:21.401885Z"
    }
   },
   "outputs": [],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3), max_df = 0.99, min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:46:44.793780Z",
     "start_time": "2020-03-14T14:45:42.528916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)\", document_cleaner_func=None,\n",
       "        dtype=<class 'numpy.float32'>, max_df=0.99, min_df=5,\n",
       "        min_word_counts=1, ngram_range=(1, 3),\n",
       "        stop_words={'for', 'when', 'm', 'ma', \"wasn't\", \"couldn't\", 'yourselves', 'wasn', 'too', 'those', \"haven't\", 'couldn', \"weren't\", 'your', 're', 'no', 'is', 'other', 'theirs', 'who', 'very', 'itself', 'while', 'that', 'off', 'll', 'am', 'if', 'until', 'them', 'which', 'during', 'wouldn', \"you'll\", 'b...', \"mustn't\", 'didn', 'only', 'their', 'of', 'above', 'out', 'mightn', 'aren', 's', \"you're\", 'was'},\n",
       "        token_cleaner_func=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer_func=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer.dump(\"models/CountVectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words={'for', 'when', 'm', 'ma', \"wasn't\", \"couldn't\", 'yourselves', 'wasn', 'too', 'those', \"haven't\", 'couldn', \"weren't\", 'your', 're', 'no', 'is', 'other', 'theirs', 'who', 'very', 'itself', 'while', 'that', 'off', 'll', 'am', 'if', 'until', 'them', 'which', 'during', 'wouldn', \"you'll\", 'b...', \"mustn't\", 'didn', 'only', 'their', 'of', 'above', 'out', 'mightn', 'aren', 's', \"you're\", 'was'},\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will the result obtained with our implementation of the CountVectorizer with the result obtained using the sklearn version of the CountVectorizer. One of the objectives of this deliverable was to implement and understend how the vectorizers worked, so we set as an objective to obtain the same as the sklearn vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our CountVectorizer:  (291088, 285364) (291088, 6)\n",
      "With sklearn CountVectorizer:  (291088, 285364) (291088, 6)\n"
     ]
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "print(\"With our CountVectorizer: \", X_tr_q1q2.shape, train_df.shape)\n",
    "\n",
    "X_tr_q1q2_sk = get_features_from_list(q1_train, q2_train,countvect_sk)\n",
    "print(\"With sklearn CountVectorizer: \", X_tr_q1q2_sk.shape, train_df.shape)\n",
    "\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, CountVectorizer)\n",
    "\n",
    "X_val_q1q2_sk  = get_features_from_list(q1_val, q2_val, countvect_sk)\n",
    "\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)\n",
    "\n",
    "X_te_q1q2_sk  = get_features_from_list(q1_test, q2_test, countvect_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using our implementation of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873892706438838\n",
      "Result on validation:  0.7564062563763118\n",
      "Result on test:  0.7533668105876712\n"
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, train_labels)\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the parameters of the logistic regression in a JSON file, so that we can import them later easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logistic(logistic, filename):\n",
    "    logistic_params = {}\n",
    "    logistic_params['coef_'] = logistic.coef_.tolist()\n",
    "    logistic_params['classes_'] = logistic.classes_.tolist()\n",
    "    logistic_params['intercept_'] = logistic.intercept_.tolist()\n",
    "\n",
    "    #Dump the parameters to a JSON file\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(logistic_params, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic, 'models/logistic.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8873667596649119\n",
      "Result on validation:  0.7563817405636126\n",
      "Result on test:  0.7533304796390545\n"
     ]
    }
   ],
   "source": [
    "logistic_sk = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic_sk.fit(X_tr_q1q2_sk, train_labels)\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic_sk, 'models/logistic_sk.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "We wanted to identify the mistakes that the classifier was doing in this case. We saw that the classifier was making mistages mainly for the following reasons:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.9074094431924367\n",
      "Accuracy on validation:  0.788647044274054\n",
      "Accuracy on test:  0.7851665883400529\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels==logistic.predict(X_tr_q1q2))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Why do men like women's feet?  Treated q1:  why do men like woman foot\n",
      "Original q2:  Why do men like womens feet?  Treated q2:  why do men like woman foot\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=False,\n",
       "        stop_words={'y', 'shan', 'hers', 'by', 'against', 'a', 'here', 'down', 'from', 'now', \"wouldn't\", 'their', 'off', 'an', 'once', 'ourselves', 'mustn', 'there', 'those', 'after', \"isn't\", 'does', 'where', \"you're\", 'did', 'both', 've', \"hasn't\", \"she's\", 'we', 'very', 'each', 'of', 'will', 'can', 'him..., 'few', 'my', 'her', 'itself', 'have', 'then', \"aren't\", 'that', 'on', 'between', 'these', 'doesn'},\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=False,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tf(stop_words = set(stopwords.words('english')), ngram_range=(1,3), max_df = 0.4, min_df = 5)\n",
    "tfidf_vectorizer.fit(all_questions)\n",
    "tfidf_vectorizer.dump(\"models/TfIdfVectorizer.pkl\")\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that additionally, we want to compare our result with that given by the implementation of sklearn of the TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With our TfIdf Vectorizer: (291088, 285364) (291088, 6)\n",
      "With sklearn TfIdf Vectorizer: (291088, 285364) (291088, 6)\n"
     ]
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_tr_q1q2_sk = get_features_from_list(q1_train, q2_train, tfidf_sk)\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, tfidf_vectorizer)\n",
    "X_val_q1q2_sk  = get_features_from_list(q1_val, q2_val, tfidf_sk)\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)\n",
    "X_te_q1q2_sk  = get_features_from_list(q1_test, q2_test, tfidf_sk)\n",
    "\n",
    "print(\"With our TfIdf Vectorizer:\", X_tr_q1q2.shape, train_df.shape)\n",
    "print(\"With sklearn TfIdf Vectorizer:\", X_tr_q1q2_sk.shape, train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using our implementation of TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8303725534934316\n",
      "Result on validation:  0.7532320054109183\n",
      "Result on test:  0.7486764630961492\n"
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, train_labels)\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(X_tr_q1q2)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic, 'models/logistic_tfidf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result using sklearn implementation of TfIdf Vectorizer. Note that the result is different because the formula that sklearn uses is different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.8039620081117552\n",
      "Result on validation:  0.7444771581829623\n",
      "Result on test:  0.7401463445535814\n"
     ]
    }
   ],
   "source": [
    "logistic_sk = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic_sk.fit(X_tr_q1q2_sk, train_labels)\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic_sk.predict(X_tr_q1q2_sk)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic_sk.predict(X_val_q1q2_sk)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic_sk.predict(X_te_q1q2_sk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic_sk, 'models/logistic_tfidf_sk.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative information about the mistakes\n",
    "\n",
    "So, in this case, the mistakes are practically the same, hence:\n",
    "- The questions are the same, but the sentences have lots of different words.\n",
    "- The questions are the same, but one sentence is way larger than the other.\n",
    "- The questions are asking about the same thing but for different years, hence they must be classified as different.\n",
    "- One of the questions is a subset of the other. This mistake is the harder to solve because sometimes it is even debatable of the questions should be the same or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8615642005166823\n",
      "Accuracy on validation:  0.7928827603264902\n",
      "Accuracy on test:  0.7880729179549334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training: \", np.sum(train_labels ==logistic.predict(X_tr_q1q2))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels ==logistic.predict(X_val_q1q2))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==logistic.predict(X_te_q1q2))/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original q1:  Are Persians considered Caucasian?  Treated q1:  be Persian consider Caucasian\n",
      "Original q2:  Are Persians White?  Treated q2:  be Persian white\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "mistake_indices, predictions = get_mistakes(logistic, X_tr_q1q2, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND MODEL: NAIVE CLASSIFIER WITH EXTRA FEATURES\n",
    "\n",
    "Given the mistakes encountered in the previous model, we tried to code some extra features to tackle with those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to obtain the extra features.\n",
    "\n",
    "Here we give a list of extra features that we could add to the feature vector.\n",
    "\n",
    "1. Lenght of the question\n",
    "\n",
    "2. Is there a [math] tag? \n",
    "\n",
    "3. Is there a number in the question?\n",
    "\n",
    "4. Is it the same number in both questions? \n",
    "\n",
    "5. % of intersection words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qlength(questions):\n",
    "    qlen = []\n",
    "    for quest in questions:\n",
    "        clean_doc_pattern = re.compile( r\"('\\w)|([^a-zA-Z0-9.])\") #Find words containing alphanumeric or points\n",
    "        q = re.sub('\\'s', '', quest) #Remove 's\n",
    "        q = re.sub('\\'t', ' not', q) #Change 't for not'\n",
    "        q = re.sub('\\'re', ' are', q) #Change 're for are'\n",
    "        q = re.sub('[?%!@#$\\'\\\"\"]', '', q)#Remove symbols\n",
    "        q = re.sub('\\.\\s', ' ', q)#Remove points with a space afterwards\n",
    "        clean_q = clean_doc_pattern.sub(\" \", q)\n",
    "        qlen.append(len(re.findall(r\"(?u)\\b[\\w.,]+\\b\",q)))\n",
    "        \n",
    "    return np.array(qlen).reshape(-1,1)\n",
    "\n",
    "def is_math(questions):\n",
    "    math=[]\n",
    "    for quest in questions:\n",
    "        if '[math]' in quest:\n",
    "            math.append(1)\n",
    "        else:\n",
    "            math.append(0)\n",
    "    return np.array(math).reshape(-1,1)\n",
    "    \n",
    "def is_number(word):\n",
    "    try :  \n",
    "        w = float(word) \n",
    "        if(np.isnan(w)):\n",
    "            return 0\n",
    "        if(np.isinf(w)):\n",
    "            return 0\n",
    "        res = 1\n",
    "    except : \n",
    "        res = 0\n",
    "    return res    \n",
    "\n",
    "def has_numbers(questions):\n",
    "    num=np.zeros((len(questions)))\n",
    "    which_num = np.zeros((len(questions)))\n",
    "    i=0\n",
    "    for quest in questions:\n",
    "        for w in re.findall(r\"(?u)\\b[\\w.,]+\\b\",quest):\n",
    "            is_num = is_number(w)\n",
    "            if is_num==1:\n",
    "                num[i]=1\n",
    "                which_num[i]=float(w)\n",
    "                if(np.isnan(which_num[i])):\n",
    "                    print(which_num[i])\n",
    "                    print(float(w))\n",
    "                break\n",
    "        i+=1\n",
    "    return num.reshape(-1,1), which_num.reshape(-1,1)\n",
    "\n",
    "\n",
    "def is_different_number(which_num1, which_num2):\n",
    "    dif = which_num1 - which_num2\n",
    "    dif[dif>0]=1\n",
    "    return np.array(dif).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row, q1, q2, q_dict):\n",
    "    set1 = set(q_dict[q1[row]])\n",
    "    set2 = set(q_dict[q2[row]])\n",
    "    return(len(set1.intersection(set2))/len(set1.union(set2)))\n",
    "\n",
    "\n",
    "def intersection(q1_train, q2_train,q1_val,q2_val, q1_test, q2_test):\n",
    "    q1 = q1_train + q1_val +  q1_test\n",
    "    q2 = q2_train + q1_val + q2_test\n",
    "    q_dict = defaultdict(set)\n",
    "    for i in range(len(q1)):\n",
    "            q_dict[q1[i]].add(q2[i])\n",
    "            q_dict[q2[i]].add(q1[i])\n",
    "\n",
    "    intersect_train = []\n",
    "    intersect_test = []\n",
    "    intersect_val = []\n",
    "    for row in range(len(q1_train)):\n",
    "        intersect_train.append(q1_q2_intersect(row, q1_train, q2_train, q_dict))\n",
    "    \n",
    "    for row in range(len(q1_val)):\n",
    "        intersect_val.append(q1_q2_intersect(row, q1_val, q2_val, q_dict))\n",
    "        \n",
    "    for row in range(len(q1_test)):\n",
    "        intersect_test.append(q1_q2_intersect(row, q1_test, q2_test, q_dict))\n",
    "    \n",
    "    intersect_train = np.array(intersect_train).reshape(-1,1)\n",
    "    intersect_val = np.array(intersect_val).reshape(-1,1)\n",
    "    intersect_test = np.array(intersect_test).reshape(-1,1)\n",
    "    return intersect_train, intersect_val, intersect_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_train, intersect_val, intersect_test = intersection(q1_train, q2_train, q1_val, q2_val, q1_test, q2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndocs, nvars = X_tr_q1q2.shape\n",
    "nvars = (int)(nvars/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(Xq1,Xq2):\n",
    "    union = scipy.sum((Xq1!=0)+(Xq2!=0) ,axis=1)\n",
    "    union[union==0]=1\n",
    "    intersection = scipy.sum((Xq1!=0).multiply(Xq2!=0), axis=1)\n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_train = intersection(X_tr_q1q2[:,:nvars], X_tr_q1q2[:,nvars:])\n",
    "intersect_val = intersection(X_val_q1q2[:,:nvars], X_val_q1q2[:,nvars:])\n",
    "intersect_test = intersection(X_te_q1q2[:,:nvars], X_te_q1q2[:,nvars:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1_train, which_num1_train=  has_numbers(q1_train_raw)\n",
    "num2_train, which_num2_train =  has_numbers(q2_train_raw)\n",
    "dif_number_train = is_different_number(which_num1_train,which_num2_train)\n",
    "\n",
    "num1_val, which_num1_val=  has_numbers(q1_val_raw)\n",
    "num2_val, which_num2_val =  has_numbers(q2_val_raw)\n",
    "dif_number_val = is_different_number(which_num1_val,which_num2_val)\n",
    "\n",
    "num1_test, which_num1_test=  has_numbers(q1_test_raw)\n",
    "num2_test, which_num2_test =  has_numbers(q2_test_raw)\n",
    "dif_number_test = is_different_number(which_num1_test,which_num2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "math1_train = is_math(q1_train_raw)\n",
    "math2_train = is_math(q2_train_raw)\n",
    "\n",
    "math1_val = is_math(q1_val_raw)\n",
    "math2_val = is_math(q2_val_raw)\n",
    "\n",
    "math1_test = is_math(q1_test_raw)\n",
    "math2_test = is_math(q2_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1_train = get_qlength(q1_train_raw)\n",
    "len2_train = get_qlength(q2_train_raw)\n",
    "\n",
    "len1_val = get_qlength(q1_val_raw)\n",
    "len2_val = get_qlength(q2_val_raw)\n",
    "\n",
    "len1_test = get_qlength(q1_test_raw)\n",
    "len2_test = get_qlength(q2_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:32.452092Z",
     "start_time": "2020-03-14T14:44:32.447068Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)\n",
    "\n",
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since we have already check that our CountVectorizer yields the same result as the sklearn one, we will only use ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, CountVectorizer)\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following result. We see that the AUC has dropped a lot. We think that this may be due to the imbalance of the values of the different features, i.e., we are not normalizing the values of any of the features. We thought that it would be necessary to change the model, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(new_X_tr_q1q2, train_labels)\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2)))\n",
    "\n",
    "#val roc auc metrics\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic, 'models/logistic_extra_features.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.99, max_features=None,\n",
       "                min_df=5, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=False,\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "                use_idf=False, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tf(stop_words = set(stopwords.words('english')), ngram_range=(1,3), max_df = 0.4, min_df = 5)\n",
    "tfidf_vectorizer.fit(all_questions)\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will only run the code for our TfIdfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_val_q1q2  = get_features_from_list(q1_val, q2_val, tfidf_vectorizer)\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "print('initial shape', X_tr_q1q2.shape)\n",
    "\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very similar thing happens with the tfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on train:  0.4849733415832702\n",
      "Result on validation:  0.48544005699141674\n",
      "Result on test:  0.48445389374241093\n"
     ]
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(new_X_tr_q1q2, train_labels)\n",
    "\n",
    "print(\"Result on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = logistic.predict(new_X_tr_q1q2)))\n",
    "\n",
    "print(\"Result on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = logistic.predict(new_X_val_q1q2)))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"Result on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = logistic.predict(new_X_te_q1q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logistic(logistic, 'models/logistic_extra_features_tfidf.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD MODEL: XGBOOST\n",
    "\n",
    "Given all the previous results, a thing was clear: we needed to change the classifier. So our take was: combine everything we have done until now (text with the spell checking and the extra features) but with a more sophisticated model. We chose the XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:32.452092Z",
     "start_time": "2020-03-14T14:44:32.447068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ignasi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 227068)\n",
      "final shape (291088, 227076)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3), max_df = 0.99, min_df = 6)\n",
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)\n",
    "\n",
    "countvect_sk = sklearn.feature_extraction.text.CountVectorizer(stop_words = set(stopwords.words('english')),ngram_range=(1, 3),\n",
    "                                                              max_df = 0.99, min_df = 5)\n",
    "countvect_sk.fit(all_questions)\n",
    "\n",
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,CountVectorizer)\n",
    "X_val_q1q2 = get_features_from_list(q1_val, q2_val,CountVectorizer)\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, CountVectorizer)\n",
    "\n",
    "print('initial shape', X_tr_q1q2.shape)\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'auc'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "evallist = [(d_train, 'train'), (d_test, 'test'), (d_val, 'val')]\n",
    "\n",
    "num_iters = 50000\n",
    "\n",
    "xgb_count = xgb.train(params, d_train, num_iters, evallist, early_stopping_rounds=50, verbose_eval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_count.save_model('models/xgb_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8415084098279558\n",
      "Accuracy on validation:  0.8431548355181796\n",
      "Accuracy on test:  0.8305547997724406\n",
      "AUC on train:  0.9159446541394345\n",
      "AUC on validation:  0.8989376768402263\n",
      "AUC on test:  0.8939774284673486\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "pred_test = xgb_count.predict(d_test)\n",
    "pred_train = xgb_count.predict(d_train)\n",
    "pred_val = xgb_count.predict(d_val)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_count, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model - with TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape (291088, 285364)\n",
      "final shape (291088, 285372)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = tf(stop_words = set(stopwords.words('english')), ngram_range=(1,3), max_df = 0.4, min_df = 5)\n",
    "tfidf_vectorizer.fit(all_questions)\n",
    "\n",
    "tfidf_sk = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=False,\n",
    "                                                          stop_words = set(stopwords.words('english')),\n",
    "                                                          ngram_range=(1,3), max_df = 0.99, min_df = 5)\n",
    "tfidf_sk.fit(all_questions)\n",
    "\n",
    "X_tr_q1q2 = get_features_from_list(q1_train, q2_train,tfidf_vectorizer)\n",
    "X_val_q1q2 = get_features_from_list(q1_val, q2_val,tfidf_vectorizer)\n",
    "X_te_q1q2  = get_features_from_list(q1_test, q2_test, tfidf_vectorizer)\n",
    "\n",
    "print('initial shape', X_tr_q1q2.shape)\n",
    "new_X_tr_q1q2 = sparse.hstack((X_tr_q1q2,intersect_train, num1_train, num2_train,\n",
    "                               dif_number_train,math1_train,math2_train,len1_train, len2_train))\n",
    "new_X_val_q1q2 = sparse.hstack((X_val_q1q2,intersect_val, num1_val, num2_val,\n",
    "                               dif_number_val, math1_val,math2_val,len1_val, len2_val))\n",
    "new_X_te_q1q2 = sparse.hstack((X_te_q1q2,intersect_test, num1_test, num2_test,\n",
    "                               dif_number_test, math1_test,math2_test,len1_test, len2_test))\n",
    "print('final shape', new_X_tr_q1q2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.82935\ttest-auc:0.82916\tval-auc:0.83357\n",
      "Multiple eval metrics have been passed: 'val-auc' will be used for early stopping.\n",
      "\n",
      "Will train until val-auc hasn't improved in 50 rounds.\n",
      "[10]\ttrain-auc:0.83838\ttest-auc:0.83868\tval-auc:0.84108\n",
      "[20]\ttrain-auc:0.84049\ttest-auc:0.84095\tval-auc:0.84337\n",
      "[30]\ttrain-auc:0.84172\ttest-auc:0.84195\tval-auc:0.84498\n",
      "[40]\ttrain-auc:0.84266\ttest-auc:0.84295\tval-auc:0.84631\n",
      "[50]\ttrain-auc:0.84302\ttest-auc:0.84349\tval-auc:0.84688\n",
      "[60]\ttrain-auc:0.84321\ttest-auc:0.84352\tval-auc:0.84705\n",
      "[70]\ttrain-auc:0.84330\ttest-auc:0.84355\tval-auc:0.84719\n",
      "[80]\ttrain-auc:0.84556\ttest-auc:0.84539\tval-auc:0.84932\n",
      "[90]\ttrain-auc:0.84538\ttest-auc:0.84523\tval-auc:0.84918\n",
      "[100]\ttrain-auc:0.84570\ttest-auc:0.84549\tval-auc:0.84942\n",
      "[110]\ttrain-auc:0.84674\ttest-auc:0.84681\tval-auc:0.85088\n",
      "[120]\ttrain-auc:0.84779\ttest-auc:0.84781\tval-auc:0.85189\n",
      "[130]\ttrain-auc:0.84844\ttest-auc:0.84837\tval-auc:0.85277\n",
      "[140]\ttrain-auc:0.84923\ttest-auc:0.84915\tval-auc:0.85364\n",
      "[150]\ttrain-auc:0.84971\ttest-auc:0.84956\tval-auc:0.85399\n",
      "[160]\ttrain-auc:0.85054\ttest-auc:0.85035\tval-auc:0.85499\n",
      "[170]\ttrain-auc:0.85122\ttest-auc:0.85089\tval-auc:0.85570\n",
      "[180]\ttrain-auc:0.85204\ttest-auc:0.85161\tval-auc:0.85634\n",
      "[190]\ttrain-auc:0.85274\ttest-auc:0.85227\tval-auc:0.85697\n",
      "[200]\ttrain-auc:0.85382\ttest-auc:0.85330\tval-auc:0.85811\n",
      "[210]\ttrain-auc:0.85426\ttest-auc:0.85372\tval-auc:0.85881\n",
      "[220]\ttrain-auc:0.85473\ttest-auc:0.85414\tval-auc:0.85925\n",
      "[230]\ttrain-auc:0.85541\ttest-auc:0.85486\tval-auc:0.85977\n",
      "[240]\ttrain-auc:0.85603\ttest-auc:0.85548\tval-auc:0.86041\n",
      "[250]\ttrain-auc:0.85654\ttest-auc:0.85594\tval-auc:0.86098\n",
      "[260]\ttrain-auc:0.85702\ttest-auc:0.85638\tval-auc:0.86138\n",
      "[270]\ttrain-auc:0.85741\ttest-auc:0.85672\tval-auc:0.86170\n",
      "[280]\ttrain-auc:0.85779\ttest-auc:0.85700\tval-auc:0.86226\n",
      "[290]\ttrain-auc:0.85812\ttest-auc:0.85726\tval-auc:0.86240\n",
      "[300]\ttrain-auc:0.85859\ttest-auc:0.85759\tval-auc:0.86288\n",
      "[310]\ttrain-auc:0.85924\ttest-auc:0.85821\tval-auc:0.86345\n",
      "[320]\ttrain-auc:0.85948\ttest-auc:0.85835\tval-auc:0.86374\n",
      "[330]\ttrain-auc:0.85990\ttest-auc:0.85871\tval-auc:0.86420\n",
      "[340]\ttrain-auc:0.86016\ttest-auc:0.85896\tval-auc:0.86436\n",
      "[350]\ttrain-auc:0.86048\ttest-auc:0.85919\tval-auc:0.86456\n",
      "[360]\ttrain-auc:0.86078\ttest-auc:0.85945\tval-auc:0.86477\n",
      "[370]\ttrain-auc:0.86109\ttest-auc:0.85968\tval-auc:0.86494\n",
      "[380]\ttrain-auc:0.86155\ttest-auc:0.86012\tval-auc:0.86546\n",
      "[390]\ttrain-auc:0.86180\ttest-auc:0.86037\tval-auc:0.86569\n",
      "[400]\ttrain-auc:0.86219\ttest-auc:0.86068\tval-auc:0.86594\n",
      "[410]\ttrain-auc:0.86248\ttest-auc:0.86087\tval-auc:0.86606\n",
      "[420]\ttrain-auc:0.86272\ttest-auc:0.86109\tval-auc:0.86625\n",
      "[430]\ttrain-auc:0.86298\ttest-auc:0.86135\tval-auc:0.86647\n",
      "[440]\ttrain-auc:0.86328\ttest-auc:0.86161\tval-auc:0.86678\n",
      "[450]\ttrain-auc:0.86351\ttest-auc:0.86186\tval-auc:0.86699\n",
      "[460]\ttrain-auc:0.86373\ttest-auc:0.86198\tval-auc:0.86711\n",
      "[470]\ttrain-auc:0.86398\ttest-auc:0.86220\tval-auc:0.86732\n",
      "[480]\ttrain-auc:0.86417\ttest-auc:0.86238\tval-auc:0.86748\n",
      "[490]\ttrain-auc:0.86438\ttest-auc:0.86257\tval-auc:0.86769\n",
      "[500]\ttrain-auc:0.86456\ttest-auc:0.86273\tval-auc:0.86789\n",
      "[510]\ttrain-auc:0.86475\ttest-auc:0.86281\tval-auc:0.86792\n",
      "[520]\ttrain-auc:0.86505\ttest-auc:0.86311\tval-auc:0.86811\n",
      "[530]\ttrain-auc:0.86530\ttest-auc:0.86334\tval-auc:0.86831\n",
      "[540]\ttrain-auc:0.86550\ttest-auc:0.86348\tval-auc:0.86848\n",
      "[550]\ttrain-auc:0.86572\ttest-auc:0.86366\tval-auc:0.86867\n",
      "[560]\ttrain-auc:0.86602\ttest-auc:0.86392\tval-auc:0.86896\n",
      "[570]\ttrain-auc:0.86620\ttest-auc:0.86404\tval-auc:0.86908\n",
      "[580]\ttrain-auc:0.86644\ttest-auc:0.86421\tval-auc:0.86927\n",
      "[590]\ttrain-auc:0.86666\ttest-auc:0.86438\tval-auc:0.86950\n",
      "[600]\ttrain-auc:0.86684\ttest-auc:0.86452\tval-auc:0.86958\n",
      "[610]\ttrain-auc:0.86701\ttest-auc:0.86465\tval-auc:0.86976\n",
      "[620]\ttrain-auc:0.86722\ttest-auc:0.86482\tval-auc:0.86993\n",
      "[630]\ttrain-auc:0.86742\ttest-auc:0.86500\tval-auc:0.87006\n",
      "[640]\ttrain-auc:0.86768\ttest-auc:0.86522\tval-auc:0.87028\n",
      "[650]\ttrain-auc:0.86781\ttest-auc:0.86530\tval-auc:0.87035\n",
      "[660]\ttrain-auc:0.86812\ttest-auc:0.86557\tval-auc:0.87066\n",
      "[670]\ttrain-auc:0.86833\ttest-auc:0.86577\tval-auc:0.87084\n",
      "[680]\ttrain-auc:0.86854\ttest-auc:0.86593\tval-auc:0.87102\n",
      "[690]\ttrain-auc:0.86871\ttest-auc:0.86605\tval-auc:0.87123\n",
      "[700]\ttrain-auc:0.86885\ttest-auc:0.86617\tval-auc:0.87136\n",
      "[710]\ttrain-auc:0.86898\ttest-auc:0.86627\tval-auc:0.87145\n",
      "[720]\ttrain-auc:0.86919\ttest-auc:0.86643\tval-auc:0.87158\n",
      "[730]\ttrain-auc:0.86935\ttest-auc:0.86658\tval-auc:0.87172\n",
      "[740]\ttrain-auc:0.86949\ttest-auc:0.86669\tval-auc:0.87186\n",
      "[750]\ttrain-auc:0.86969\ttest-auc:0.86685\tval-auc:0.87203\n",
      "[760]\ttrain-auc:0.86993\ttest-auc:0.86704\tval-auc:0.87217\n",
      "[770]\ttrain-auc:0.87006\ttest-auc:0.86714\tval-auc:0.87230\n",
      "[780]\ttrain-auc:0.87027\ttest-auc:0.86731\tval-auc:0.87251\n",
      "[790]\ttrain-auc:0.87042\ttest-auc:0.86744\tval-auc:0.87265\n",
      "[800]\ttrain-auc:0.87057\ttest-auc:0.86756\tval-auc:0.87275\n",
      "[810]\ttrain-auc:0.87070\ttest-auc:0.86764\tval-auc:0.87279\n",
      "[820]\ttrain-auc:0.87088\ttest-auc:0.86777\tval-auc:0.87291\n",
      "[830]\ttrain-auc:0.87106\ttest-auc:0.86791\tval-auc:0.87301\n",
      "[840]\ttrain-auc:0.87118\ttest-auc:0.86801\tval-auc:0.87310\n",
      "[850]\ttrain-auc:0.87139\ttest-auc:0.86817\tval-auc:0.87327\n",
      "[860]\ttrain-auc:0.87151\ttest-auc:0.86827\tval-auc:0.87339\n",
      "[870]\ttrain-auc:0.87168\ttest-auc:0.86840\tval-auc:0.87345\n",
      "[880]\ttrain-auc:0.87182\ttest-auc:0.86850\tval-auc:0.87359\n",
      "[890]\ttrain-auc:0.87197\ttest-auc:0.86861\tval-auc:0.87364\n",
      "[900]\ttrain-auc:0.87215\ttest-auc:0.86873\tval-auc:0.87376\n",
      "[910]\ttrain-auc:0.87228\ttest-auc:0.86880\tval-auc:0.87384\n",
      "[920]\ttrain-auc:0.87242\ttest-auc:0.86891\tval-auc:0.87388\n",
      "[930]\ttrain-auc:0.87257\ttest-auc:0.86900\tval-auc:0.87396\n",
      "[940]\ttrain-auc:0.87271\ttest-auc:0.86906\tval-auc:0.87406\n",
      "[950]\ttrain-auc:0.87289\ttest-auc:0.86923\tval-auc:0.87423\n",
      "[960]\ttrain-auc:0.87302\ttest-auc:0.86935\tval-auc:0.87430\n",
      "[970]\ttrain-auc:0.87313\ttest-auc:0.86943\tval-auc:0.87435\n",
      "[980]\ttrain-auc:0.87326\ttest-auc:0.86951\tval-auc:0.87446\n",
      "[990]\ttrain-auc:0.87339\ttest-auc:0.86962\tval-auc:0.87454\n",
      "[1000]\ttrain-auc:0.87355\ttest-auc:0.86974\tval-auc:0.87461\n",
      "[1010]\ttrain-auc:0.87365\ttest-auc:0.86982\tval-auc:0.87466\n",
      "[1020]\ttrain-auc:0.87382\ttest-auc:0.86994\tval-auc:0.87483\n",
      "[1030]\ttrain-auc:0.87397\ttest-auc:0.87007\tval-auc:0.87495\n",
      "[1040]\ttrain-auc:0.87406\ttest-auc:0.87013\tval-auc:0.87501\n",
      "[1050]\ttrain-auc:0.87419\ttest-auc:0.87023\tval-auc:0.87514\n",
      "[1060]\ttrain-auc:0.87432\ttest-auc:0.87034\tval-auc:0.87524\n",
      "[1070]\ttrain-auc:0.87443\ttest-auc:0.87040\tval-auc:0.87530\n",
      "[1080]\ttrain-auc:0.87454\ttest-auc:0.87048\tval-auc:0.87540\n",
      "[1090]\ttrain-auc:0.87464\ttest-auc:0.87055\tval-auc:0.87548\n",
      "[1100]\ttrain-auc:0.87473\ttest-auc:0.87061\tval-auc:0.87555\n",
      "[1110]\ttrain-auc:0.87489\ttest-auc:0.87072\tval-auc:0.87569\n",
      "[1120]\ttrain-auc:0.87500\ttest-auc:0.87077\tval-auc:0.87571\n",
      "[1130]\ttrain-auc:0.87511\ttest-auc:0.87086\tval-auc:0.87577\n",
      "[1140]\ttrain-auc:0.87522\ttest-auc:0.87094\tval-auc:0.87586\n",
      "[1150]\ttrain-auc:0.87533\ttest-auc:0.87101\tval-auc:0.87592\n",
      "[1160]\ttrain-auc:0.87542\ttest-auc:0.87108\tval-auc:0.87601\n",
      "[1170]\ttrain-auc:0.87557\ttest-auc:0.87118\tval-auc:0.87615\n",
      "[1180]\ttrain-auc:0.87565\ttest-auc:0.87127\tval-auc:0.87621\n",
      "[1190]\ttrain-auc:0.87575\ttest-auc:0.87135\tval-auc:0.87629\n",
      "[1200]\ttrain-auc:0.87584\ttest-auc:0.87141\tval-auc:0.87633\n",
      "[1210]\ttrain-auc:0.87594\ttest-auc:0.87148\tval-auc:0.87640\n",
      "[1220]\ttrain-auc:0.87604\ttest-auc:0.87154\tval-auc:0.87645\n",
      "[1230]\ttrain-auc:0.87614\ttest-auc:0.87162\tval-auc:0.87651\n",
      "[1240]\ttrain-auc:0.87626\ttest-auc:0.87173\tval-auc:0.87658\n",
      "[1250]\ttrain-auc:0.87637\ttest-auc:0.87179\tval-auc:0.87665\n",
      "[1260]\ttrain-auc:0.87647\ttest-auc:0.87184\tval-auc:0.87672\n",
      "[1270]\ttrain-auc:0.87662\ttest-auc:0.87197\tval-auc:0.87683\n",
      "[1280]\ttrain-auc:0.87670\ttest-auc:0.87204\tval-auc:0.87692\n",
      "[1290]\ttrain-auc:0.87679\ttest-auc:0.87211\tval-auc:0.87698\n",
      "[1300]\ttrain-auc:0.87687\ttest-auc:0.87217\tval-auc:0.87704\n",
      "[1310]\ttrain-auc:0.87697\ttest-auc:0.87222\tval-auc:0.87707\n",
      "[1320]\ttrain-auc:0.87708\ttest-auc:0.87230\tval-auc:0.87714\n",
      "[1330]\ttrain-auc:0.87715\ttest-auc:0.87235\tval-auc:0.87718\n",
      "[1340]\ttrain-auc:0.87724\ttest-auc:0.87240\tval-auc:0.87722\n",
      "[1350]\ttrain-auc:0.87734\ttest-auc:0.87248\tval-auc:0.87726\n",
      "[1360]\ttrain-auc:0.87745\ttest-auc:0.87255\tval-auc:0.87732\n",
      "[1370]\ttrain-auc:0.87753\ttest-auc:0.87261\tval-auc:0.87734\n",
      "[1380]\ttrain-auc:0.87761\ttest-auc:0.87264\tval-auc:0.87738\n",
      "[1390]\ttrain-auc:0.87772\ttest-auc:0.87273\tval-auc:0.87746\n",
      "[1400]\ttrain-auc:0.87780\ttest-auc:0.87277\tval-auc:0.87753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1410]\ttrain-auc:0.87788\ttest-auc:0.87283\tval-auc:0.87758\n",
      "[1420]\ttrain-auc:0.87795\ttest-auc:0.87286\tval-auc:0.87762\n",
      "[1430]\ttrain-auc:0.87803\ttest-auc:0.87291\tval-auc:0.87765\n",
      "[1440]\ttrain-auc:0.87815\ttest-auc:0.87296\tval-auc:0.87772\n",
      "[1450]\ttrain-auc:0.87825\ttest-auc:0.87302\tval-auc:0.87778\n",
      "[1460]\ttrain-auc:0.87833\ttest-auc:0.87306\tval-auc:0.87785\n",
      "[1470]\ttrain-auc:0.87842\ttest-auc:0.87313\tval-auc:0.87789\n",
      "[1480]\ttrain-auc:0.87848\ttest-auc:0.87317\tval-auc:0.87792\n",
      "[1490]\ttrain-auc:0.87856\ttest-auc:0.87320\tval-auc:0.87799\n",
      "[1500]\ttrain-auc:0.87863\ttest-auc:0.87325\tval-auc:0.87804\n",
      "[1510]\ttrain-auc:0.87875\ttest-auc:0.87334\tval-auc:0.87812\n",
      "[1520]\ttrain-auc:0.87884\ttest-auc:0.87340\tval-auc:0.87812\n",
      "[1530]\ttrain-auc:0.87893\ttest-auc:0.87346\tval-auc:0.87820\n",
      "[1540]\ttrain-auc:0.87901\ttest-auc:0.87351\tval-auc:0.87827\n",
      "[1550]\ttrain-auc:0.87913\ttest-auc:0.87360\tval-auc:0.87833\n",
      "[1560]\ttrain-auc:0.87922\ttest-auc:0.87366\tval-auc:0.87839\n",
      "[1570]\ttrain-auc:0.87930\ttest-auc:0.87371\tval-auc:0.87844\n",
      "[1580]\ttrain-auc:0.87938\ttest-auc:0.87376\tval-auc:0.87849\n",
      "[1590]\ttrain-auc:0.87947\ttest-auc:0.87381\tval-auc:0.87857\n",
      "[1600]\ttrain-auc:0.87954\ttest-auc:0.87387\tval-auc:0.87863\n",
      "[1610]\ttrain-auc:0.87963\ttest-auc:0.87394\tval-auc:0.87870\n",
      "[1620]\ttrain-auc:0.87978\ttest-auc:0.87405\tval-auc:0.87880\n",
      "[1630]\ttrain-auc:0.87988\ttest-auc:0.87414\tval-auc:0.87891\n",
      "[1640]\ttrain-auc:0.87996\ttest-auc:0.87419\tval-auc:0.87897\n",
      "[1650]\ttrain-auc:0.88004\ttest-auc:0.87423\tval-auc:0.87901\n",
      "[1660]\ttrain-auc:0.88013\ttest-auc:0.87429\tval-auc:0.87904\n",
      "[1670]\ttrain-auc:0.88023\ttest-auc:0.87434\tval-auc:0.87911\n",
      "[1680]\ttrain-auc:0.88028\ttest-auc:0.87437\tval-auc:0.87913\n",
      "[1690]\ttrain-auc:0.88035\ttest-auc:0.87442\tval-auc:0.87916\n",
      "[1700]\ttrain-auc:0.88043\ttest-auc:0.87446\tval-auc:0.87921\n",
      "[1710]\ttrain-auc:0.88052\ttest-auc:0.87451\tval-auc:0.87928\n",
      "[1720]\ttrain-auc:0.88058\ttest-auc:0.87455\tval-auc:0.87932\n",
      "[1730]\ttrain-auc:0.88065\ttest-auc:0.87458\tval-auc:0.87935\n",
      "[1740]\ttrain-auc:0.88072\ttest-auc:0.87460\tval-auc:0.87937\n",
      "[1750]\ttrain-auc:0.88079\ttest-auc:0.87464\tval-auc:0.87940\n",
      "[1760]\ttrain-auc:0.88087\ttest-auc:0.87468\tval-auc:0.87945\n",
      "[1770]\ttrain-auc:0.88095\ttest-auc:0.87474\tval-auc:0.87950\n",
      "[1780]\ttrain-auc:0.88104\ttest-auc:0.87480\tval-auc:0.87953\n",
      "[1790]\ttrain-auc:0.88111\ttest-auc:0.87482\tval-auc:0.87960\n",
      "[1800]\ttrain-auc:0.88119\ttest-auc:0.87486\tval-auc:0.87966\n",
      "[1810]\ttrain-auc:0.88126\ttest-auc:0.87490\tval-auc:0.87972\n",
      "[1820]\ttrain-auc:0.88132\ttest-auc:0.87494\tval-auc:0.87976\n",
      "[1830]\ttrain-auc:0.88141\ttest-auc:0.87500\tval-auc:0.87985\n",
      "[1840]\ttrain-auc:0.88149\ttest-auc:0.87505\tval-auc:0.87989\n",
      "[1850]\ttrain-auc:0.88156\ttest-auc:0.87508\tval-auc:0.87997\n",
      "[1860]\ttrain-auc:0.88163\ttest-auc:0.87513\tval-auc:0.88001\n",
      "[1870]\ttrain-auc:0.88170\ttest-auc:0.87514\tval-auc:0.88003\n",
      "[1880]\ttrain-auc:0.88179\ttest-auc:0.87518\tval-auc:0.88008\n",
      "[1890]\ttrain-auc:0.88186\ttest-auc:0.87523\tval-auc:0.88012\n",
      "[1900]\ttrain-auc:0.88193\ttest-auc:0.87528\tval-auc:0.88015\n",
      "[1910]\ttrain-auc:0.88199\ttest-auc:0.87531\tval-auc:0.88019\n",
      "[1920]\ttrain-auc:0.88206\ttest-auc:0.87534\tval-auc:0.88024\n",
      "[1930]\ttrain-auc:0.88213\ttest-auc:0.87539\tval-auc:0.88030\n",
      "[1940]\ttrain-auc:0.88219\ttest-auc:0.87541\tval-auc:0.88034\n",
      "[1950]\ttrain-auc:0.88224\ttest-auc:0.87545\tval-auc:0.88038\n",
      "[1960]\ttrain-auc:0.88231\ttest-auc:0.87549\tval-auc:0.88040\n",
      "[1970]\ttrain-auc:0.88237\ttest-auc:0.87552\tval-auc:0.88043\n",
      "[1980]\ttrain-auc:0.88247\ttest-auc:0.87559\tval-auc:0.88051\n",
      "[1990]\ttrain-auc:0.88256\ttest-auc:0.87563\tval-auc:0.88056\n",
      "[2000]\ttrain-auc:0.88263\ttest-auc:0.87568\tval-auc:0.88059\n",
      "[2010]\ttrain-auc:0.88270\ttest-auc:0.87571\tval-auc:0.88065\n",
      "[2020]\ttrain-auc:0.88278\ttest-auc:0.87576\tval-auc:0.88070\n",
      "[2030]\ttrain-auc:0.88282\ttest-auc:0.87579\tval-auc:0.88073\n",
      "[2040]\ttrain-auc:0.88292\ttest-auc:0.87588\tval-auc:0.88076\n",
      "[2050]\ttrain-auc:0.88299\ttest-auc:0.87591\tval-auc:0.88081\n",
      "[2060]\ttrain-auc:0.88309\ttest-auc:0.87596\tval-auc:0.88088\n",
      "[2070]\ttrain-auc:0.88315\ttest-auc:0.87602\tval-auc:0.88092\n",
      "[2080]\ttrain-auc:0.88323\ttest-auc:0.87606\tval-auc:0.88097\n",
      "[2090]\ttrain-auc:0.88329\ttest-auc:0.87609\tval-auc:0.88099\n",
      "[2100]\ttrain-auc:0.88335\ttest-auc:0.87612\tval-auc:0.88102\n",
      "[2110]\ttrain-auc:0.88346\ttest-auc:0.87619\tval-auc:0.88109\n",
      "[2120]\ttrain-auc:0.88353\ttest-auc:0.87623\tval-auc:0.88111\n",
      "[2130]\ttrain-auc:0.88358\ttest-auc:0.87624\tval-auc:0.88115\n",
      "[2140]\ttrain-auc:0.88365\ttest-auc:0.87629\tval-auc:0.88121\n",
      "[2150]\ttrain-auc:0.88374\ttest-auc:0.87637\tval-auc:0.88122\n",
      "[2160]\ttrain-auc:0.88381\ttest-auc:0.87640\tval-auc:0.88127\n",
      "[2170]\ttrain-auc:0.88389\ttest-auc:0.87645\tval-auc:0.88131\n",
      "[2180]\ttrain-auc:0.88394\ttest-auc:0.87647\tval-auc:0.88134\n",
      "[2190]\ttrain-auc:0.88401\ttest-auc:0.87651\tval-auc:0.88140\n",
      "[2200]\ttrain-auc:0.88409\ttest-auc:0.87655\tval-auc:0.88144\n",
      "[2210]\ttrain-auc:0.88414\ttest-auc:0.87658\tval-auc:0.88147\n",
      "[2220]\ttrain-auc:0.88420\ttest-auc:0.87661\tval-auc:0.88151\n",
      "[2230]\ttrain-auc:0.88426\ttest-auc:0.87664\tval-auc:0.88154\n",
      "[2240]\ttrain-auc:0.88433\ttest-auc:0.87667\tval-auc:0.88156\n",
      "[2250]\ttrain-auc:0.88439\ttest-auc:0.87670\tval-auc:0.88159\n",
      "[2260]\ttrain-auc:0.88445\ttest-auc:0.87674\tval-auc:0.88162\n",
      "[2270]\ttrain-auc:0.88450\ttest-auc:0.87677\tval-auc:0.88166\n",
      "[2280]\ttrain-auc:0.88456\ttest-auc:0.87679\tval-auc:0.88168\n",
      "[2290]\ttrain-auc:0.88461\ttest-auc:0.87682\tval-auc:0.88171\n",
      "[2300]\ttrain-auc:0.88466\ttest-auc:0.87684\tval-auc:0.88173\n",
      "[2310]\ttrain-auc:0.88475\ttest-auc:0.87692\tval-auc:0.88179\n",
      "[2320]\ttrain-auc:0.88482\ttest-auc:0.87695\tval-auc:0.88185\n",
      "[2330]\ttrain-auc:0.88489\ttest-auc:0.87698\tval-auc:0.88189\n",
      "[2340]\ttrain-auc:0.88496\ttest-auc:0.87702\tval-auc:0.88193\n",
      "[2350]\ttrain-auc:0.88502\ttest-auc:0.87706\tval-auc:0.88195\n",
      "[2360]\ttrain-auc:0.88509\ttest-auc:0.87708\tval-auc:0.88197\n",
      "[2370]\ttrain-auc:0.88515\ttest-auc:0.87712\tval-auc:0.88200\n",
      "[2380]\ttrain-auc:0.88519\ttest-auc:0.87715\tval-auc:0.88202\n",
      "[2390]\ttrain-auc:0.88526\ttest-auc:0.87720\tval-auc:0.88204\n",
      "[2400]\ttrain-auc:0.88531\ttest-auc:0.87723\tval-auc:0.88203\n",
      "[2410]\ttrain-auc:0.88538\ttest-auc:0.87725\tval-auc:0.88206\n",
      "[2420]\ttrain-auc:0.88544\ttest-auc:0.87727\tval-auc:0.88209\n",
      "[2430]\ttrain-auc:0.88552\ttest-auc:0.87732\tval-auc:0.88216\n",
      "[2440]\ttrain-auc:0.88561\ttest-auc:0.87736\tval-auc:0.88223\n",
      "[2450]\ttrain-auc:0.88567\ttest-auc:0.87739\tval-auc:0.88226\n",
      "[2460]\ttrain-auc:0.88573\ttest-auc:0.87742\tval-auc:0.88230\n",
      "[2470]\ttrain-auc:0.88579\ttest-auc:0.87745\tval-auc:0.88234\n",
      "[2480]\ttrain-auc:0.88583\ttest-auc:0.87746\tval-auc:0.88235\n",
      "[2490]\ttrain-auc:0.88589\ttest-auc:0.87748\tval-auc:0.88238\n",
      "[2500]\ttrain-auc:0.88595\ttest-auc:0.87752\tval-auc:0.88242\n",
      "[2510]\ttrain-auc:0.88602\ttest-auc:0.87756\tval-auc:0.88245\n",
      "[2520]\ttrain-auc:0.88608\ttest-auc:0.87758\tval-auc:0.88248\n",
      "[2530]\ttrain-auc:0.88614\ttest-auc:0.87762\tval-auc:0.88252\n",
      "[2540]\ttrain-auc:0.88622\ttest-auc:0.87767\tval-auc:0.88256\n",
      "[2550]\ttrain-auc:0.88627\ttest-auc:0.87770\tval-auc:0.88258\n",
      "[2560]\ttrain-auc:0.88634\ttest-auc:0.87772\tval-auc:0.88261\n",
      "[2570]\ttrain-auc:0.88639\ttest-auc:0.87775\tval-auc:0.88264\n",
      "[2580]\ttrain-auc:0.88644\ttest-auc:0.87777\tval-auc:0.88267\n",
      "[2590]\ttrain-auc:0.88648\ttest-auc:0.87780\tval-auc:0.88271\n",
      "[2600]\ttrain-auc:0.88653\ttest-auc:0.87784\tval-auc:0.88275\n",
      "[2610]\ttrain-auc:0.88660\ttest-auc:0.87787\tval-auc:0.88277\n",
      "[2620]\ttrain-auc:0.88664\ttest-auc:0.87788\tval-auc:0.88279\n",
      "[2630]\ttrain-auc:0.88669\ttest-auc:0.87790\tval-auc:0.88280\n",
      "[2640]\ttrain-auc:0.88674\ttest-auc:0.87792\tval-auc:0.88283\n",
      "[2650]\ttrain-auc:0.88680\ttest-auc:0.87796\tval-auc:0.88287\n",
      "[2660]\ttrain-auc:0.88686\ttest-auc:0.87799\tval-auc:0.88292\n",
      "[2670]\ttrain-auc:0.88691\ttest-auc:0.87802\tval-auc:0.88294\n",
      "[2680]\ttrain-auc:0.88696\ttest-auc:0.87805\tval-auc:0.88296\n",
      "[2690]\ttrain-auc:0.88702\ttest-auc:0.87809\tval-auc:0.88299\n",
      "[2700]\ttrain-auc:0.88708\ttest-auc:0.87812\tval-auc:0.88302\n",
      "[2710]\ttrain-auc:0.88715\ttest-auc:0.87816\tval-auc:0.88305\n",
      "[2720]\ttrain-auc:0.88719\ttest-auc:0.87819\tval-auc:0.88307\n",
      "[2730]\ttrain-auc:0.88725\ttest-auc:0.87823\tval-auc:0.88309\n",
      "[2740]\ttrain-auc:0.88730\ttest-auc:0.87825\tval-auc:0.88311\n",
      "[2750]\ttrain-auc:0.88736\ttest-auc:0.87827\tval-auc:0.88313\n",
      "[2760]\ttrain-auc:0.88742\ttest-auc:0.87831\tval-auc:0.88318\n",
      "[2770]\ttrain-auc:0.88748\ttest-auc:0.87832\tval-auc:0.88319\n",
      "[2780]\ttrain-auc:0.88751\ttest-auc:0.87834\tval-auc:0.88321\n",
      "[2790]\ttrain-auc:0.88756\ttest-auc:0.87836\tval-auc:0.88322\n",
      "[2800]\ttrain-auc:0.88762\ttest-auc:0.87839\tval-auc:0.88324\n",
      "[2810]\ttrain-auc:0.88770\ttest-auc:0.87845\tval-auc:0.88330\n",
      "[2820]\ttrain-auc:0.88776\ttest-auc:0.87847\tval-auc:0.88333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2830]\ttrain-auc:0.88783\ttest-auc:0.87850\tval-auc:0.88334\n",
      "[2840]\ttrain-auc:0.88789\ttest-auc:0.87853\tval-auc:0.88338\n",
      "[2850]\ttrain-auc:0.88795\ttest-auc:0.87856\tval-auc:0.88342\n",
      "[2860]\ttrain-auc:0.88800\ttest-auc:0.87858\tval-auc:0.88345\n",
      "[2870]\ttrain-auc:0.88805\ttest-auc:0.87859\tval-auc:0.88347\n",
      "[2880]\ttrain-auc:0.88810\ttest-auc:0.87861\tval-auc:0.88347\n",
      "[2890]\ttrain-auc:0.88818\ttest-auc:0.87866\tval-auc:0.88355\n",
      "[2900]\ttrain-auc:0.88823\ttest-auc:0.87867\tval-auc:0.88355\n",
      "[2910]\ttrain-auc:0.88831\ttest-auc:0.87874\tval-auc:0.88362\n",
      "[2920]\ttrain-auc:0.88837\ttest-auc:0.87876\tval-auc:0.88366\n",
      "[2930]\ttrain-auc:0.88841\ttest-auc:0.87879\tval-auc:0.88367\n",
      "[2940]\ttrain-auc:0.88847\ttest-auc:0.87883\tval-auc:0.88369\n",
      "[2950]\ttrain-auc:0.88852\ttest-auc:0.87885\tval-auc:0.88371\n",
      "[2960]\ttrain-auc:0.88856\ttest-auc:0.87888\tval-auc:0.88373\n",
      "[2970]\ttrain-auc:0.88861\ttest-auc:0.87890\tval-auc:0.88377\n",
      "[2980]\ttrain-auc:0.88866\ttest-auc:0.87892\tval-auc:0.88380\n",
      "[2990]\ttrain-auc:0.88871\ttest-auc:0.87896\tval-auc:0.88383\n",
      "[3000]\ttrain-auc:0.88878\ttest-auc:0.87899\tval-auc:0.88386\n",
      "[3010]\ttrain-auc:0.88884\ttest-auc:0.87902\tval-auc:0.88392\n",
      "[3020]\ttrain-auc:0.88890\ttest-auc:0.87906\tval-auc:0.88394\n",
      "[3030]\ttrain-auc:0.88895\ttest-auc:0.87909\tval-auc:0.88397\n",
      "[3040]\ttrain-auc:0.88902\ttest-auc:0.87913\tval-auc:0.88402\n",
      "[3050]\ttrain-auc:0.88908\ttest-auc:0.87916\tval-auc:0.88402\n",
      "[3060]\ttrain-auc:0.88918\ttest-auc:0.87923\tval-auc:0.88413\n",
      "[3070]\ttrain-auc:0.88924\ttest-auc:0.87925\tval-auc:0.88416\n",
      "[3080]\ttrain-auc:0.88929\ttest-auc:0.87927\tval-auc:0.88419\n",
      "[3090]\ttrain-auc:0.88933\ttest-auc:0.87929\tval-auc:0.88421\n",
      "[3100]\ttrain-auc:0.88938\ttest-auc:0.87932\tval-auc:0.88423\n",
      "[3110]\ttrain-auc:0.88942\ttest-auc:0.87934\tval-auc:0.88425\n",
      "[3120]\ttrain-auc:0.88948\ttest-auc:0.87936\tval-auc:0.88427\n",
      "[3130]\ttrain-auc:0.88953\ttest-auc:0.87938\tval-auc:0.88429\n",
      "[3140]\ttrain-auc:0.88958\ttest-auc:0.87939\tval-auc:0.88431\n",
      "[3150]\ttrain-auc:0.88963\ttest-auc:0.87942\tval-auc:0.88432\n",
      "[3160]\ttrain-auc:0.88967\ttest-auc:0.87944\tval-auc:0.88434\n",
      "[3170]\ttrain-auc:0.88972\ttest-auc:0.87946\tval-auc:0.88436\n",
      "[3180]\ttrain-auc:0.88977\ttest-auc:0.87948\tval-auc:0.88439\n",
      "[3190]\ttrain-auc:0.88982\ttest-auc:0.87949\tval-auc:0.88440\n",
      "[3200]\ttrain-auc:0.88986\ttest-auc:0.87952\tval-auc:0.88442\n",
      "[3210]\ttrain-auc:0.88994\ttest-auc:0.87957\tval-auc:0.88448\n",
      "[3220]\ttrain-auc:0.88999\ttest-auc:0.87960\tval-auc:0.88450\n",
      "[3230]\ttrain-auc:0.89004\ttest-auc:0.87962\tval-auc:0.88453\n",
      "[3240]\ttrain-auc:0.89013\ttest-auc:0.87969\tval-auc:0.88463\n",
      "[3250]\ttrain-auc:0.89018\ttest-auc:0.87971\tval-auc:0.88466\n",
      "[3260]\ttrain-auc:0.89023\ttest-auc:0.87973\tval-auc:0.88468\n",
      "[3270]\ttrain-auc:0.89027\ttest-auc:0.87974\tval-auc:0.88470\n",
      "[3280]\ttrain-auc:0.89032\ttest-auc:0.87975\tval-auc:0.88473\n",
      "[3290]\ttrain-auc:0.89039\ttest-auc:0.87980\tval-auc:0.88478\n",
      "[3300]\ttrain-auc:0.89043\ttest-auc:0.87982\tval-auc:0.88480\n",
      "[3310]\ttrain-auc:0.89048\ttest-auc:0.87985\tval-auc:0.88482\n",
      "[3320]\ttrain-auc:0.89051\ttest-auc:0.87986\tval-auc:0.88484\n",
      "[3330]\ttrain-auc:0.89056\ttest-auc:0.87989\tval-auc:0.88486\n",
      "[3340]\ttrain-auc:0.89061\ttest-auc:0.87991\tval-auc:0.88488\n",
      "[3350]\ttrain-auc:0.89066\ttest-auc:0.87992\tval-auc:0.88488\n",
      "[3360]\ttrain-auc:0.89072\ttest-auc:0.87995\tval-auc:0.88492\n",
      "[3370]\ttrain-auc:0.89077\ttest-auc:0.87997\tval-auc:0.88495\n",
      "[3380]\ttrain-auc:0.89084\ttest-auc:0.88002\tval-auc:0.88501\n",
      "[3390]\ttrain-auc:0.89089\ttest-auc:0.88004\tval-auc:0.88504\n",
      "[3400]\ttrain-auc:0.89093\ttest-auc:0.88005\tval-auc:0.88508\n",
      "[3410]\ttrain-auc:0.89097\ttest-auc:0.88007\tval-auc:0.88509\n",
      "[3420]\ttrain-auc:0.89101\ttest-auc:0.88008\tval-auc:0.88511\n",
      "[3430]\ttrain-auc:0.89106\ttest-auc:0.88011\tval-auc:0.88513\n",
      "[3440]\ttrain-auc:0.89110\ttest-auc:0.88013\tval-auc:0.88514\n",
      "[3450]\ttrain-auc:0.89114\ttest-auc:0.88014\tval-auc:0.88515\n",
      "[3460]\ttrain-auc:0.89117\ttest-auc:0.88016\tval-auc:0.88517\n",
      "[3470]\ttrain-auc:0.89122\ttest-auc:0.88018\tval-auc:0.88519\n",
      "[3480]\ttrain-auc:0.89126\ttest-auc:0.88019\tval-auc:0.88521\n",
      "[3490]\ttrain-auc:0.89130\ttest-auc:0.88021\tval-auc:0.88524\n",
      "[3500]\ttrain-auc:0.89134\ttest-auc:0.88022\tval-auc:0.88524\n",
      "[3510]\ttrain-auc:0.89138\ttest-auc:0.88023\tval-auc:0.88526\n",
      "[3520]\ttrain-auc:0.89143\ttest-auc:0.88026\tval-auc:0.88528\n",
      "[3530]\ttrain-auc:0.89147\ttest-auc:0.88027\tval-auc:0.88530\n",
      "[3540]\ttrain-auc:0.89152\ttest-auc:0.88030\tval-auc:0.88532\n",
      "[3550]\ttrain-auc:0.89157\ttest-auc:0.88033\tval-auc:0.88534\n",
      "[3560]\ttrain-auc:0.89161\ttest-auc:0.88034\tval-auc:0.88536\n",
      "[3570]\ttrain-auc:0.89165\ttest-auc:0.88036\tval-auc:0.88537\n",
      "[3580]\ttrain-auc:0.89171\ttest-auc:0.88039\tval-auc:0.88540\n",
      "[3590]\ttrain-auc:0.89175\ttest-auc:0.88041\tval-auc:0.88543\n",
      "[3600]\ttrain-auc:0.89183\ttest-auc:0.88048\tval-auc:0.88548\n",
      "[3610]\ttrain-auc:0.89189\ttest-auc:0.88051\tval-auc:0.88550\n",
      "[3620]\ttrain-auc:0.89192\ttest-auc:0.88052\tval-auc:0.88551\n",
      "[3630]\ttrain-auc:0.89199\ttest-auc:0.88056\tval-auc:0.88555\n",
      "[3640]\ttrain-auc:0.89204\ttest-auc:0.88058\tval-auc:0.88556\n",
      "[3650]\ttrain-auc:0.89207\ttest-auc:0.88060\tval-auc:0.88557\n",
      "[3660]\ttrain-auc:0.89211\ttest-auc:0.88061\tval-auc:0.88558\n",
      "[3670]\ttrain-auc:0.89216\ttest-auc:0.88062\tval-auc:0.88561\n",
      "[3680]\ttrain-auc:0.89220\ttest-auc:0.88064\tval-auc:0.88564\n",
      "[3690]\ttrain-auc:0.89224\ttest-auc:0.88066\tval-auc:0.88567\n",
      "[3700]\ttrain-auc:0.89229\ttest-auc:0.88067\tval-auc:0.88568\n",
      "[3710]\ttrain-auc:0.89235\ttest-auc:0.88072\tval-auc:0.88570\n",
      "[3720]\ttrain-auc:0.89239\ttest-auc:0.88073\tval-auc:0.88573\n",
      "[3730]\ttrain-auc:0.89244\ttest-auc:0.88075\tval-auc:0.88573\n",
      "[3740]\ttrain-auc:0.89247\ttest-auc:0.88075\tval-auc:0.88575\n",
      "[3750]\ttrain-auc:0.89251\ttest-auc:0.88077\tval-auc:0.88576\n",
      "[3760]\ttrain-auc:0.89257\ttest-auc:0.88080\tval-auc:0.88580\n",
      "[3770]\ttrain-auc:0.89261\ttest-auc:0.88081\tval-auc:0.88581\n",
      "[3780]\ttrain-auc:0.89265\ttest-auc:0.88082\tval-auc:0.88581\n",
      "[3790]\ttrain-auc:0.89270\ttest-auc:0.88085\tval-auc:0.88584\n",
      "[3800]\ttrain-auc:0.89274\ttest-auc:0.88086\tval-auc:0.88587\n",
      "[3810]\ttrain-auc:0.89280\ttest-auc:0.88089\tval-auc:0.88591\n",
      "[3820]\ttrain-auc:0.89285\ttest-auc:0.88091\tval-auc:0.88593\n",
      "[3830]\ttrain-auc:0.89290\ttest-auc:0.88094\tval-auc:0.88596\n",
      "[3840]\ttrain-auc:0.89295\ttest-auc:0.88096\tval-auc:0.88598\n",
      "[3850]\ttrain-auc:0.89299\ttest-auc:0.88097\tval-auc:0.88599\n",
      "[3860]\ttrain-auc:0.89303\ttest-auc:0.88098\tval-auc:0.88602\n",
      "[3870]\ttrain-auc:0.89308\ttest-auc:0.88100\tval-auc:0.88604\n",
      "[3880]\ttrain-auc:0.89312\ttest-auc:0.88102\tval-auc:0.88606\n",
      "[3890]\ttrain-auc:0.89315\ttest-auc:0.88104\tval-auc:0.88606\n",
      "[3900]\ttrain-auc:0.89319\ttest-auc:0.88106\tval-auc:0.88610\n",
      "[3910]\ttrain-auc:0.89323\ttest-auc:0.88107\tval-auc:0.88611\n",
      "[3920]\ttrain-auc:0.89326\ttest-auc:0.88108\tval-auc:0.88612\n",
      "[3930]\ttrain-auc:0.89331\ttest-auc:0.88111\tval-auc:0.88614\n",
      "[3940]\ttrain-auc:0.89335\ttest-auc:0.88112\tval-auc:0.88616\n",
      "[3950]\ttrain-auc:0.89339\ttest-auc:0.88114\tval-auc:0.88618\n",
      "[3960]\ttrain-auc:0.89344\ttest-auc:0.88117\tval-auc:0.88620\n",
      "[3970]\ttrain-auc:0.89349\ttest-auc:0.88118\tval-auc:0.88622\n",
      "[3980]\ttrain-auc:0.89354\ttest-auc:0.88121\tval-auc:0.88624\n",
      "[3990]\ttrain-auc:0.89360\ttest-auc:0.88125\tval-auc:0.88628\n",
      "[4000]\ttrain-auc:0.89364\ttest-auc:0.88127\tval-auc:0.88629\n",
      "[4010]\ttrain-auc:0.89368\ttest-auc:0.88129\tval-auc:0.88631\n",
      "[4020]\ttrain-auc:0.89372\ttest-auc:0.88130\tval-auc:0.88632\n",
      "[4030]\ttrain-auc:0.89375\ttest-auc:0.88131\tval-auc:0.88633\n",
      "[4040]\ttrain-auc:0.89378\ttest-auc:0.88132\tval-auc:0.88634\n",
      "[4050]\ttrain-auc:0.89382\ttest-auc:0.88134\tval-auc:0.88637\n",
      "[4060]\ttrain-auc:0.89387\ttest-auc:0.88135\tval-auc:0.88636\n",
      "[4070]\ttrain-auc:0.89391\ttest-auc:0.88137\tval-auc:0.88638\n",
      "[4080]\ttrain-auc:0.89399\ttest-auc:0.88143\tval-auc:0.88644\n",
      "[4090]\ttrain-auc:0.89402\ttest-auc:0.88144\tval-auc:0.88645\n",
      "[4100]\ttrain-auc:0.89407\ttest-auc:0.88147\tval-auc:0.88646\n",
      "[4110]\ttrain-auc:0.89411\ttest-auc:0.88148\tval-auc:0.88648\n",
      "[4120]\ttrain-auc:0.89415\ttest-auc:0.88149\tval-auc:0.88650\n",
      "[4130]\ttrain-auc:0.89420\ttest-auc:0.88151\tval-auc:0.88653\n",
      "[4140]\ttrain-auc:0.89424\ttest-auc:0.88152\tval-auc:0.88656\n",
      "[4150]\ttrain-auc:0.89427\ttest-auc:0.88155\tval-auc:0.88657\n",
      "[4160]\ttrain-auc:0.89431\ttest-auc:0.88156\tval-auc:0.88658\n",
      "[4170]\ttrain-auc:0.89435\ttest-auc:0.88157\tval-auc:0.88659\n",
      "[4180]\ttrain-auc:0.89439\ttest-auc:0.88159\tval-auc:0.88661\n",
      "[4190]\ttrain-auc:0.89443\ttest-auc:0.88161\tval-auc:0.88662\n",
      "[4200]\ttrain-auc:0.89447\ttest-auc:0.88164\tval-auc:0.88663\n",
      "[4210]\ttrain-auc:0.89451\ttest-auc:0.88165\tval-auc:0.88665\n",
      "[4220]\ttrain-auc:0.89455\ttest-auc:0.88166\tval-auc:0.88667\n",
      "[4230]\ttrain-auc:0.89461\ttest-auc:0.88170\tval-auc:0.88671\n",
      "[4240]\ttrain-auc:0.89465\ttest-auc:0.88171\tval-auc:0.88674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4250]\ttrain-auc:0.89469\ttest-auc:0.88172\tval-auc:0.88676\n",
      "[4260]\ttrain-auc:0.89472\ttest-auc:0.88173\tval-auc:0.88677\n",
      "[4270]\ttrain-auc:0.89477\ttest-auc:0.88175\tval-auc:0.88678\n",
      "[4280]\ttrain-auc:0.89481\ttest-auc:0.88177\tval-auc:0.88679\n",
      "[4290]\ttrain-auc:0.89485\ttest-auc:0.88179\tval-auc:0.88682\n",
      "[4300]\ttrain-auc:0.89489\ttest-auc:0.88181\tval-auc:0.88683\n",
      "[4310]\ttrain-auc:0.89492\ttest-auc:0.88182\tval-auc:0.88684\n",
      "[4320]\ttrain-auc:0.89496\ttest-auc:0.88182\tval-auc:0.88687\n",
      "[4330]\ttrain-auc:0.89502\ttest-auc:0.88184\tval-auc:0.88691\n",
      "[4340]\ttrain-auc:0.89506\ttest-auc:0.88186\tval-auc:0.88693\n",
      "[4350]\ttrain-auc:0.89509\ttest-auc:0.88187\tval-auc:0.88694\n",
      "[4360]\ttrain-auc:0.89512\ttest-auc:0.88188\tval-auc:0.88694\n",
      "[4370]\ttrain-auc:0.89516\ttest-auc:0.88189\tval-auc:0.88696\n",
      "[4380]\ttrain-auc:0.89522\ttest-auc:0.88192\tval-auc:0.88698\n",
      "[4390]\ttrain-auc:0.89527\ttest-auc:0.88194\tval-auc:0.88702\n",
      "[4400]\ttrain-auc:0.89530\ttest-auc:0.88196\tval-auc:0.88704\n",
      "[4410]\ttrain-auc:0.89533\ttest-auc:0.88197\tval-auc:0.88704\n",
      "[4420]\ttrain-auc:0.89537\ttest-auc:0.88199\tval-auc:0.88706\n",
      "[4430]\ttrain-auc:0.89541\ttest-auc:0.88200\tval-auc:0.88707\n",
      "[4440]\ttrain-auc:0.89544\ttest-auc:0.88201\tval-auc:0.88708\n",
      "[4450]\ttrain-auc:0.89548\ttest-auc:0.88203\tval-auc:0.88709\n",
      "[4460]\ttrain-auc:0.89553\ttest-auc:0.88206\tval-auc:0.88713\n",
      "[4470]\ttrain-auc:0.89556\ttest-auc:0.88207\tval-auc:0.88713\n",
      "[4480]\ttrain-auc:0.89560\ttest-auc:0.88209\tval-auc:0.88714\n",
      "[4490]\ttrain-auc:0.89564\ttest-auc:0.88210\tval-auc:0.88715\n",
      "[4500]\ttrain-auc:0.89568\ttest-auc:0.88212\tval-auc:0.88718\n",
      "[4510]\ttrain-auc:0.89572\ttest-auc:0.88213\tval-auc:0.88719\n",
      "[4520]\ttrain-auc:0.89576\ttest-auc:0.88215\tval-auc:0.88723\n",
      "[4530]\ttrain-auc:0.89579\ttest-auc:0.88216\tval-auc:0.88725\n",
      "[4540]\ttrain-auc:0.89582\ttest-auc:0.88218\tval-auc:0.88727\n",
      "[4550]\ttrain-auc:0.89587\ttest-auc:0.88221\tval-auc:0.88728\n",
      "[4560]\ttrain-auc:0.89591\ttest-auc:0.88222\tval-auc:0.88729\n",
      "[4570]\ttrain-auc:0.89596\ttest-auc:0.88224\tval-auc:0.88732\n",
      "[4580]\ttrain-auc:0.89600\ttest-auc:0.88225\tval-auc:0.88733\n",
      "[4590]\ttrain-auc:0.89604\ttest-auc:0.88226\tval-auc:0.88734\n",
      "[4600]\ttrain-auc:0.89607\ttest-auc:0.88227\tval-auc:0.88736\n",
      "[4610]\ttrain-auc:0.89611\ttest-auc:0.88229\tval-auc:0.88737\n",
      "[4620]\ttrain-auc:0.89615\ttest-auc:0.88230\tval-auc:0.88740\n",
      "[4630]\ttrain-auc:0.89619\ttest-auc:0.88232\tval-auc:0.88743\n",
      "[4640]\ttrain-auc:0.89622\ttest-auc:0.88233\tval-auc:0.88745\n",
      "[4650]\ttrain-auc:0.89627\ttest-auc:0.88238\tval-auc:0.88749\n",
      "[4660]\ttrain-auc:0.89631\ttest-auc:0.88240\tval-auc:0.88751\n",
      "[4670]\ttrain-auc:0.89634\ttest-auc:0.88242\tval-auc:0.88753\n",
      "[4680]\ttrain-auc:0.89638\ttest-auc:0.88243\tval-auc:0.88754\n",
      "[4690]\ttrain-auc:0.89642\ttest-auc:0.88245\tval-auc:0.88756\n",
      "[4700]\ttrain-auc:0.89646\ttest-auc:0.88245\tval-auc:0.88758\n",
      "[4710]\ttrain-auc:0.89649\ttest-auc:0.88245\tval-auc:0.88758\n",
      "[4720]\ttrain-auc:0.89653\ttest-auc:0.88247\tval-auc:0.88759\n",
      "[4730]\ttrain-auc:0.89656\ttest-auc:0.88247\tval-auc:0.88761\n",
      "[4740]\ttrain-auc:0.89660\ttest-auc:0.88249\tval-auc:0.88761\n",
      "[4750]\ttrain-auc:0.89665\ttest-auc:0.88251\tval-auc:0.88762\n",
      "[4760]\ttrain-auc:0.89668\ttest-auc:0.88252\tval-auc:0.88763\n",
      "[4770]\ttrain-auc:0.89671\ttest-auc:0.88253\tval-auc:0.88762\n",
      "[4780]\ttrain-auc:0.89675\ttest-auc:0.88254\tval-auc:0.88763\n",
      "[4790]\ttrain-auc:0.89678\ttest-auc:0.88255\tval-auc:0.88764\n",
      "[4800]\ttrain-auc:0.89683\ttest-auc:0.88257\tval-auc:0.88768\n",
      "[4810]\ttrain-auc:0.89686\ttest-auc:0.88259\tval-auc:0.88768\n",
      "[4820]\ttrain-auc:0.89689\ttest-auc:0.88260\tval-auc:0.88770\n",
      "[4830]\ttrain-auc:0.89693\ttest-auc:0.88261\tval-auc:0.88771\n",
      "[4840]\ttrain-auc:0.89696\ttest-auc:0.88262\tval-auc:0.88773\n",
      "[4850]\ttrain-auc:0.89701\ttest-auc:0.88263\tval-auc:0.88774\n",
      "[4860]\ttrain-auc:0.89705\ttest-auc:0.88264\tval-auc:0.88775\n",
      "[4870]\ttrain-auc:0.89708\ttest-auc:0.88265\tval-auc:0.88777\n",
      "[4880]\ttrain-auc:0.89712\ttest-auc:0.88266\tval-auc:0.88779\n",
      "[4890]\ttrain-auc:0.89716\ttest-auc:0.88267\tval-auc:0.88781\n",
      "[4900]\ttrain-auc:0.89719\ttest-auc:0.88268\tval-auc:0.88782\n",
      "[4910]\ttrain-auc:0.89722\ttest-auc:0.88269\tval-auc:0.88783\n",
      "[4920]\ttrain-auc:0.89725\ttest-auc:0.88270\tval-auc:0.88783\n",
      "[4930]\ttrain-auc:0.89729\ttest-auc:0.88272\tval-auc:0.88785\n",
      "[4940]\ttrain-auc:0.89734\ttest-auc:0.88274\tval-auc:0.88786\n",
      "[4950]\ttrain-auc:0.89737\ttest-auc:0.88276\tval-auc:0.88788\n",
      "[4960]\ttrain-auc:0.89740\ttest-auc:0.88278\tval-auc:0.88789\n",
      "[4970]\ttrain-auc:0.89745\ttest-auc:0.88281\tval-auc:0.88791\n",
      "[4980]\ttrain-auc:0.89749\ttest-auc:0.88282\tval-auc:0.88791\n",
      "[4990]\ttrain-auc:0.89752\ttest-auc:0.88283\tval-auc:0.88794\n",
      "[5000]\ttrain-auc:0.89756\ttest-auc:0.88284\tval-auc:0.88796\n",
      "[5010]\ttrain-auc:0.89760\ttest-auc:0.88285\tval-auc:0.88796\n",
      "[5020]\ttrain-auc:0.89764\ttest-auc:0.88287\tval-auc:0.88799\n",
      "[5030]\ttrain-auc:0.89768\ttest-auc:0.88290\tval-auc:0.88802\n",
      "[5040]\ttrain-auc:0.89772\ttest-auc:0.88291\tval-auc:0.88803\n",
      "[5050]\ttrain-auc:0.89775\ttest-auc:0.88293\tval-auc:0.88804\n",
      "[5060]\ttrain-auc:0.89779\ttest-auc:0.88293\tval-auc:0.88804\n",
      "[5070]\ttrain-auc:0.89783\ttest-auc:0.88295\tval-auc:0.88805\n",
      "[5080]\ttrain-auc:0.89787\ttest-auc:0.88296\tval-auc:0.88807\n",
      "[5090]\ttrain-auc:0.89790\ttest-auc:0.88297\tval-auc:0.88808\n",
      "[5100]\ttrain-auc:0.89793\ttest-auc:0.88298\tval-auc:0.88808\n",
      "[5110]\ttrain-auc:0.89797\ttest-auc:0.88299\tval-auc:0.88810\n",
      "[5120]\ttrain-auc:0.89800\ttest-auc:0.88300\tval-auc:0.88811\n",
      "[5130]\ttrain-auc:0.89802\ttest-auc:0.88301\tval-auc:0.88812\n",
      "[5140]\ttrain-auc:0.89807\ttest-auc:0.88304\tval-auc:0.88814\n",
      "[5150]\ttrain-auc:0.89811\ttest-auc:0.88306\tval-auc:0.88817\n",
      "[5160]\ttrain-auc:0.89814\ttest-auc:0.88307\tval-auc:0.88819\n",
      "[5170]\ttrain-auc:0.89818\ttest-auc:0.88309\tval-auc:0.88820\n",
      "[5180]\ttrain-auc:0.89821\ttest-auc:0.88309\tval-auc:0.88822\n",
      "[5190]\ttrain-auc:0.89825\ttest-auc:0.88310\tval-auc:0.88824\n",
      "[5200]\ttrain-auc:0.89829\ttest-auc:0.88311\tval-auc:0.88826\n",
      "[5210]\ttrain-auc:0.89832\ttest-auc:0.88313\tval-auc:0.88825\n",
      "[5220]\ttrain-auc:0.89835\ttest-auc:0.88313\tval-auc:0.88826\n",
      "[5230]\ttrain-auc:0.89839\ttest-auc:0.88315\tval-auc:0.88828\n",
      "[5240]\ttrain-auc:0.89844\ttest-auc:0.88317\tval-auc:0.88831\n",
      "[5250]\ttrain-auc:0.89847\ttest-auc:0.88319\tval-auc:0.88833\n",
      "[5260]\ttrain-auc:0.89851\ttest-auc:0.88320\tval-auc:0.88836\n",
      "[5270]\ttrain-auc:0.89854\ttest-auc:0.88320\tval-auc:0.88837\n",
      "[5280]\ttrain-auc:0.89857\ttest-auc:0.88321\tval-auc:0.88838\n",
      "[5290]\ttrain-auc:0.89861\ttest-auc:0.88322\tval-auc:0.88840\n",
      "[5300]\ttrain-auc:0.89864\ttest-auc:0.88323\tval-auc:0.88840\n",
      "[5310]\ttrain-auc:0.89868\ttest-auc:0.88324\tval-auc:0.88841\n",
      "[5320]\ttrain-auc:0.89871\ttest-auc:0.88325\tval-auc:0.88843\n",
      "[5330]\ttrain-auc:0.89875\ttest-auc:0.88328\tval-auc:0.88845\n",
      "[5340]\ttrain-auc:0.89879\ttest-auc:0.88330\tval-auc:0.88847\n",
      "[5350]\ttrain-auc:0.89883\ttest-auc:0.88332\tval-auc:0.88848\n",
      "[5360]\ttrain-auc:0.89886\ttest-auc:0.88333\tval-auc:0.88850\n",
      "[5370]\ttrain-auc:0.89890\ttest-auc:0.88334\tval-auc:0.88851\n",
      "[5380]\ttrain-auc:0.89893\ttest-auc:0.88335\tval-auc:0.88852\n",
      "[5390]\ttrain-auc:0.89897\ttest-auc:0.88337\tval-auc:0.88853\n",
      "[5400]\ttrain-auc:0.89900\ttest-auc:0.88339\tval-auc:0.88856\n",
      "[5410]\ttrain-auc:0.89903\ttest-auc:0.88340\tval-auc:0.88856\n",
      "[5420]\ttrain-auc:0.89908\ttest-auc:0.88342\tval-auc:0.88857\n",
      "[5430]\ttrain-auc:0.89912\ttest-auc:0.88344\tval-auc:0.88859\n",
      "[5440]\ttrain-auc:0.89916\ttest-auc:0.88347\tval-auc:0.88860\n",
      "[5450]\ttrain-auc:0.89919\ttest-auc:0.88348\tval-auc:0.88860\n",
      "[5460]\ttrain-auc:0.89922\ttest-auc:0.88349\tval-auc:0.88861\n",
      "[5470]\ttrain-auc:0.89926\ttest-auc:0.88352\tval-auc:0.88863\n",
      "[5480]\ttrain-auc:0.89930\ttest-auc:0.88354\tval-auc:0.88864\n",
      "[5490]\ttrain-auc:0.89933\ttest-auc:0.88355\tval-auc:0.88867\n",
      "[5500]\ttrain-auc:0.89937\ttest-auc:0.88357\tval-auc:0.88868\n",
      "[5510]\ttrain-auc:0.89939\ttest-auc:0.88357\tval-auc:0.88869\n",
      "[5520]\ttrain-auc:0.89943\ttest-auc:0.88358\tval-auc:0.88871\n",
      "[5530]\ttrain-auc:0.89946\ttest-auc:0.88359\tval-auc:0.88873\n",
      "[5540]\ttrain-auc:0.89949\ttest-auc:0.88360\tval-auc:0.88872\n",
      "[5550]\ttrain-auc:0.89953\ttest-auc:0.88362\tval-auc:0.88874\n",
      "[5560]\ttrain-auc:0.89956\ttest-auc:0.88364\tval-auc:0.88875\n",
      "[5570]\ttrain-auc:0.89959\ttest-auc:0.88365\tval-auc:0.88875\n",
      "[5580]\ttrain-auc:0.89962\ttest-auc:0.88365\tval-auc:0.88876\n",
      "[5590]\ttrain-auc:0.89966\ttest-auc:0.88367\tval-auc:0.88877\n",
      "[5600]\ttrain-auc:0.89969\ttest-auc:0.88368\tval-auc:0.88879\n",
      "[5610]\ttrain-auc:0.89973\ttest-auc:0.88370\tval-auc:0.88881\n",
      "[5620]\ttrain-auc:0.89976\ttest-auc:0.88371\tval-auc:0.88882\n",
      "[5630]\ttrain-auc:0.89980\ttest-auc:0.88373\tval-auc:0.88884\n",
      "[5640]\ttrain-auc:0.89985\ttest-auc:0.88375\tval-auc:0.88887\n",
      "[5650]\ttrain-auc:0.89989\ttest-auc:0.88377\tval-auc:0.88889\n",
      "[5660]\ttrain-auc:0.89993\ttest-auc:0.88379\tval-auc:0.88891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5670]\ttrain-auc:0.89996\ttest-auc:0.88380\tval-auc:0.88892\n",
      "[5680]\ttrain-auc:0.90000\ttest-auc:0.88382\tval-auc:0.88892\n",
      "[5690]\ttrain-auc:0.90003\ttest-auc:0.88383\tval-auc:0.88893\n",
      "[5700]\ttrain-auc:0.90006\ttest-auc:0.88385\tval-auc:0.88895\n",
      "[5710]\ttrain-auc:0.90009\ttest-auc:0.88387\tval-auc:0.88896\n",
      "[5720]\ttrain-auc:0.90012\ttest-auc:0.88387\tval-auc:0.88897\n",
      "[5730]\ttrain-auc:0.90015\ttest-auc:0.88388\tval-auc:0.88897\n",
      "[5740]\ttrain-auc:0.90018\ttest-auc:0.88389\tval-auc:0.88898\n",
      "[5750]\ttrain-auc:0.90021\ttest-auc:0.88391\tval-auc:0.88899\n",
      "[5760]\ttrain-auc:0.90025\ttest-auc:0.88392\tval-auc:0.88899\n",
      "[5770]\ttrain-auc:0.90028\ttest-auc:0.88394\tval-auc:0.88901\n",
      "[5780]\ttrain-auc:0.90031\ttest-auc:0.88394\tval-auc:0.88902\n",
      "[5790]\ttrain-auc:0.90034\ttest-auc:0.88396\tval-auc:0.88903\n",
      "[5800]\ttrain-auc:0.90037\ttest-auc:0.88396\tval-auc:0.88903\n",
      "[5810]\ttrain-auc:0.90041\ttest-auc:0.88396\tval-auc:0.88904\n",
      "[5820]\ttrain-auc:0.90045\ttest-auc:0.88398\tval-auc:0.88907\n",
      "[5830]\ttrain-auc:0.90049\ttest-auc:0.88400\tval-auc:0.88909\n",
      "[5840]\ttrain-auc:0.90052\ttest-auc:0.88402\tval-auc:0.88910\n",
      "[5850]\ttrain-auc:0.90055\ttest-auc:0.88403\tval-auc:0.88910\n",
      "[5860]\ttrain-auc:0.90058\ttest-auc:0.88404\tval-auc:0.88910\n",
      "[5870]\ttrain-auc:0.90062\ttest-auc:0.88406\tval-auc:0.88912\n",
      "[5880]\ttrain-auc:0.90065\ttest-auc:0.88407\tval-auc:0.88912\n",
      "[5890]\ttrain-auc:0.90068\ttest-auc:0.88408\tval-auc:0.88913\n",
      "[5900]\ttrain-auc:0.90071\ttest-auc:0.88409\tval-auc:0.88914\n",
      "[5910]\ttrain-auc:0.90075\ttest-auc:0.88410\tval-auc:0.88915\n",
      "[5920]\ttrain-auc:0.90078\ttest-auc:0.88411\tval-auc:0.88914\n",
      "[5930]\ttrain-auc:0.90082\ttest-auc:0.88412\tval-auc:0.88916\n",
      "[5940]\ttrain-auc:0.90085\ttest-auc:0.88414\tval-auc:0.88918\n",
      "[5950]\ttrain-auc:0.90089\ttest-auc:0.88415\tval-auc:0.88920\n",
      "[5960]\ttrain-auc:0.90092\ttest-auc:0.88417\tval-auc:0.88921\n",
      "[5970]\ttrain-auc:0.90096\ttest-auc:0.88419\tval-auc:0.88922\n",
      "[5980]\ttrain-auc:0.90098\ttest-auc:0.88419\tval-auc:0.88922\n",
      "[5990]\ttrain-auc:0.90101\ttest-auc:0.88420\tval-auc:0.88923\n",
      "[6000]\ttrain-auc:0.90104\ttest-auc:0.88421\tval-auc:0.88924\n",
      "[6010]\ttrain-auc:0.90107\ttest-auc:0.88422\tval-auc:0.88926\n",
      "[6020]\ttrain-auc:0.90110\ttest-auc:0.88423\tval-auc:0.88926\n",
      "[6030]\ttrain-auc:0.90113\ttest-auc:0.88423\tval-auc:0.88927\n",
      "[6040]\ttrain-auc:0.90116\ttest-auc:0.88424\tval-auc:0.88929\n",
      "[6050]\ttrain-auc:0.90120\ttest-auc:0.88425\tval-auc:0.88931\n",
      "[6060]\ttrain-auc:0.90123\ttest-auc:0.88425\tval-auc:0.88931\n",
      "[6070]\ttrain-auc:0.90126\ttest-auc:0.88426\tval-auc:0.88932\n",
      "[6080]\ttrain-auc:0.90128\ttest-auc:0.88426\tval-auc:0.88933\n",
      "[6090]\ttrain-auc:0.90132\ttest-auc:0.88427\tval-auc:0.88934\n",
      "[6100]\ttrain-auc:0.90135\ttest-auc:0.88428\tval-auc:0.88935\n",
      "[6110]\ttrain-auc:0.90138\ttest-auc:0.88429\tval-auc:0.88935\n",
      "[6120]\ttrain-auc:0.90141\ttest-auc:0.88431\tval-auc:0.88937\n",
      "[6130]\ttrain-auc:0.90145\ttest-auc:0.88434\tval-auc:0.88939\n",
      "[6140]\ttrain-auc:0.90148\ttest-auc:0.88435\tval-auc:0.88940\n",
      "[6150]\ttrain-auc:0.90151\ttest-auc:0.88436\tval-auc:0.88942\n",
      "[6160]\ttrain-auc:0.90155\ttest-auc:0.88436\tval-auc:0.88943\n",
      "[6170]\ttrain-auc:0.90157\ttest-auc:0.88436\tval-auc:0.88943\n",
      "[6180]\ttrain-auc:0.90161\ttest-auc:0.88438\tval-auc:0.88945\n",
      "[6190]\ttrain-auc:0.90165\ttest-auc:0.88439\tval-auc:0.88946\n",
      "[6200]\ttrain-auc:0.90168\ttest-auc:0.88440\tval-auc:0.88948\n",
      "[6210]\ttrain-auc:0.90173\ttest-auc:0.88442\tval-auc:0.88950\n",
      "[6220]\ttrain-auc:0.90176\ttest-auc:0.88443\tval-auc:0.88950\n",
      "[6230]\ttrain-auc:0.90179\ttest-auc:0.88445\tval-auc:0.88951\n",
      "[6240]\ttrain-auc:0.90182\ttest-auc:0.88446\tval-auc:0.88952\n",
      "[6250]\ttrain-auc:0.90185\ttest-auc:0.88447\tval-auc:0.88953\n",
      "[6260]\ttrain-auc:0.90189\ttest-auc:0.88449\tval-auc:0.88954\n",
      "[6270]\ttrain-auc:0.90193\ttest-auc:0.88450\tval-auc:0.88956\n",
      "[6280]\ttrain-auc:0.90196\ttest-auc:0.88453\tval-auc:0.88960\n",
      "[6290]\ttrain-auc:0.90199\ttest-auc:0.88454\tval-auc:0.88962\n",
      "[6300]\ttrain-auc:0.90202\ttest-auc:0.88455\tval-auc:0.88962\n",
      "[6310]\ttrain-auc:0.90205\ttest-auc:0.88456\tval-auc:0.88962\n",
      "[6320]\ttrain-auc:0.90208\ttest-auc:0.88457\tval-auc:0.88964\n",
      "[6330]\ttrain-auc:0.90211\ttest-auc:0.88458\tval-auc:0.88964\n",
      "[6340]\ttrain-auc:0.90214\ttest-auc:0.88458\tval-auc:0.88965\n",
      "[6350]\ttrain-auc:0.90217\ttest-auc:0.88459\tval-auc:0.88966\n",
      "[6360]\ttrain-auc:0.90220\ttest-auc:0.88460\tval-auc:0.88968\n",
      "[6370]\ttrain-auc:0.90223\ttest-auc:0.88461\tval-auc:0.88969\n",
      "[6380]\ttrain-auc:0.90226\ttest-auc:0.88462\tval-auc:0.88970\n",
      "[6390]\ttrain-auc:0.90229\ttest-auc:0.88462\tval-auc:0.88971\n",
      "[6400]\ttrain-auc:0.90232\ttest-auc:0.88464\tval-auc:0.88973\n",
      "[6410]\ttrain-auc:0.90235\ttest-auc:0.88465\tval-auc:0.88974\n",
      "[6420]\ttrain-auc:0.90238\ttest-auc:0.88466\tval-auc:0.88975\n",
      "[6430]\ttrain-auc:0.90241\ttest-auc:0.88467\tval-auc:0.88975\n",
      "[6440]\ttrain-auc:0.90244\ttest-auc:0.88468\tval-auc:0.88975\n",
      "[6450]\ttrain-auc:0.90247\ttest-auc:0.88468\tval-auc:0.88975\n",
      "[6460]\ttrain-auc:0.90251\ttest-auc:0.88468\tval-auc:0.88976\n",
      "[6470]\ttrain-auc:0.90254\ttest-auc:0.88470\tval-auc:0.88976\n",
      "[6480]\ttrain-auc:0.90257\ttest-auc:0.88471\tval-auc:0.88976\n",
      "[6490]\ttrain-auc:0.90260\ttest-auc:0.88472\tval-auc:0.88977\n",
      "[6500]\ttrain-auc:0.90263\ttest-auc:0.88473\tval-auc:0.88977\n",
      "[6510]\ttrain-auc:0.90265\ttest-auc:0.88474\tval-auc:0.88979\n",
      "[6520]\ttrain-auc:0.90268\ttest-auc:0.88475\tval-auc:0.88979\n",
      "[6530]\ttrain-auc:0.90271\ttest-auc:0.88476\tval-auc:0.88980\n",
      "[6540]\ttrain-auc:0.90274\ttest-auc:0.88477\tval-auc:0.88982\n",
      "[6550]\ttrain-auc:0.90277\ttest-auc:0.88478\tval-auc:0.88982\n",
      "[6560]\ttrain-auc:0.90280\ttest-auc:0.88480\tval-auc:0.88985\n",
      "[6570]\ttrain-auc:0.90284\ttest-auc:0.88481\tval-auc:0.88987\n",
      "[6580]\ttrain-auc:0.90287\ttest-auc:0.88482\tval-auc:0.88989\n",
      "[6590]\ttrain-auc:0.90290\ttest-auc:0.88484\tval-auc:0.88990\n",
      "[6600]\ttrain-auc:0.90293\ttest-auc:0.88485\tval-auc:0.88990\n",
      "[6610]\ttrain-auc:0.90297\ttest-auc:0.88487\tval-auc:0.88992\n",
      "[6620]\ttrain-auc:0.90300\ttest-auc:0.88488\tval-auc:0.88992\n",
      "[6630]\ttrain-auc:0.90303\ttest-auc:0.88488\tval-auc:0.88994\n",
      "[6640]\ttrain-auc:0.90306\ttest-auc:0.88489\tval-auc:0.88996\n",
      "[6650]\ttrain-auc:0.90309\ttest-auc:0.88490\tval-auc:0.88996\n",
      "[6660]\ttrain-auc:0.90313\ttest-auc:0.88491\tval-auc:0.88998\n",
      "[6670]\ttrain-auc:0.90316\ttest-auc:0.88491\tval-auc:0.88998\n",
      "[6680]\ttrain-auc:0.90319\ttest-auc:0.88492\tval-auc:0.88999\n",
      "[6690]\ttrain-auc:0.90322\ttest-auc:0.88494\tval-auc:0.89000\n",
      "[6700]\ttrain-auc:0.90324\ttest-auc:0.88494\tval-auc:0.89001\n",
      "[6710]\ttrain-auc:0.90327\ttest-auc:0.88495\tval-auc:0.89002\n",
      "[6720]\ttrain-auc:0.90329\ttest-auc:0.88496\tval-auc:0.89003\n",
      "[6730]\ttrain-auc:0.90333\ttest-auc:0.88498\tval-auc:0.89004\n",
      "[6740]\ttrain-auc:0.90337\ttest-auc:0.88500\tval-auc:0.89005\n",
      "[6750]\ttrain-auc:0.90340\ttest-auc:0.88502\tval-auc:0.89006\n",
      "[6760]\ttrain-auc:0.90343\ttest-auc:0.88502\tval-auc:0.89007\n",
      "[6770]\ttrain-auc:0.90346\ttest-auc:0.88504\tval-auc:0.89009\n",
      "[6780]\ttrain-auc:0.90349\ttest-auc:0.88504\tval-auc:0.89010\n",
      "[6790]\ttrain-auc:0.90352\ttest-auc:0.88506\tval-auc:0.89011\n",
      "[6800]\ttrain-auc:0.90354\ttest-auc:0.88507\tval-auc:0.89013\n",
      "[6810]\ttrain-auc:0.90357\ttest-auc:0.88507\tval-auc:0.89013\n",
      "[6820]\ttrain-auc:0.90360\ttest-auc:0.88508\tval-auc:0.89013\n",
      "[6830]\ttrain-auc:0.90363\ttest-auc:0.88509\tval-auc:0.89014\n",
      "[6840]\ttrain-auc:0.90365\ttest-auc:0.88510\tval-auc:0.89015\n",
      "[6850]\ttrain-auc:0.90368\ttest-auc:0.88510\tval-auc:0.89016\n",
      "[6860]\ttrain-auc:0.90371\ttest-auc:0.88511\tval-auc:0.89017\n",
      "[6870]\ttrain-auc:0.90374\ttest-auc:0.88511\tval-auc:0.89018\n",
      "[6880]\ttrain-auc:0.90377\ttest-auc:0.88513\tval-auc:0.89018\n",
      "[6890]\ttrain-auc:0.90380\ttest-auc:0.88514\tval-auc:0.89018\n",
      "[6900]\ttrain-auc:0.90383\ttest-auc:0.88515\tval-auc:0.89019\n",
      "[6910]\ttrain-auc:0.90385\ttest-auc:0.88516\tval-auc:0.89021\n",
      "[6920]\ttrain-auc:0.90389\ttest-auc:0.88518\tval-auc:0.89024\n",
      "[6930]\ttrain-auc:0.90392\ttest-auc:0.88520\tval-auc:0.89025\n",
      "[6940]\ttrain-auc:0.90396\ttest-auc:0.88521\tval-auc:0.89026\n",
      "[6950]\ttrain-auc:0.90399\ttest-auc:0.88521\tval-auc:0.89026\n",
      "[6960]\ttrain-auc:0.90401\ttest-auc:0.88521\tval-auc:0.89026\n",
      "[6970]\ttrain-auc:0.90403\ttest-auc:0.88522\tval-auc:0.89028\n",
      "[6980]\ttrain-auc:0.90407\ttest-auc:0.88524\tval-auc:0.89030\n",
      "[6990]\ttrain-auc:0.90411\ttest-auc:0.88526\tval-auc:0.89032\n",
      "[7000]\ttrain-auc:0.90414\ttest-auc:0.88527\tval-auc:0.89035\n",
      "[7010]\ttrain-auc:0.90416\ttest-auc:0.88527\tval-auc:0.89036\n",
      "[7020]\ttrain-auc:0.90419\ttest-auc:0.88528\tval-auc:0.89036\n",
      "[7030]\ttrain-auc:0.90421\ttest-auc:0.88528\tval-auc:0.89037\n",
      "[7040]\ttrain-auc:0.90424\ttest-auc:0.88528\tval-auc:0.89038\n",
      "[7050]\ttrain-auc:0.90427\ttest-auc:0.88531\tval-auc:0.89039\n",
      "[7060]\ttrain-auc:0.90430\ttest-auc:0.88531\tval-auc:0.89041\n",
      "[7070]\ttrain-auc:0.90433\ttest-auc:0.88532\tval-auc:0.89042\n",
      "[7080]\ttrain-auc:0.90436\ttest-auc:0.88533\tval-auc:0.89043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7090]\ttrain-auc:0.90439\ttest-auc:0.88534\tval-auc:0.89044\n",
      "[7100]\ttrain-auc:0.90442\ttest-auc:0.88535\tval-auc:0.89045\n",
      "[7110]\ttrain-auc:0.90445\ttest-auc:0.88536\tval-auc:0.89046\n",
      "[7120]\ttrain-auc:0.90448\ttest-auc:0.88537\tval-auc:0.89047\n",
      "[7130]\ttrain-auc:0.90451\ttest-auc:0.88538\tval-auc:0.89048\n",
      "[7140]\ttrain-auc:0.90455\ttest-auc:0.88541\tval-auc:0.89051\n",
      "[7150]\ttrain-auc:0.90458\ttest-auc:0.88542\tval-auc:0.89051\n",
      "[7160]\ttrain-auc:0.90460\ttest-auc:0.88542\tval-auc:0.89051\n",
      "[7170]\ttrain-auc:0.90463\ttest-auc:0.88543\tval-auc:0.89052\n",
      "[7180]\ttrain-auc:0.90465\ttest-auc:0.88544\tval-auc:0.89051\n",
      "[7190]\ttrain-auc:0.90468\ttest-auc:0.88545\tval-auc:0.89051\n",
      "[7200]\ttrain-auc:0.90470\ttest-auc:0.88546\tval-auc:0.89052\n",
      "[7210]\ttrain-auc:0.90473\ttest-auc:0.88546\tval-auc:0.89052\n",
      "[7220]\ttrain-auc:0.90476\ttest-auc:0.88546\tval-auc:0.89053\n",
      "[7230]\ttrain-auc:0.90479\ttest-auc:0.88547\tval-auc:0.89054\n",
      "[7240]\ttrain-auc:0.90481\ttest-auc:0.88548\tval-auc:0.89054\n",
      "[7250]\ttrain-auc:0.90483\ttest-auc:0.88549\tval-auc:0.89055\n",
      "[7260]\ttrain-auc:0.90486\ttest-auc:0.88550\tval-auc:0.89056\n",
      "[7270]\ttrain-auc:0.90489\ttest-auc:0.88551\tval-auc:0.89057\n",
      "[7280]\ttrain-auc:0.90493\ttest-auc:0.88552\tval-auc:0.89058\n",
      "[7290]\ttrain-auc:0.90495\ttest-auc:0.88553\tval-auc:0.89059\n",
      "[7300]\ttrain-auc:0.90498\ttest-auc:0.88553\tval-auc:0.89059\n",
      "[7310]\ttrain-auc:0.90501\ttest-auc:0.88554\tval-auc:0.89060\n",
      "[7320]\ttrain-auc:0.90504\ttest-auc:0.88555\tval-auc:0.89060\n",
      "[7330]\ttrain-auc:0.90507\ttest-auc:0.88555\tval-auc:0.89061\n",
      "[7340]\ttrain-auc:0.90509\ttest-auc:0.88556\tval-auc:0.89062\n",
      "[7350]\ttrain-auc:0.90512\ttest-auc:0.88557\tval-auc:0.89064\n",
      "[7360]\ttrain-auc:0.90514\ttest-auc:0.88558\tval-auc:0.89065\n",
      "[7370]\ttrain-auc:0.90516\ttest-auc:0.88558\tval-auc:0.89065\n",
      "[7380]\ttrain-auc:0.90519\ttest-auc:0.88560\tval-auc:0.89066\n",
      "[7390]\ttrain-auc:0.90522\ttest-auc:0.88560\tval-auc:0.89067\n",
      "[7400]\ttrain-auc:0.90524\ttest-auc:0.88561\tval-auc:0.89069\n",
      "[7410]\ttrain-auc:0.90527\ttest-auc:0.88562\tval-auc:0.89069\n",
      "[7420]\ttrain-auc:0.90530\ttest-auc:0.88563\tval-auc:0.89070\n",
      "[7430]\ttrain-auc:0.90533\ttest-auc:0.88564\tval-auc:0.89070\n",
      "[7440]\ttrain-auc:0.90536\ttest-auc:0.88566\tval-auc:0.89072\n",
      "[7450]\ttrain-auc:0.90539\ttest-auc:0.88567\tval-auc:0.89072\n",
      "[7460]\ttrain-auc:0.90542\ttest-auc:0.88568\tval-auc:0.89074\n",
      "[7470]\ttrain-auc:0.90545\ttest-auc:0.88569\tval-auc:0.89075\n",
      "[7480]\ttrain-auc:0.90547\ttest-auc:0.88570\tval-auc:0.89076\n",
      "[7490]\ttrain-auc:0.90550\ttest-auc:0.88570\tval-auc:0.89077\n",
      "[7500]\ttrain-auc:0.90553\ttest-auc:0.88571\tval-auc:0.89078\n",
      "[7510]\ttrain-auc:0.90555\ttest-auc:0.88572\tval-auc:0.89079\n",
      "[7520]\ttrain-auc:0.90558\ttest-auc:0.88573\tval-auc:0.89079\n",
      "[7530]\ttrain-auc:0.90561\ttest-auc:0.88574\tval-auc:0.89079\n",
      "[7540]\ttrain-auc:0.90563\ttest-auc:0.88575\tval-auc:0.89080\n",
      "[7550]\ttrain-auc:0.90566\ttest-auc:0.88576\tval-auc:0.89080\n",
      "[7560]\ttrain-auc:0.90569\ttest-auc:0.88577\tval-auc:0.89081\n",
      "[7570]\ttrain-auc:0.90571\ttest-auc:0.88577\tval-auc:0.89082\n",
      "[7580]\ttrain-auc:0.90573\ttest-auc:0.88578\tval-auc:0.89083\n",
      "[7590]\ttrain-auc:0.90577\ttest-auc:0.88580\tval-auc:0.89084\n",
      "[7600]\ttrain-auc:0.90579\ttest-auc:0.88580\tval-auc:0.89085\n",
      "[7610]\ttrain-auc:0.90582\ttest-auc:0.88581\tval-auc:0.89085\n",
      "[7620]\ttrain-auc:0.90585\ttest-auc:0.88583\tval-auc:0.89087\n",
      "[7630]\ttrain-auc:0.90588\ttest-auc:0.88584\tval-auc:0.89088\n",
      "[7640]\ttrain-auc:0.90591\ttest-auc:0.88584\tval-auc:0.89088\n",
      "[7650]\ttrain-auc:0.90593\ttest-auc:0.88585\tval-auc:0.89089\n",
      "[7660]\ttrain-auc:0.90596\ttest-auc:0.88586\tval-auc:0.89089\n",
      "[7670]\ttrain-auc:0.90599\ttest-auc:0.88588\tval-auc:0.89090\n",
      "[7680]\ttrain-auc:0.90602\ttest-auc:0.88588\tval-auc:0.89092\n",
      "[7690]\ttrain-auc:0.90604\ttest-auc:0.88589\tval-auc:0.89093\n",
      "[7700]\ttrain-auc:0.90608\ttest-auc:0.88590\tval-auc:0.89096\n",
      "[7710]\ttrain-auc:0.90610\ttest-auc:0.88591\tval-auc:0.89097\n",
      "[7720]\ttrain-auc:0.90612\ttest-auc:0.88592\tval-auc:0.89097\n",
      "[7730]\ttrain-auc:0.90615\ttest-auc:0.88592\tval-auc:0.89099\n",
      "[7740]\ttrain-auc:0.90618\ttest-auc:0.88593\tval-auc:0.89100\n",
      "[7750]\ttrain-auc:0.90620\ttest-auc:0.88593\tval-auc:0.89101\n",
      "[7760]\ttrain-auc:0.90623\ttest-auc:0.88594\tval-auc:0.89103\n",
      "[7770]\ttrain-auc:0.90626\ttest-auc:0.88595\tval-auc:0.89104\n",
      "[7780]\ttrain-auc:0.90629\ttest-auc:0.88596\tval-auc:0.89105\n",
      "[7790]\ttrain-auc:0.90631\ttest-auc:0.88597\tval-auc:0.89105\n",
      "[7800]\ttrain-auc:0.90634\ttest-auc:0.88597\tval-auc:0.89105\n",
      "[7810]\ttrain-auc:0.90636\ttest-auc:0.88598\tval-auc:0.89106\n",
      "[7820]\ttrain-auc:0.90639\ttest-auc:0.88599\tval-auc:0.89107\n",
      "[7830]\ttrain-auc:0.90641\ttest-auc:0.88600\tval-auc:0.89107\n",
      "[7840]\ttrain-auc:0.90644\ttest-auc:0.88601\tval-auc:0.89108\n",
      "[7850]\ttrain-auc:0.90646\ttest-auc:0.88601\tval-auc:0.89108\n",
      "[7860]\ttrain-auc:0.90649\ttest-auc:0.88602\tval-auc:0.89109\n",
      "[7870]\ttrain-auc:0.90652\ttest-auc:0.88603\tval-auc:0.89109\n",
      "[7880]\ttrain-auc:0.90655\ttest-auc:0.88604\tval-auc:0.89110\n",
      "[7890]\ttrain-auc:0.90658\ttest-auc:0.88606\tval-auc:0.89113\n",
      "[7900]\ttrain-auc:0.90660\ttest-auc:0.88606\tval-auc:0.89113\n",
      "[7910]\ttrain-auc:0.90662\ttest-auc:0.88608\tval-auc:0.89115\n",
      "[7920]\ttrain-auc:0.90666\ttest-auc:0.88609\tval-auc:0.89116\n",
      "[7930]\ttrain-auc:0.90669\ttest-auc:0.88610\tval-auc:0.89116\n",
      "[7940]\ttrain-auc:0.90671\ttest-auc:0.88610\tval-auc:0.89116\n",
      "[7950]\ttrain-auc:0.90673\ttest-auc:0.88611\tval-auc:0.89117\n",
      "[7960]\ttrain-auc:0.90676\ttest-auc:0.88612\tval-auc:0.89118\n",
      "[7970]\ttrain-auc:0.90678\ttest-auc:0.88612\tval-auc:0.89117\n",
      "[7980]\ttrain-auc:0.90681\ttest-auc:0.88614\tval-auc:0.89118\n",
      "[7990]\ttrain-auc:0.90683\ttest-auc:0.88614\tval-auc:0.89119\n",
      "[8000]\ttrain-auc:0.90686\ttest-auc:0.88615\tval-auc:0.89120\n",
      "[8010]\ttrain-auc:0.90688\ttest-auc:0.88616\tval-auc:0.89121\n",
      "[8020]\ttrain-auc:0.90691\ttest-auc:0.88616\tval-auc:0.89122\n",
      "[8030]\ttrain-auc:0.90694\ttest-auc:0.88617\tval-auc:0.89122\n",
      "[8040]\ttrain-auc:0.90697\ttest-auc:0.88618\tval-auc:0.89124\n",
      "[8050]\ttrain-auc:0.90700\ttest-auc:0.88619\tval-auc:0.89126\n",
      "[8060]\ttrain-auc:0.90702\ttest-auc:0.88619\tval-auc:0.89127\n",
      "[8070]\ttrain-auc:0.90705\ttest-auc:0.88620\tval-auc:0.89128\n",
      "[8080]\ttrain-auc:0.90708\ttest-auc:0.88621\tval-auc:0.89129\n",
      "[8090]\ttrain-auc:0.90710\ttest-auc:0.88621\tval-auc:0.89130\n",
      "[8100]\ttrain-auc:0.90712\ttest-auc:0.88622\tval-auc:0.89130\n",
      "[8110]\ttrain-auc:0.90715\ttest-auc:0.88623\tval-auc:0.89130\n",
      "[8120]\ttrain-auc:0.90718\ttest-auc:0.88624\tval-auc:0.89132\n",
      "[8130]\ttrain-auc:0.90720\ttest-auc:0.88625\tval-auc:0.89133\n",
      "[8140]\ttrain-auc:0.90723\ttest-auc:0.88626\tval-auc:0.89133\n",
      "[8150]\ttrain-auc:0.90725\ttest-auc:0.88627\tval-auc:0.89134\n",
      "[8160]\ttrain-auc:0.90728\ttest-auc:0.88627\tval-auc:0.89136\n",
      "[8170]\ttrain-auc:0.90730\ttest-auc:0.88627\tval-auc:0.89136\n",
      "[8180]\ttrain-auc:0.90733\ttest-auc:0.88628\tval-auc:0.89137\n",
      "[8190]\ttrain-auc:0.90735\ttest-auc:0.88628\tval-auc:0.89137\n",
      "[8200]\ttrain-auc:0.90737\ttest-auc:0.88628\tval-auc:0.89138\n",
      "[8210]\ttrain-auc:0.90740\ttest-auc:0.88630\tval-auc:0.89140\n",
      "[8220]\ttrain-auc:0.90742\ttest-auc:0.88630\tval-auc:0.89139\n",
      "[8230]\ttrain-auc:0.90744\ttest-auc:0.88630\tval-auc:0.89140\n",
      "[8240]\ttrain-auc:0.90746\ttest-auc:0.88631\tval-auc:0.89141\n",
      "[8250]\ttrain-auc:0.90749\ttest-auc:0.88631\tval-auc:0.89141\n",
      "[8260]\ttrain-auc:0.90752\ttest-auc:0.88632\tval-auc:0.89142\n",
      "[8270]\ttrain-auc:0.90754\ttest-auc:0.88632\tval-auc:0.89142\n",
      "[8280]\ttrain-auc:0.90757\ttest-auc:0.88633\tval-auc:0.89143\n",
      "[8290]\ttrain-auc:0.90760\ttest-auc:0.88633\tval-auc:0.89144\n",
      "[8300]\ttrain-auc:0.90762\ttest-auc:0.88634\tval-auc:0.89144\n",
      "[8310]\ttrain-auc:0.90766\ttest-auc:0.88636\tval-auc:0.89147\n",
      "[8320]\ttrain-auc:0.90768\ttest-auc:0.88637\tval-auc:0.89148\n",
      "[8330]\ttrain-auc:0.90771\ttest-auc:0.88637\tval-auc:0.89148\n",
      "[8340]\ttrain-auc:0.90773\ttest-auc:0.88638\tval-auc:0.89150\n",
      "[8350]\ttrain-auc:0.90776\ttest-auc:0.88639\tval-auc:0.89150\n",
      "[8360]\ttrain-auc:0.90778\ttest-auc:0.88640\tval-auc:0.89151\n",
      "[8370]\ttrain-auc:0.90781\ttest-auc:0.88640\tval-auc:0.89151\n",
      "[8380]\ttrain-auc:0.90783\ttest-auc:0.88641\tval-auc:0.89152\n",
      "[8390]\ttrain-auc:0.90785\ttest-auc:0.88642\tval-auc:0.89153\n",
      "[8400]\ttrain-auc:0.90788\ttest-auc:0.88642\tval-auc:0.89154\n",
      "[8410]\ttrain-auc:0.90791\ttest-auc:0.88644\tval-auc:0.89155\n",
      "[8420]\ttrain-auc:0.90793\ttest-auc:0.88644\tval-auc:0.89155\n",
      "[8430]\ttrain-auc:0.90795\ttest-auc:0.88645\tval-auc:0.89155\n",
      "[8440]\ttrain-auc:0.90798\ttest-auc:0.88647\tval-auc:0.89157\n",
      "[8450]\ttrain-auc:0.90801\ttest-auc:0.88648\tval-auc:0.89157\n",
      "[8460]\ttrain-auc:0.90803\ttest-auc:0.88648\tval-auc:0.89158\n",
      "[8470]\ttrain-auc:0.90805\ttest-auc:0.88649\tval-auc:0.89158\n",
      "[8480]\ttrain-auc:0.90807\ttest-auc:0.88649\tval-auc:0.89159\n",
      "[8490]\ttrain-auc:0.90809\ttest-auc:0.88650\tval-auc:0.89159\n",
      "[8500]\ttrain-auc:0.90812\ttest-auc:0.88650\tval-auc:0.89160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8510]\ttrain-auc:0.90815\ttest-auc:0.88651\tval-auc:0.89160\n",
      "[8520]\ttrain-auc:0.90817\ttest-auc:0.88652\tval-auc:0.89162\n",
      "[8530]\ttrain-auc:0.90819\ttest-auc:0.88652\tval-auc:0.89162\n",
      "[8540]\ttrain-auc:0.90822\ttest-auc:0.88653\tval-auc:0.89164\n",
      "[8550]\ttrain-auc:0.90824\ttest-auc:0.88654\tval-auc:0.89164\n",
      "[8560]\ttrain-auc:0.90827\ttest-auc:0.88654\tval-auc:0.89165\n",
      "[8570]\ttrain-auc:0.90829\ttest-auc:0.88654\tval-auc:0.89166\n",
      "[8580]\ttrain-auc:0.90832\ttest-auc:0.88655\tval-auc:0.89166\n",
      "[8590]\ttrain-auc:0.90834\ttest-auc:0.88655\tval-auc:0.89167\n",
      "[8600]\ttrain-auc:0.90837\ttest-auc:0.88655\tval-auc:0.89167\n",
      "[8610]\ttrain-auc:0.90839\ttest-auc:0.88656\tval-auc:0.89169\n",
      "[8620]\ttrain-auc:0.90842\ttest-auc:0.88657\tval-auc:0.89171\n",
      "[8630]\ttrain-auc:0.90845\ttest-auc:0.88657\tval-auc:0.89172\n",
      "[8640]\ttrain-auc:0.90847\ttest-auc:0.88658\tval-auc:0.89173\n",
      "[8650]\ttrain-auc:0.90849\ttest-auc:0.88658\tval-auc:0.89174\n",
      "[8660]\ttrain-auc:0.90852\ttest-auc:0.88659\tval-auc:0.89175\n",
      "[8670]\ttrain-auc:0.90855\ttest-auc:0.88660\tval-auc:0.89177\n",
      "[8680]\ttrain-auc:0.90857\ttest-auc:0.88660\tval-auc:0.89179\n",
      "[8690]\ttrain-auc:0.90860\ttest-auc:0.88661\tval-auc:0.89180\n",
      "[8700]\ttrain-auc:0.90862\ttest-auc:0.88662\tval-auc:0.89180\n",
      "[8710]\ttrain-auc:0.90865\ttest-auc:0.88662\tval-auc:0.89180\n",
      "[8720]\ttrain-auc:0.90867\ttest-auc:0.88663\tval-auc:0.89180\n",
      "[8730]\ttrain-auc:0.90870\ttest-auc:0.88665\tval-auc:0.89180\n",
      "[8740]\ttrain-auc:0.90872\ttest-auc:0.88665\tval-auc:0.89181\n",
      "[8750]\ttrain-auc:0.90874\ttest-auc:0.88666\tval-auc:0.89181\n",
      "[8760]\ttrain-auc:0.90877\ttest-auc:0.88667\tval-auc:0.89181\n",
      "[8770]\ttrain-auc:0.90879\ttest-auc:0.88668\tval-auc:0.89181\n",
      "[8780]\ttrain-auc:0.90881\ttest-auc:0.88668\tval-auc:0.89182\n",
      "[8790]\ttrain-auc:0.90884\ttest-auc:0.88669\tval-auc:0.89182\n",
      "[8800]\ttrain-auc:0.90886\ttest-auc:0.88670\tval-auc:0.89183\n",
      "[8810]\ttrain-auc:0.90889\ttest-auc:0.88671\tval-auc:0.89183\n",
      "[8820]\ttrain-auc:0.90891\ttest-auc:0.88672\tval-auc:0.89184\n",
      "[8830]\ttrain-auc:0.90894\ttest-auc:0.88672\tval-auc:0.89185\n",
      "[8840]\ttrain-auc:0.90897\ttest-auc:0.88674\tval-auc:0.89187\n",
      "[8850]\ttrain-auc:0.90900\ttest-auc:0.88675\tval-auc:0.89189\n",
      "[8860]\ttrain-auc:0.90902\ttest-auc:0.88676\tval-auc:0.89189\n",
      "[8870]\ttrain-auc:0.90904\ttest-auc:0.88676\tval-auc:0.89189\n",
      "[8880]\ttrain-auc:0.90907\ttest-auc:0.88676\tval-auc:0.89190\n",
      "[8890]\ttrain-auc:0.90909\ttest-auc:0.88677\tval-auc:0.89192\n",
      "[8900]\ttrain-auc:0.90911\ttest-auc:0.88677\tval-auc:0.89191\n",
      "[8910]\ttrain-auc:0.90914\ttest-auc:0.88678\tval-auc:0.89192\n",
      "[8920]\ttrain-auc:0.90916\ttest-auc:0.88679\tval-auc:0.89194\n",
      "[8930]\ttrain-auc:0.90918\ttest-auc:0.88679\tval-auc:0.89195\n",
      "[8940]\ttrain-auc:0.90921\ttest-auc:0.88680\tval-auc:0.89195\n",
      "[8950]\ttrain-auc:0.90924\ttest-auc:0.88681\tval-auc:0.89196\n",
      "[8960]\ttrain-auc:0.90926\ttest-auc:0.88683\tval-auc:0.89198\n",
      "[8970]\ttrain-auc:0.90929\ttest-auc:0.88684\tval-auc:0.89199\n",
      "[8980]\ttrain-auc:0.90932\ttest-auc:0.88684\tval-auc:0.89200\n",
      "[8990]\ttrain-auc:0.90935\ttest-auc:0.88686\tval-auc:0.89202\n",
      "[9000]\ttrain-auc:0.90937\ttest-auc:0.88686\tval-auc:0.89202\n",
      "[9010]\ttrain-auc:0.90939\ttest-auc:0.88686\tval-auc:0.89202\n",
      "[9020]\ttrain-auc:0.90942\ttest-auc:0.88687\tval-auc:0.89203\n",
      "[9030]\ttrain-auc:0.90944\ttest-auc:0.88688\tval-auc:0.89204\n",
      "[9040]\ttrain-auc:0.90946\ttest-auc:0.88688\tval-auc:0.89204\n",
      "[9050]\ttrain-auc:0.90949\ttest-auc:0.88689\tval-auc:0.89205\n",
      "[9060]\ttrain-auc:0.90951\ttest-auc:0.88690\tval-auc:0.89206\n",
      "[9070]\ttrain-auc:0.90954\ttest-auc:0.88690\tval-auc:0.89207\n",
      "[9080]\ttrain-auc:0.90956\ttest-auc:0.88691\tval-auc:0.89208\n",
      "[9090]\ttrain-auc:0.90959\ttest-auc:0.88691\tval-auc:0.89208\n",
      "[9100]\ttrain-auc:0.90961\ttest-auc:0.88691\tval-auc:0.89209\n",
      "[9110]\ttrain-auc:0.90964\ttest-auc:0.88692\tval-auc:0.89211\n",
      "[9120]\ttrain-auc:0.90967\ttest-auc:0.88693\tval-auc:0.89212\n",
      "[9130]\ttrain-auc:0.90969\ttest-auc:0.88694\tval-auc:0.89214\n",
      "[9140]\ttrain-auc:0.90972\ttest-auc:0.88694\tval-auc:0.89213\n",
      "[9150]\ttrain-auc:0.90974\ttest-auc:0.88695\tval-auc:0.89214\n",
      "[9160]\ttrain-auc:0.90977\ttest-auc:0.88695\tval-auc:0.89216\n",
      "[9170]\ttrain-auc:0.90979\ttest-auc:0.88697\tval-auc:0.89217\n",
      "[9180]\ttrain-auc:0.90982\ttest-auc:0.88697\tval-auc:0.89216\n",
      "[9190]\ttrain-auc:0.90984\ttest-auc:0.88697\tval-auc:0.89217\n",
      "[9200]\ttrain-auc:0.90987\ttest-auc:0.88697\tval-auc:0.89218\n",
      "[9210]\ttrain-auc:0.90989\ttest-auc:0.88699\tval-auc:0.89219\n",
      "[9220]\ttrain-auc:0.90991\ttest-auc:0.88699\tval-auc:0.89220\n",
      "[9230]\ttrain-auc:0.90993\ttest-auc:0.88700\tval-auc:0.89220\n",
      "[9240]\ttrain-auc:0.90995\ttest-auc:0.88701\tval-auc:0.89221\n",
      "[9250]\ttrain-auc:0.90997\ttest-auc:0.88701\tval-auc:0.89221\n",
      "[9260]\ttrain-auc:0.91000\ttest-auc:0.88702\tval-auc:0.89222\n",
      "[9270]\ttrain-auc:0.91001\ttest-auc:0.88702\tval-auc:0.89222\n",
      "[9280]\ttrain-auc:0.91003\ttest-auc:0.88702\tval-auc:0.89224\n",
      "[9290]\ttrain-auc:0.91006\ttest-auc:0.88703\tval-auc:0.89224\n",
      "[9300]\ttrain-auc:0.91008\ttest-auc:0.88704\tval-auc:0.89225\n",
      "[9310]\ttrain-auc:0.91009\ttest-auc:0.88704\tval-auc:0.89226\n",
      "[9320]\ttrain-auc:0.91012\ttest-auc:0.88705\tval-auc:0.89227\n",
      "[9330]\ttrain-auc:0.91014\ttest-auc:0.88706\tval-auc:0.89227\n",
      "[9340]\ttrain-auc:0.91016\ttest-auc:0.88707\tval-auc:0.89228\n",
      "[9350]\ttrain-auc:0.91019\ttest-auc:0.88707\tval-auc:0.89228\n",
      "[9360]\ttrain-auc:0.91021\ttest-auc:0.88708\tval-auc:0.89229\n",
      "[9370]\ttrain-auc:0.91023\ttest-auc:0.88708\tval-auc:0.89228\n",
      "[9380]\ttrain-auc:0.91025\ttest-auc:0.88708\tval-auc:0.89229\n",
      "[9390]\ttrain-auc:0.91027\ttest-auc:0.88708\tval-auc:0.89228\n",
      "[9400]\ttrain-auc:0.91029\ttest-auc:0.88709\tval-auc:0.89229\n",
      "[9410]\ttrain-auc:0.91031\ttest-auc:0.88709\tval-auc:0.89228\n",
      "[9420]\ttrain-auc:0.91034\ttest-auc:0.88710\tval-auc:0.89229\n",
      "[9430]\ttrain-auc:0.91036\ttest-auc:0.88710\tval-auc:0.89230\n",
      "[9440]\ttrain-auc:0.91038\ttest-auc:0.88711\tval-auc:0.89230\n",
      "[9450]\ttrain-auc:0.91040\ttest-auc:0.88711\tval-auc:0.89231\n",
      "[9460]\ttrain-auc:0.91043\ttest-auc:0.88711\tval-auc:0.89233\n",
      "[9470]\ttrain-auc:0.91044\ttest-auc:0.88712\tval-auc:0.89233\n",
      "[9480]\ttrain-auc:0.91046\ttest-auc:0.88713\tval-auc:0.89234\n",
      "[9490]\ttrain-auc:0.91048\ttest-auc:0.88714\tval-auc:0.89234\n",
      "[9500]\ttrain-auc:0.91051\ttest-auc:0.88715\tval-auc:0.89236\n",
      "[9510]\ttrain-auc:0.91053\ttest-auc:0.88716\tval-auc:0.89238\n",
      "[9520]\ttrain-auc:0.91056\ttest-auc:0.88716\tval-auc:0.89239\n",
      "[9530]\ttrain-auc:0.91058\ttest-auc:0.88717\tval-auc:0.89240\n",
      "[9540]\ttrain-auc:0.91061\ttest-auc:0.88718\tval-auc:0.89242\n",
      "[9550]\ttrain-auc:0.91063\ttest-auc:0.88719\tval-auc:0.89242\n",
      "[9560]\ttrain-auc:0.91066\ttest-auc:0.88719\tval-auc:0.89243\n",
      "[9570]\ttrain-auc:0.91069\ttest-auc:0.88720\tval-auc:0.89245\n",
      "[9580]\ttrain-auc:0.91071\ttest-auc:0.88721\tval-auc:0.89245\n",
      "[9590]\ttrain-auc:0.91073\ttest-auc:0.88721\tval-auc:0.89245\n",
      "[9600]\ttrain-auc:0.91075\ttest-auc:0.88722\tval-auc:0.89246\n",
      "[9610]\ttrain-auc:0.91078\ttest-auc:0.88722\tval-auc:0.89246\n",
      "[9620]\ttrain-auc:0.91080\ttest-auc:0.88723\tval-auc:0.89247\n",
      "[9630]\ttrain-auc:0.91083\ttest-auc:0.88724\tval-auc:0.89249\n",
      "[9640]\ttrain-auc:0.91085\ttest-auc:0.88725\tval-auc:0.89250\n",
      "[9650]\ttrain-auc:0.91088\ttest-auc:0.88726\tval-auc:0.89250\n",
      "[9660]\ttrain-auc:0.91091\ttest-auc:0.88727\tval-auc:0.89252\n",
      "[9670]\ttrain-auc:0.91093\ttest-auc:0.88727\tval-auc:0.89252\n",
      "[9680]\ttrain-auc:0.91096\ttest-auc:0.88728\tval-auc:0.89252\n",
      "[9690]\ttrain-auc:0.91098\ttest-auc:0.88730\tval-auc:0.89253\n",
      "[9700]\ttrain-auc:0.91101\ttest-auc:0.88730\tval-auc:0.89255\n",
      "[9710]\ttrain-auc:0.91103\ttest-auc:0.88731\tval-auc:0.89256\n",
      "[9720]\ttrain-auc:0.91105\ttest-auc:0.88732\tval-auc:0.89257\n",
      "[9730]\ttrain-auc:0.91108\ttest-auc:0.88733\tval-auc:0.89258\n",
      "[9740]\ttrain-auc:0.91110\ttest-auc:0.88733\tval-auc:0.89258\n",
      "[9750]\ttrain-auc:0.91112\ttest-auc:0.88734\tval-auc:0.89259\n",
      "[9760]\ttrain-auc:0.91114\ttest-auc:0.88734\tval-auc:0.89259\n",
      "[9770]\ttrain-auc:0.91117\ttest-auc:0.88736\tval-auc:0.89260\n",
      "[9780]\ttrain-auc:0.91119\ttest-auc:0.88736\tval-auc:0.89260\n",
      "[9790]\ttrain-auc:0.91122\ttest-auc:0.88737\tval-auc:0.89261\n",
      "[9800]\ttrain-auc:0.91124\ttest-auc:0.88737\tval-auc:0.89261\n",
      "[9810]\ttrain-auc:0.91126\ttest-auc:0.88737\tval-auc:0.89262\n",
      "[9820]\ttrain-auc:0.91128\ttest-auc:0.88738\tval-auc:0.89262\n",
      "[9830]\ttrain-auc:0.91131\ttest-auc:0.88739\tval-auc:0.89264\n",
      "[9840]\ttrain-auc:0.91133\ttest-auc:0.88740\tval-auc:0.89264\n",
      "[9850]\ttrain-auc:0.91135\ttest-auc:0.88740\tval-auc:0.89264\n",
      "[9860]\ttrain-auc:0.91138\ttest-auc:0.88741\tval-auc:0.89266\n",
      "[9870]\ttrain-auc:0.91141\ttest-auc:0.88744\tval-auc:0.89268\n",
      "[9880]\ttrain-auc:0.91143\ttest-auc:0.88745\tval-auc:0.89268\n",
      "[9890]\ttrain-auc:0.91145\ttest-auc:0.88745\tval-auc:0.89268\n",
      "[9900]\ttrain-auc:0.91147\ttest-auc:0.88745\tval-auc:0.89267\n",
      "[9910]\ttrain-auc:0.91149\ttest-auc:0.88746\tval-auc:0.89267\n",
      "[9920]\ttrain-auc:0.91151\ttest-auc:0.88746\tval-auc:0.89268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9930]\ttrain-auc:0.91153\ttest-auc:0.88747\tval-auc:0.89269\n",
      "[9940]\ttrain-auc:0.91155\ttest-auc:0.88747\tval-auc:0.89271\n",
      "[9950]\ttrain-auc:0.91158\ttest-auc:0.88748\tval-auc:0.89272\n",
      "[9960]\ttrain-auc:0.91160\ttest-auc:0.88748\tval-auc:0.89272\n",
      "[9970]\ttrain-auc:0.91162\ttest-auc:0.88749\tval-auc:0.89273\n",
      "[9980]\ttrain-auc:0.91164\ttest-auc:0.88750\tval-auc:0.89273\n",
      "[9990]\ttrain-auc:0.91167\ttest-auc:0.88751\tval-auc:0.89274\n",
      "[10000]\ttrain-auc:0.91169\ttest-auc:0.88752\tval-auc:0.89275\n",
      "[10010]\ttrain-auc:0.91172\ttest-auc:0.88753\tval-auc:0.89275\n",
      "[10020]\ttrain-auc:0.91173\ttest-auc:0.88753\tval-auc:0.89276\n",
      "[10030]\ttrain-auc:0.91175\ttest-auc:0.88754\tval-auc:0.89277\n",
      "[10040]\ttrain-auc:0.91178\ttest-auc:0.88754\tval-auc:0.89277\n",
      "[10050]\ttrain-auc:0.91180\ttest-auc:0.88754\tval-auc:0.89277\n",
      "[10060]\ttrain-auc:0.91182\ttest-auc:0.88755\tval-auc:0.89277\n",
      "[10070]\ttrain-auc:0.91184\ttest-auc:0.88755\tval-auc:0.89277\n",
      "[10080]\ttrain-auc:0.91186\ttest-auc:0.88756\tval-auc:0.89277\n",
      "[10090]\ttrain-auc:0.91189\ttest-auc:0.88757\tval-auc:0.89277\n",
      "[10100]\ttrain-auc:0.91191\ttest-auc:0.88757\tval-auc:0.89278\n",
      "[10110]\ttrain-auc:0.91193\ttest-auc:0.88757\tval-auc:0.89279\n",
      "[10120]\ttrain-auc:0.91195\ttest-auc:0.88757\tval-auc:0.89279\n",
      "[10130]\ttrain-auc:0.91197\ttest-auc:0.88758\tval-auc:0.89280\n",
      "[10140]\ttrain-auc:0.91199\ttest-auc:0.88758\tval-auc:0.89281\n",
      "[10150]\ttrain-auc:0.91202\ttest-auc:0.88759\tval-auc:0.89282\n",
      "[10160]\ttrain-auc:0.91204\ttest-auc:0.88760\tval-auc:0.89283\n",
      "[10170]\ttrain-auc:0.91206\ttest-auc:0.88760\tval-auc:0.89283\n",
      "[10180]\ttrain-auc:0.91208\ttest-auc:0.88761\tval-auc:0.89284\n",
      "[10190]\ttrain-auc:0.91211\ttest-auc:0.88762\tval-auc:0.89285\n",
      "[10200]\ttrain-auc:0.91213\ttest-auc:0.88763\tval-auc:0.89285\n",
      "[10210]\ttrain-auc:0.91215\ttest-auc:0.88764\tval-auc:0.89287\n",
      "[10220]\ttrain-auc:0.91217\ttest-auc:0.88765\tval-auc:0.89287\n",
      "[10230]\ttrain-auc:0.91220\ttest-auc:0.88765\tval-auc:0.89288\n",
      "[10240]\ttrain-auc:0.91222\ttest-auc:0.88766\tval-auc:0.89289\n",
      "[10250]\ttrain-auc:0.91224\ttest-auc:0.88767\tval-auc:0.89289\n",
      "[10260]\ttrain-auc:0.91226\ttest-auc:0.88767\tval-auc:0.89289\n",
      "[10270]\ttrain-auc:0.91228\ttest-auc:0.88767\tval-auc:0.89289\n",
      "[10280]\ttrain-auc:0.91231\ttest-auc:0.88769\tval-auc:0.89292\n",
      "[10290]\ttrain-auc:0.91233\ttest-auc:0.88770\tval-auc:0.89293\n",
      "[10300]\ttrain-auc:0.91235\ttest-auc:0.88770\tval-auc:0.89294\n",
      "[10310]\ttrain-auc:0.91237\ttest-auc:0.88770\tval-auc:0.89294\n",
      "[10320]\ttrain-auc:0.91239\ttest-auc:0.88771\tval-auc:0.89295\n",
      "[10330]\ttrain-auc:0.91241\ttest-auc:0.88772\tval-auc:0.89296\n",
      "[10340]\ttrain-auc:0.91244\ttest-auc:0.88773\tval-auc:0.89296\n",
      "[10350]\ttrain-auc:0.91246\ttest-auc:0.88775\tval-auc:0.89298\n",
      "[10360]\ttrain-auc:0.91249\ttest-auc:0.88775\tval-auc:0.89299\n",
      "[10370]\ttrain-auc:0.91251\ttest-auc:0.88776\tval-auc:0.89298\n",
      "[10380]\ttrain-auc:0.91254\ttest-auc:0.88777\tval-auc:0.89300\n",
      "[10390]\ttrain-auc:0.91256\ttest-auc:0.88777\tval-auc:0.89301\n",
      "[10400]\ttrain-auc:0.91258\ttest-auc:0.88777\tval-auc:0.89302\n",
      "[10410]\ttrain-auc:0.91260\ttest-auc:0.88778\tval-auc:0.89302\n",
      "[10420]\ttrain-auc:0.91262\ttest-auc:0.88778\tval-auc:0.89303\n",
      "[10430]\ttrain-auc:0.91264\ttest-auc:0.88779\tval-auc:0.89304\n",
      "[10440]\ttrain-auc:0.91267\ttest-auc:0.88779\tval-auc:0.89305\n",
      "[10450]\ttrain-auc:0.91268\ttest-auc:0.88780\tval-auc:0.89306\n",
      "[10460]\ttrain-auc:0.91270\ttest-auc:0.88780\tval-auc:0.89306\n",
      "[10470]\ttrain-auc:0.91272\ttest-auc:0.88780\tval-auc:0.89306\n",
      "[10480]\ttrain-auc:0.91275\ttest-auc:0.88781\tval-auc:0.89307\n",
      "[10490]\ttrain-auc:0.91277\ttest-auc:0.88782\tval-auc:0.89307\n",
      "[10500]\ttrain-auc:0.91279\ttest-auc:0.88782\tval-auc:0.89308\n",
      "[10510]\ttrain-auc:0.91281\ttest-auc:0.88783\tval-auc:0.89308\n",
      "[10520]\ttrain-auc:0.91283\ttest-auc:0.88784\tval-auc:0.89309\n",
      "[10530]\ttrain-auc:0.91285\ttest-auc:0.88785\tval-auc:0.89310\n",
      "[10540]\ttrain-auc:0.91287\ttest-auc:0.88786\tval-auc:0.89311\n",
      "[10550]\ttrain-auc:0.91289\ttest-auc:0.88787\tval-auc:0.89311\n",
      "[10560]\ttrain-auc:0.91292\ttest-auc:0.88787\tval-auc:0.89312\n",
      "[10570]\ttrain-auc:0.91294\ttest-auc:0.88787\tval-auc:0.89312\n",
      "[10580]\ttrain-auc:0.91297\ttest-auc:0.88788\tval-auc:0.89315\n",
      "[10590]\ttrain-auc:0.91298\ttest-auc:0.88788\tval-auc:0.89315\n",
      "[10600]\ttrain-auc:0.91300\ttest-auc:0.88789\tval-auc:0.89317\n",
      "[10610]\ttrain-auc:0.91302\ttest-auc:0.88790\tval-auc:0.89318\n",
      "[10620]\ttrain-auc:0.91304\ttest-auc:0.88791\tval-auc:0.89317\n",
      "[10630]\ttrain-auc:0.91306\ttest-auc:0.88791\tval-auc:0.89319\n",
      "[10640]\ttrain-auc:0.91309\ttest-auc:0.88792\tval-auc:0.89319\n",
      "[10650]\ttrain-auc:0.91311\ttest-auc:0.88792\tval-auc:0.89319\n",
      "[10660]\ttrain-auc:0.91313\ttest-auc:0.88793\tval-auc:0.89320\n",
      "[10670]\ttrain-auc:0.91315\ttest-auc:0.88793\tval-auc:0.89320\n",
      "[10680]\ttrain-auc:0.91317\ttest-auc:0.88793\tval-auc:0.89320\n",
      "[10690]\ttrain-auc:0.91319\ttest-auc:0.88794\tval-auc:0.89321\n",
      "[10700]\ttrain-auc:0.91322\ttest-auc:0.88796\tval-auc:0.89322\n",
      "[10710]\ttrain-auc:0.91324\ttest-auc:0.88796\tval-auc:0.89323\n",
      "[10720]\ttrain-auc:0.91325\ttest-auc:0.88796\tval-auc:0.89323\n",
      "[10730]\ttrain-auc:0.91327\ttest-auc:0.88796\tval-auc:0.89324\n",
      "[10740]\ttrain-auc:0.91329\ttest-auc:0.88796\tval-auc:0.89324\n",
      "[10750]\ttrain-auc:0.91332\ttest-auc:0.88797\tval-auc:0.89324\n",
      "[10760]\ttrain-auc:0.91334\ttest-auc:0.88798\tval-auc:0.89325\n",
      "[10770]\ttrain-auc:0.91336\ttest-auc:0.88798\tval-auc:0.89326\n",
      "[10780]\ttrain-auc:0.91339\ttest-auc:0.88799\tval-auc:0.89326\n",
      "[10790]\ttrain-auc:0.91341\ttest-auc:0.88799\tval-auc:0.89327\n",
      "[10800]\ttrain-auc:0.91342\ttest-auc:0.88800\tval-auc:0.89328\n",
      "[10810]\ttrain-auc:0.91344\ttest-auc:0.88800\tval-auc:0.89328\n",
      "[10820]\ttrain-auc:0.91346\ttest-auc:0.88801\tval-auc:0.89329\n",
      "[10830]\ttrain-auc:0.91348\ttest-auc:0.88802\tval-auc:0.89330\n",
      "[10840]\ttrain-auc:0.91350\ttest-auc:0.88803\tval-auc:0.89331\n",
      "[10850]\ttrain-auc:0.91352\ttest-auc:0.88803\tval-auc:0.89332\n",
      "[10860]\ttrain-auc:0.91354\ttest-auc:0.88804\tval-auc:0.89331\n",
      "[10870]\ttrain-auc:0.91356\ttest-auc:0.88805\tval-auc:0.89332\n",
      "[10880]\ttrain-auc:0.91358\ttest-auc:0.88805\tval-auc:0.89333\n",
      "[10890]\ttrain-auc:0.91359\ttest-auc:0.88805\tval-auc:0.89333\n",
      "[10900]\ttrain-auc:0.91361\ttest-auc:0.88806\tval-auc:0.89334\n",
      "[10910]\ttrain-auc:0.91363\ttest-auc:0.88806\tval-auc:0.89335\n",
      "[10920]\ttrain-auc:0.91365\ttest-auc:0.88806\tval-auc:0.89335\n",
      "[10930]\ttrain-auc:0.91367\ttest-auc:0.88807\tval-auc:0.89336\n",
      "[10940]\ttrain-auc:0.91370\ttest-auc:0.88807\tval-auc:0.89337\n",
      "[10950]\ttrain-auc:0.91372\ttest-auc:0.88807\tval-auc:0.89338\n",
      "[10960]\ttrain-auc:0.91374\ttest-auc:0.88808\tval-auc:0.89337\n",
      "[10970]\ttrain-auc:0.91376\ttest-auc:0.88808\tval-auc:0.89338\n",
      "[10980]\ttrain-auc:0.91378\ttest-auc:0.88808\tval-auc:0.89338\n",
      "[10990]\ttrain-auc:0.91379\ttest-auc:0.88809\tval-auc:0.89338\n",
      "[11000]\ttrain-auc:0.91381\ttest-auc:0.88809\tval-auc:0.89339\n",
      "[11010]\ttrain-auc:0.91383\ttest-auc:0.88809\tval-auc:0.89340\n",
      "[11020]\ttrain-auc:0.91386\ttest-auc:0.88809\tval-auc:0.89341\n",
      "[11030]\ttrain-auc:0.91388\ttest-auc:0.88811\tval-auc:0.89342\n",
      "[11040]\ttrain-auc:0.91391\ttest-auc:0.88813\tval-auc:0.89343\n",
      "[11050]\ttrain-auc:0.91393\ttest-auc:0.88813\tval-auc:0.89343\n",
      "[11060]\ttrain-auc:0.91395\ttest-auc:0.88813\tval-auc:0.89343\n",
      "[11070]\ttrain-auc:0.91397\ttest-auc:0.88813\tval-auc:0.89344\n",
      "[11080]\ttrain-auc:0.91399\ttest-auc:0.88814\tval-auc:0.89344\n",
      "[11090]\ttrain-auc:0.91400\ttest-auc:0.88814\tval-auc:0.89344\n",
      "[11100]\ttrain-auc:0.91403\ttest-auc:0.88815\tval-auc:0.89344\n",
      "[11110]\ttrain-auc:0.91405\ttest-auc:0.88816\tval-auc:0.89344\n",
      "[11120]\ttrain-auc:0.91407\ttest-auc:0.88816\tval-auc:0.89344\n",
      "[11130]\ttrain-auc:0.91409\ttest-auc:0.88817\tval-auc:0.89345\n",
      "[11140]\ttrain-auc:0.91411\ttest-auc:0.88818\tval-auc:0.89345\n",
      "[11150]\ttrain-auc:0.91413\ttest-auc:0.88819\tval-auc:0.89345\n",
      "[11160]\ttrain-auc:0.91415\ttest-auc:0.88820\tval-auc:0.89347\n",
      "[11170]\ttrain-auc:0.91417\ttest-auc:0.88820\tval-auc:0.89348\n",
      "[11180]\ttrain-auc:0.91419\ttest-auc:0.88820\tval-auc:0.89348\n",
      "[11190]\ttrain-auc:0.91421\ttest-auc:0.88821\tval-auc:0.89349\n",
      "[11200]\ttrain-auc:0.91424\ttest-auc:0.88822\tval-auc:0.89349\n",
      "[11210]\ttrain-auc:0.91426\ttest-auc:0.88822\tval-auc:0.89350\n",
      "[11220]\ttrain-auc:0.91427\ttest-auc:0.88823\tval-auc:0.89351\n",
      "[11230]\ttrain-auc:0.91429\ttest-auc:0.88823\tval-auc:0.89352\n",
      "[11240]\ttrain-auc:0.91432\ttest-auc:0.88823\tval-auc:0.89353\n",
      "[11250]\ttrain-auc:0.91434\ttest-auc:0.88824\tval-auc:0.89353\n",
      "[11260]\ttrain-auc:0.91436\ttest-auc:0.88824\tval-auc:0.89353\n",
      "[11270]\ttrain-auc:0.91438\ttest-auc:0.88825\tval-auc:0.89353\n",
      "[11280]\ttrain-auc:0.91440\ttest-auc:0.88825\tval-auc:0.89354\n",
      "[11290]\ttrain-auc:0.91443\ttest-auc:0.88826\tval-auc:0.89356\n",
      "[11300]\ttrain-auc:0.91445\ttest-auc:0.88827\tval-auc:0.89357\n",
      "[11310]\ttrain-auc:0.91446\ttest-auc:0.88827\tval-auc:0.89358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11320]\ttrain-auc:0.91448\ttest-auc:0.88827\tval-auc:0.89358\n",
      "[11330]\ttrain-auc:0.91450\ttest-auc:0.88828\tval-auc:0.89359\n",
      "[11340]\ttrain-auc:0.91452\ttest-auc:0.88828\tval-auc:0.89359\n",
      "[11350]\ttrain-auc:0.91454\ttest-auc:0.88828\tval-auc:0.89360\n",
      "[11360]\ttrain-auc:0.91456\ttest-auc:0.88829\tval-auc:0.89360\n",
      "[11370]\ttrain-auc:0.91458\ttest-auc:0.88830\tval-auc:0.89361\n",
      "[11380]\ttrain-auc:0.91460\ttest-auc:0.88830\tval-auc:0.89361\n",
      "[11390]\ttrain-auc:0.91462\ttest-auc:0.88831\tval-auc:0.89362\n",
      "[11400]\ttrain-auc:0.91465\ttest-auc:0.88831\tval-auc:0.89362\n",
      "[11410]\ttrain-auc:0.91467\ttest-auc:0.88831\tval-auc:0.89363\n",
      "[11420]\ttrain-auc:0.91468\ttest-auc:0.88831\tval-auc:0.89364\n",
      "[11430]\ttrain-auc:0.91470\ttest-auc:0.88832\tval-auc:0.89364\n",
      "[11440]\ttrain-auc:0.91472\ttest-auc:0.88833\tval-auc:0.89366\n",
      "[11450]\ttrain-auc:0.91474\ttest-auc:0.88835\tval-auc:0.89366\n",
      "[11460]\ttrain-auc:0.91477\ttest-auc:0.88836\tval-auc:0.89368\n",
      "[11470]\ttrain-auc:0.91480\ttest-auc:0.88838\tval-auc:0.89369\n",
      "[11480]\ttrain-auc:0.91482\ttest-auc:0.88839\tval-auc:0.89370\n",
      "[11490]\ttrain-auc:0.91484\ttest-auc:0.88839\tval-auc:0.89370\n",
      "[11500]\ttrain-auc:0.91486\ttest-auc:0.88840\tval-auc:0.89370\n",
      "[11510]\ttrain-auc:0.91488\ttest-auc:0.88841\tval-auc:0.89371\n",
      "[11520]\ttrain-auc:0.91490\ttest-auc:0.88842\tval-auc:0.89372\n",
      "[11530]\ttrain-auc:0.91492\ttest-auc:0.88842\tval-auc:0.89372\n",
      "[11540]\ttrain-auc:0.91494\ttest-auc:0.88842\tval-auc:0.89373\n",
      "[11550]\ttrain-auc:0.91496\ttest-auc:0.88842\tval-auc:0.89373\n",
      "[11560]\ttrain-auc:0.91498\ttest-auc:0.88843\tval-auc:0.89373\n",
      "[11570]\ttrain-auc:0.91500\ttest-auc:0.88843\tval-auc:0.89373\n",
      "[11580]\ttrain-auc:0.91502\ttest-auc:0.88845\tval-auc:0.89373\n",
      "[11590]\ttrain-auc:0.91504\ttest-auc:0.88845\tval-auc:0.89373\n",
      "[11600]\ttrain-auc:0.91506\ttest-auc:0.88845\tval-auc:0.89375\n",
      "[11610]\ttrain-auc:0.91508\ttest-auc:0.88845\tval-auc:0.89375\n",
      "[11620]\ttrain-auc:0.91510\ttest-auc:0.88846\tval-auc:0.89375\n",
      "[11630]\ttrain-auc:0.91512\ttest-auc:0.88848\tval-auc:0.89377\n",
      "[11640]\ttrain-auc:0.91514\ttest-auc:0.88848\tval-auc:0.89378\n",
      "[11650]\ttrain-auc:0.91516\ttest-auc:0.88848\tval-auc:0.89379\n",
      "[11660]\ttrain-auc:0.91519\ttest-auc:0.88849\tval-auc:0.89379\n",
      "[11670]\ttrain-auc:0.91520\ttest-auc:0.88849\tval-auc:0.89380\n",
      "[11680]\ttrain-auc:0.91523\ttest-auc:0.88850\tval-auc:0.89382\n",
      "[11690]\ttrain-auc:0.91525\ttest-auc:0.88850\tval-auc:0.89383\n",
      "[11700]\ttrain-auc:0.91526\ttest-auc:0.88851\tval-auc:0.89383\n",
      "[11710]\ttrain-auc:0.91528\ttest-auc:0.88851\tval-auc:0.89383\n",
      "[11720]\ttrain-auc:0.91530\ttest-auc:0.88852\tval-auc:0.89383\n",
      "[11730]\ttrain-auc:0.91532\ttest-auc:0.88852\tval-auc:0.89383\n",
      "[11740]\ttrain-auc:0.91533\ttest-auc:0.88852\tval-auc:0.89383\n",
      "[11750]\ttrain-auc:0.91536\ttest-auc:0.88852\tval-auc:0.89384\n",
      "[11760]\ttrain-auc:0.91539\ttest-auc:0.88853\tval-auc:0.89386\n",
      "[11770]\ttrain-auc:0.91541\ttest-auc:0.88855\tval-auc:0.89386\n",
      "[11780]\ttrain-auc:0.91543\ttest-auc:0.88855\tval-auc:0.89385\n",
      "[11790]\ttrain-auc:0.91545\ttest-auc:0.88855\tval-auc:0.89385\n",
      "[11800]\ttrain-auc:0.91547\ttest-auc:0.88856\tval-auc:0.89386\n",
      "[11810]\ttrain-auc:0.91548\ttest-auc:0.88856\tval-auc:0.89386\n",
      "[11820]\ttrain-auc:0.91550\ttest-auc:0.88857\tval-auc:0.89385\n",
      "[11830]\ttrain-auc:0.91552\ttest-auc:0.88857\tval-auc:0.89386\n",
      "[11840]\ttrain-auc:0.91553\ttest-auc:0.88857\tval-auc:0.89387\n",
      "[11850]\ttrain-auc:0.91555\ttest-auc:0.88858\tval-auc:0.89388\n",
      "[11860]\ttrain-auc:0.91557\ttest-auc:0.88858\tval-auc:0.89388\n",
      "[11870]\ttrain-auc:0.91559\ttest-auc:0.88859\tval-auc:0.89389\n",
      "[11880]\ttrain-auc:0.91561\ttest-auc:0.88859\tval-auc:0.89389\n",
      "[11890]\ttrain-auc:0.91564\ttest-auc:0.88860\tval-auc:0.89390\n",
      "[11900]\ttrain-auc:0.91566\ttest-auc:0.88861\tval-auc:0.89390\n",
      "[11910]\ttrain-auc:0.91568\ttest-auc:0.88862\tval-auc:0.89390\n",
      "[11920]\ttrain-auc:0.91570\ttest-auc:0.88862\tval-auc:0.89391\n",
      "[11930]\ttrain-auc:0.91571\ttest-auc:0.88863\tval-auc:0.89392\n",
      "[11940]\ttrain-auc:0.91574\ttest-auc:0.88864\tval-auc:0.89393\n",
      "[11950]\ttrain-auc:0.91575\ttest-auc:0.88864\tval-auc:0.89394\n",
      "[11960]\ttrain-auc:0.91577\ttest-auc:0.88865\tval-auc:0.89394\n",
      "[11970]\ttrain-auc:0.91579\ttest-auc:0.88865\tval-auc:0.89395\n",
      "[11980]\ttrain-auc:0.91581\ttest-auc:0.88865\tval-auc:0.89394\n",
      "[11990]\ttrain-auc:0.91583\ttest-auc:0.88866\tval-auc:0.89395\n",
      "[12000]\ttrain-auc:0.91585\ttest-auc:0.88867\tval-auc:0.89396\n",
      "[12010]\ttrain-auc:0.91587\ttest-auc:0.88867\tval-auc:0.89397\n",
      "[12020]\ttrain-auc:0.91589\ttest-auc:0.88868\tval-auc:0.89398\n",
      "[12030]\ttrain-auc:0.91590\ttest-auc:0.88868\tval-auc:0.89398\n",
      "[12040]\ttrain-auc:0.91592\ttest-auc:0.88869\tval-auc:0.89399\n",
      "[12050]\ttrain-auc:0.91595\ttest-auc:0.88870\tval-auc:0.89399\n",
      "[12060]\ttrain-auc:0.91596\ttest-auc:0.88870\tval-auc:0.89400\n",
      "[12070]\ttrain-auc:0.91598\ttest-auc:0.88871\tval-auc:0.89400\n",
      "[12080]\ttrain-auc:0.91600\ttest-auc:0.88872\tval-auc:0.89400\n",
      "[12090]\ttrain-auc:0.91602\ttest-auc:0.88872\tval-auc:0.89401\n",
      "[12100]\ttrain-auc:0.91604\ttest-auc:0.88872\tval-auc:0.89401\n",
      "[12110]\ttrain-auc:0.91606\ttest-auc:0.88873\tval-auc:0.89402\n",
      "[12120]\ttrain-auc:0.91608\ttest-auc:0.88874\tval-auc:0.89402\n",
      "[12130]\ttrain-auc:0.91610\ttest-auc:0.88875\tval-auc:0.89403\n",
      "[12140]\ttrain-auc:0.91612\ttest-auc:0.88875\tval-auc:0.89403\n",
      "[12150]\ttrain-auc:0.91614\ttest-auc:0.88876\tval-auc:0.89404\n",
      "[12160]\ttrain-auc:0.91615\ttest-auc:0.88877\tval-auc:0.89405\n",
      "[12170]\ttrain-auc:0.91617\ttest-auc:0.88877\tval-auc:0.89405\n",
      "[12180]\ttrain-auc:0.91619\ttest-auc:0.88878\tval-auc:0.89406\n",
      "[12190]\ttrain-auc:0.91621\ttest-auc:0.88879\tval-auc:0.89406\n",
      "[12200]\ttrain-auc:0.91623\ttest-auc:0.88880\tval-auc:0.89407\n",
      "[12210]\ttrain-auc:0.91625\ttest-auc:0.88880\tval-auc:0.89409\n",
      "[12220]\ttrain-auc:0.91626\ttest-auc:0.88880\tval-auc:0.89409\n",
      "[12230]\ttrain-auc:0.91628\ttest-auc:0.88881\tval-auc:0.89409\n",
      "[12240]\ttrain-auc:0.91629\ttest-auc:0.88881\tval-auc:0.89409\n",
      "[12250]\ttrain-auc:0.91631\ttest-auc:0.88881\tval-auc:0.89410\n",
      "[12260]\ttrain-auc:0.91633\ttest-auc:0.88882\tval-auc:0.89410\n",
      "[12270]\ttrain-auc:0.91635\ttest-auc:0.88882\tval-auc:0.89411\n",
      "[12280]\ttrain-auc:0.91637\ttest-auc:0.88882\tval-auc:0.89411\n",
      "[12290]\ttrain-auc:0.91639\ttest-auc:0.88883\tval-auc:0.89411\n",
      "[12300]\ttrain-auc:0.91641\ttest-auc:0.88884\tval-auc:0.89413\n",
      "[12310]\ttrain-auc:0.91642\ttest-auc:0.88884\tval-auc:0.89414\n",
      "[12320]\ttrain-auc:0.91644\ttest-auc:0.88885\tval-auc:0.89413\n",
      "[12330]\ttrain-auc:0.91646\ttest-auc:0.88885\tval-auc:0.89414\n",
      "[12340]\ttrain-auc:0.91648\ttest-auc:0.88885\tval-auc:0.89414\n",
      "[12350]\ttrain-auc:0.91650\ttest-auc:0.88885\tval-auc:0.89414\n",
      "[12360]\ttrain-auc:0.91652\ttest-auc:0.88886\tval-auc:0.89415\n",
      "[12370]\ttrain-auc:0.91653\ttest-auc:0.88886\tval-auc:0.89416\n",
      "[12380]\ttrain-auc:0.91655\ttest-auc:0.88887\tval-auc:0.89416\n",
      "[12390]\ttrain-auc:0.91657\ttest-auc:0.88887\tval-auc:0.89417\n",
      "[12400]\ttrain-auc:0.91659\ttest-auc:0.88888\tval-auc:0.89418\n",
      "[12410]\ttrain-auc:0.91661\ttest-auc:0.88889\tval-auc:0.89419\n",
      "[12420]\ttrain-auc:0.91663\ttest-auc:0.88889\tval-auc:0.89419\n",
      "[12430]\ttrain-auc:0.91665\ttest-auc:0.88889\tval-auc:0.89419\n",
      "[12440]\ttrain-auc:0.91666\ttest-auc:0.88890\tval-auc:0.89420\n",
      "[12450]\ttrain-auc:0.91668\ttest-auc:0.88891\tval-auc:0.89421\n",
      "[12460]\ttrain-auc:0.91670\ttest-auc:0.88892\tval-auc:0.89421\n",
      "[12470]\ttrain-auc:0.91672\ttest-auc:0.88893\tval-auc:0.89422\n",
      "[12480]\ttrain-auc:0.91674\ttest-auc:0.88894\tval-auc:0.89423\n",
      "[12490]\ttrain-auc:0.91676\ttest-auc:0.88894\tval-auc:0.89423\n",
      "[12500]\ttrain-auc:0.91677\ttest-auc:0.88894\tval-auc:0.89423\n",
      "[12510]\ttrain-auc:0.91679\ttest-auc:0.88895\tval-auc:0.89423\n",
      "[12520]\ttrain-auc:0.91681\ttest-auc:0.88895\tval-auc:0.89423\n",
      "[12530]\ttrain-auc:0.91682\ttest-auc:0.88895\tval-auc:0.89423\n",
      "Stopping. Best iteration:\n",
      "[12483]\ttrain-auc:0.91674\ttest-auc:0.88894\tval-auc:0.89423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'auc'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "evallist = [(d_train, 'train'), (d_test, 'test'), (d_val, 'val')]\n",
    "\n",
    "num_iters = 50000\n",
    "\n",
    "xgb_tfidf = xgb.train(params, d_train, num_iters, evallist, early_stopping_rounds=50, verbose_eval=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tfidf.save_model('models/xgb_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training:  0.8403609904908481\n",
      "Accuracy on validation:  0.8428765767994064\n",
      "Accuracy on test:  0.8288975735239555\n",
      "AUC on train:  0.9168234628206468\n",
      "AUC on validation:  0.8942311188490448\n",
      "AUC on test:  0.8889539442559593\n",
      "Original q1:  What are 10 things you would tell your 19 year old self?  Treated q1:  what be 10 thing you would tell your 19 year old self\n",
      "Original q2:  What are some of the most important things you would tell your 19 year old self?  Treated q2:  what be some of the most important thing you would tell your 19 year old self\n",
      "true class: 1\n",
      "prediction: 0\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(new_X_tr_q1q2, label=train_labels)\n",
    "d_test = xgb.DMatrix(new_X_te_q1q2, label=test_labels)\n",
    "d_val = xgb.DMatrix(new_X_val_q1q2, label=val_labels)\n",
    "\n",
    "pred_test = xgb_tfidf.predict(d_test)\n",
    "pred_val = xgb_tfidf.predict(d_val)\n",
    "pred_train = xgb_tfidf.predict(d_train)\n",
    "\n",
    "print(\"Accuracy on training: \", np.sum(train_labels==pred_train.round(0).astype(int))/len(train_labels))\n",
    "print(\"Accuracy on validation: \", np.sum(val_labels==pred_val.round(0).astype(int))/len(val_labels))\n",
    "print(\"Accuracy on test: \", np.sum(test_labels ==pred_test.round(0).astype(int))/len(test_labels))\n",
    "\n",
    "#train roc auc metrics\n",
    "print(\"AUC on train: \", sklearn.metrics.roc_auc_score(y_true = train_labels, y_score = pred_train))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on validation: \", sklearn.metrics.roc_auc_score(y_true = val_labels, y_score = pred_val))\n",
    "\n",
    "#test roc auc metrics\n",
    "print(\"AUC on test: \", sklearn.metrics.roc_auc_score(y_true = test_labels, y_score = pred_test))\n",
    "\n",
    "\n",
    "mistake_indices, predictions = get_mistakes(xgb_tfidf, d_train, train_labels)\n",
    "print_mistake_k(4, all_questions, mistake_indices, predictions)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOURTH MODEL: DIFFERENT APPROACH WITH DEEP LEARNING\n",
    "\n",
    "Our main objective for this deliverable was to work with a more classic approach for natural language processing, mainly to implement and understand the CountVectorizer and TfIdfVectorizer. Additionally, we tried to work on the mistakes and limitations that this approach had, hence having to do a bit of feature engeenireing to tackle those problems.\n",
    "\n",
    "However, nowadays deep learning is used practically to solve anything, so, how well could it work to solve this problem? In this section we explore a completely different approach using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST BUT NOT LEAST: LET'S DO PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "import extra_features\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data from CountVectorizer to model automatically, we need to transform the data first. This is just a hacky way to do it. \n",
    "\n",
    "Note that the correct way is to modify the CountVectorized that we implemented so that the output of the  transform is already the desired matrix (i.e. doing the hstack inside). We tried to implement the most general CountVectorized that we could, so we have to resort to this way. It is in no way slower in terms of speed. It is just ugly in the sense that is doing something hack-ish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_classes import CountVectorizerTransformer, XGBModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = Pipeline([\n",
    "    ('countVectorizer', CountVectorizerTransformer(stop_words = set(stopwords.words('english')),\n",
    "                     ngram_range=(1,3), max_df = 0.99, min_df = 5)),\n",
    "    ('model', XGBModel(objective='binary:logistic', eval_metric='auc', eta=0.02, max_depth=4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countVectorizer', CountVectorizerTransformer()), ('model', XGBModel())])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.fit(all_questions, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_test = q1_test+q2_test\n",
    "all_questions_val = q1_val+q2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832060706396399"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(val_labels, model_cv.predict(all_questions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356496066563877"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.roc_auc_score(test_labels, model_cv.predict(all_questions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv.get_params()['countVectorizer'].dump(\"models/Pipeline_CountVectorizer.pkl\")\n",
    "model_cv.get_params()['model'].dump(\"models/Pipeline_XGBoost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipeline Great Again: GridSearch Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select samples to cross validate, we need the samples to be in pair with the labels. Right now we dont have that, so we are going to (once again) hack our way through it.\n",
    "\n",
    "Again note that the correct way to do it is to modify the CountVectorizer so that it automatically does this process inside the fit and transform functions. But following our filosophy of having a general CountVectorizer, this is a good way to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndocs = (int)(len(all_questions)/2)\n",
    "all_questions_tuples = [(q1_train[i], q2_train[i]) for i in range(ndocs)]\n",
    "\n",
    "class MiddleTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_tr = self.CountVectorizer.transform(X)\n",
    "        nexamples, nvars = X_tr.shape\n",
    "        split = (int)(nexamples/2)\n",
    "        XX1 = X_tr[:split,:]\n",
    "        XX2 = X_tr[split:,:]\n",
    "        return sparse.hstack([XX1,XX2], format='csr')\n",
    "\n",
    "class CrossValidationTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        XX = [x[0] for x in X]\n",
    "        for x in X:\n",
    "            XX.append(x[1])\n",
    "        return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "model_cv = Pipeline([\n",
    "    ('crossValidationTransformer', CrossValidationTransformer()),\n",
    "    ('countVectorizer', tf(#token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n",
    "                     stop_words = set(stopwords.words('english')))),\n",
    "    ('middleTransformer', MiddleTransformer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {'countVectorizer__ngram_range':[(1,3)], \n",
    "         'countVectorizer__min_df':[1,5,10,15,],\n",
    "         'countVectorizer__max_df':[.4,.3],\n",
    "         }\n",
    "\n",
    "gs = GridSearchCV(model_cv, params, scoring='roc_auc', cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed: 12.1min finished\n"
     ]
    }
   ],
   "source": [
    "results = gs.fit(all_questions_tuples, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countVectorizer__max_df': 0.4,\n",
       " 'countVectorizer__min_df': 1,\n",
       " 'countVectorizer__ngram_range': (1, 3)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436795654397367"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
