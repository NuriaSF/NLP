{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:21.485723Z",
     "start_time": "2020-03-14T14:44:21.480735Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import CountVectorizer_BagOfWords as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346692</td>\n",
       "      <td>38482</td>\n",
       "      <td>10706</td>\n",
       "      <td>Why do I get easily bored with everything?</td>\n",
       "      <td>Why do I get bored with things so quickly and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>327668</td>\n",
       "      <td>454117</td>\n",
       "      <td>345117</td>\n",
       "      <td>How do I study for Honeywell company recruitment?</td>\n",
       "      <td>How do I study for Honeywell company recruitme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272993</td>\n",
       "      <td>391373</td>\n",
       "      <td>391374</td>\n",
       "      <td>Which search engine algorithm is Quora using?</td>\n",
       "      <td>Why is Quora not using reliable search engine?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54070</td>\n",
       "      <td>82673</td>\n",
       "      <td>95496</td>\n",
       "      <td>How can I smartly cut myself?</td>\n",
       "      <td>Can someone who thinks about suicide for 7 yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46450</td>\n",
       "      <td>38384</td>\n",
       "      <td>72436</td>\n",
       "      <td>How do I see who is viewing my Instagram videos?</td>\n",
       "      <td>Can one tell who viewed my Instagram videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323427</th>\n",
       "      <td>192476</td>\n",
       "      <td>292119</td>\n",
       "      <td>292120</td>\n",
       "      <td>Is it okay to use a laptop while it is chargin...</td>\n",
       "      <td>Is it OK to use your phone while charging?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323428</th>\n",
       "      <td>17730</td>\n",
       "      <td>33641</td>\n",
       "      <td>33642</td>\n",
       "      <td>How can dogs understand human language?</td>\n",
       "      <td>Can dogs understand the human language?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323429</th>\n",
       "      <td>28030</td>\n",
       "      <td>52012</td>\n",
       "      <td>52013</td>\n",
       "      <td>What's your favourite lotion?</td>\n",
       "      <td>What's your favourite skin lotion?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323430</th>\n",
       "      <td>277869</td>\n",
       "      <td>397054</td>\n",
       "      <td>120852</td>\n",
       "      <td>How does one become a hedge fund manager?</td>\n",
       "      <td>What should I do to become a hedge fund manager?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323431</th>\n",
       "      <td>249342</td>\n",
       "      <td>362958</td>\n",
       "      <td>362959</td>\n",
       "      <td>How did the US acquire over 80 trillion financ...</td>\n",
       "      <td>We've thought about evil geniuses ruling the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323432 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "0       346692   38482   10706   \n",
       "1       327668  454117  345117   \n",
       "2       272993  391373  391374   \n",
       "3        54070   82673   95496   \n",
       "4        46450   38384   72436   \n",
       "...        ...     ...     ...   \n",
       "323427  192476  292119  292120   \n",
       "323428   17730   33641   33642   \n",
       "323429   28030   52012   52013   \n",
       "323430  277869  397054  120852   \n",
       "323431  249342  362958  362959   \n",
       "\n",
       "                                                question1  \\\n",
       "0              Why do I get easily bored with everything?   \n",
       "1       How do I study for Honeywell company recruitment?   \n",
       "2           Which search engine algorithm is Quora using?   \n",
       "3                           How can I smartly cut myself?   \n",
       "4        How do I see who is viewing my Instagram videos?   \n",
       "...                                                   ...   \n",
       "323427  Is it okay to use a laptop while it is chargin...   \n",
       "323428            How can dogs understand human language?   \n",
       "323429                      What's your favourite lotion?   \n",
       "323430          How does one become a hedge fund manager?   \n",
       "323431  How did the US acquire over 80 trillion financ...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "0       Why do I get bored with things so quickly and ...             1  \n",
       "1       How do I study for Honeywell company recruitme...             1  \n",
       "2          Why is Quora not using reliable search engine?             0  \n",
       "3       Can someone who thinks about suicide for 7 yea...             0  \n",
       "4            Can one tell who viewed my Instagram videos?             1  \n",
       "...                                                   ...           ...  \n",
       "323427         Is it OK to use your phone while charging?             0  \n",
       "323428            Can dogs understand the human language?             0  \n",
       "323429                 What's your favourite skin lotion?             1  \n",
       "323430   What should I do to become a hedge fund manager?             1  \n",
       "323431  We've thought about evil geniuses ruling the w...             0  \n",
       "\n",
       "[323432 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the data\n",
    "available_data = pd.read_csv(\"quora_train_data.csv\")\n",
    "available_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:23.479283Z",
     "start_time": "2020-03-14T14:44:23.388271Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(available_data, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the *CountVectorizer* class. It will have the following attributes and methods.\n",
    "\n",
    "ATTRIBUTES\n",
    "* **stop_words**: is a list (or set) of stop words. That is, these words will be ignored. By default, it is an empty list.\n",
    "* **ngram_range**: is the tuple giving the range of n-gram sizes to consider. By default it takes value (1,1).\n",
    "\n",
    "METHODS\n",
    "* **document_cleaner**: it defines the function to be used so as to perform the cleaning of the document. By default, such cleaning consists in lower casing the words, removing all characters after an apostrophe and removing all non alphanumeric characters.\n",
    "* **tokenizer**: defines the function to be used so as to convert the string into a list of tokens. By default, the tokens will be the sets of alphanumeric characters separated by white spaces. Notice that a token may be composed of a single character.\n",
    "* **token_cleaner**: defines the function to be used so as to perform the cleaning of the tokens (stemming, lemmatizing, doing nothing). By default, it returns the tokens as they are.\n",
    "* **fit**: it creates the vocabulary using the three above functions. It defines the attributes *self.vocabulary*, *self.n_features* and *self.word_to_ind* of the object.\n",
    "* **transform**: converts a document into a feature vector using the above methods.\n",
    "* **fit_transform**: performs the *fit* and *transform* methods in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:12.582459Z",
     "start_time": "2020-03-14T14:10:12.555530Z"
    }
   },
   "outputs": [],
   "source": [
    "class CountVectorizer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 min_word_counts=1,\n",
    "                 doc_cleaner_pattern=r\"('\\w+)|([^a-zA-Z0-9])\", #pattern for cleaning document\n",
    "                 token_pattern=r\"(?u)\\b\\w+\\b\", #pattern defining what a token is\n",
    "                 dtype=np.float32,\n",
    "                 document_cleaner_func=None,\n",
    "                 tokenizer_func=None,\n",
    "                 token_cleaner_func=None,\n",
    "                 stop_words=[],\n",
    "                 ngram_range=(1, 1)):\n",
    "        \n",
    "        self._retype = type(re.compile('hello, world'))\n",
    "\n",
    "        self.min_word_counts     = min_word_counts\n",
    "        self.doc_cleaner_pattern = doc_cleaner_pattern\n",
    "        self.token_pattern       = token_pattern #definition of what a token is\n",
    "        self.dtype               = dtype\n",
    "        \n",
    "        self.document_cleaner_func      = document_cleaner_func #function to perform the document cleaning\n",
    "        self.tokenizer_func        = tokenizer_func #function to split the document into tokens\n",
    "        self.token_cleaner_func = token_cleaner_func #function to perform the cleaning of the tokens\n",
    "        \n",
    "        self.vocabulary = set() #set containing all words in our vocabulary\n",
    "        self.word_to_ind = collections.OrderedDict() #dictionary of the vocabulary (key=word, value=integer)   \n",
    "        self.stop_words = stop_words #set of stop words\n",
    "        self.ngram_range = ngram_range\n",
    "    \n",
    "    \n",
    "    def document_cleaner(self, lower=True):        \n",
    "        \n",
    "        if self.document_cleaner_func: #inputted one\n",
    "            return self.document_cleaner_func\n",
    "        \n",
    "        else: #default \n",
    "            clean_doc_pattern = re.compile(self.doc_cleaner_pattern)\n",
    "            if lower:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc).lower()\n",
    "            else:\n",
    "                 return lambda doc: clean_doc_pattern.sub(\" \", doc)\n",
    "\n",
    "    def tokenizer(self):\n",
    "                \n",
    "        if self.tokenizer_func: #inputted one\n",
    "            return self.tokenizer_func\n",
    "        \n",
    "        else: #default\n",
    "            token_pattern_aux = re.compile(self.token_pattern)\n",
    "            return lambda doc: token_pattern_aux.findall(doc)\n",
    "\n",
    "    \n",
    "    def token_cleaner(self):\n",
    "                \n",
    "        if self.token_cleaner_func: #inputted one\n",
    "            return self.token_cleaner_func\n",
    "        else: #default\n",
    "            return lambda word: word #identity function\n",
    "        \n",
    "    \n",
    "   \n",
    "    def fit(self, X):\n",
    "\n",
    "        assert self.vocabulary == set(), \"self.vocabulary is not empty it has {} words\".format(len(self.vocabulary))\n",
    "        assert isinstance(X,list), \"X is expected to be a list of documents\"\n",
    "        \n",
    "           \n",
    "        word_to_ind = collections.OrderedDict() #vocab dictionary\n",
    "        doc_cleaner      = self.document_cleaner()\n",
    "        doc_tokenizer    = self.tokenizer()\n",
    "        word_transformer = self.token_cleaner()\n",
    "        \n",
    "        \n",
    "        for x in X: #X is the whole set of documents           \n",
    "            \n",
    "            #Create the dictionary of the words\n",
    "            x = doc_cleaner(x) #preprocess the string by cleaning it\n",
    "            tokens = doc_tokenizer(x) #creates the tokens\n",
    "            tokens_aux=[]\n",
    "            for w in tokens:\n",
    "                tokens_aux.append(word_transformer(w)) #stemming, lemmatizing or nothing\n",
    "            tokens = tokens_aux\n",
    "            \n",
    "            tokens = [tok for tok in tokens if tok not in set(self.stop_words)] #remove stopping words\n",
    "            \n",
    "            #ngrams\n",
    "            for n in np.arange(self.ngram_range[0], self.ngram_range[1]+1): \n",
    "                for token in tokens:\n",
    "                    inx = tokens.index(token)\n",
    "                    if inx+n < len(tokens):\n",
    "                        ngram = tokens[inx:inx+n]\n",
    "                        ngram = ' '.join(ngram)\n",
    "                    \n",
    "                        if ngram not in word_to_ind.keys(): #if token is not yet in the vocab dictionary, add it\n",
    "                            word_to_ind[ngram] = len(word_to_ind)\n",
    "            \n",
    "\n",
    "        self.word_to_ind =  word_to_ind     \n",
    "        self.n_features = len(word_to_ind)        \n",
    "        self.vocabulary = set(word_to_ind.keys())\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        doc_cleaner      = self.document_cleaner()\n",
    "        doc_tokenizer    = self.tokenizer()\n",
    "        word_transformer = self.token_cleaner()\n",
    "        \n",
    "        data = []\n",
    "        row = []\n",
    "        col = []\n",
    "        \n",
    "        for m, doc in enumerate(X):            \n",
    "            doc = doc_cleaner(doc)\n",
    "            tokens = doc_tokenizer(doc)\n",
    "            tokens_aux=[]\n",
    "            for w in tokens:\n",
    "                tokens_aux.append(word_transformer(w)) #stemming, lemmatizing or nothing\n",
    "            tokens = tokens_aux\n",
    "            \n",
    "            tokens = [tok for tok in tokens if tok not in set(self.stop_words)] #remove stopping words\n",
    "            \n",
    "            #ngrams\n",
    "            for n in np.arange(self.ngram_range[0], self.ngram_range[1]+1): \n",
    "                for token in tokens:\n",
    "                    inx = tokens.index(token)\n",
    "                    if inx+n < len(tokens):\n",
    "                        ngram = tokens[inx:inx+n]\n",
    "                        ngram = ' '.join(ngram)\n",
    "                    \n",
    "                        if ngram in self.word_to_ind.keys(): #if the word is not in the vocab, ignore it\n",
    "                            ngram_index = self.word_to_ind[ngram]\n",
    "                            row.append(m) #we are dealing with the m-th document\n",
    "                            col.append(ngram_index)\n",
    "                            data.append(1)\n",
    "                \n",
    "                \n",
    "        encoded_X = scipy.sparse.csr_matrix((data, (row,col)), shape=(m+1,len(self.word_to_ind)))    \n",
    "                \n",
    "        return encoded_X\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        encoded_X = self.transform(X)\n",
    "        return encoded_X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the doc_cleaner_pattern and token_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:15.753975Z",
     "start_time": "2020-03-14T14:10:15.748990Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_cleaner_pattern = r\"('\\w+)|([^a-zA-Z0-9])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:16.492004Z",
     "start_time": "2020-03-14T14:10:16.488012Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_doc_pattern = re.compile(doc_cleaner_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:19.097036Z",
     "start_time": "2020-03-14T14:10:19.091050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i  was   born  here in 1995 a  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = clean_doc_pattern.sub(\" \", \"I'll was', born'is Here in 1995 a)?\").lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:19.846030Z",
     "start_time": "2020-03-14T14:10:19.842043Z"
    }
   },
   "outputs": [],
   "source": [
    "token_pattern=r\"(?u)\\b\\w+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:20.914175Z",
     "start_time": "2020-03-14T14:10:20.909188Z"
    }
   },
   "outputs": [],
   "source": [
    "token_pattern_aux = re.compile(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:22.568749Z",
     "start_time": "2020-03-14T14:10:22.561767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'was', 'born', 'here', 'in', '1995', 'a']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxi = token_pattern_aux.findall(doc)\n",
    "auxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:31.971902Z",
     "start_time": "2020-03-14T14:44:31.662229Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "\n",
    "#Convert all elements of the documents into strings \n",
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "all_questions = q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:32.452092Z",
     "start_time": "2020-03-14T14:44:32.447068Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#How to call such stemmers and lemmatizer in the CountVectorizer object:\n",
    "#PorterStemmer(): token_cleaner_func = PorterStemmer().stem\n",
    "#LancasterStemmer(): token_cleaner_func = LancasterStemmer().stem\n",
    "#SnowballStemmer(language='english'): token_cleaner_func = SnowballStemmer(language='english').stem\n",
    "#WordNetLemmatizer(): token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:33.064627Z",
     "start_time": "2020-03-14T14:44:33.046674Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1a0aa6eb25d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m CountVectorizer = cv(token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                  ngram_range=(1,3))\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n",
    "                                 stop_words = set(stopwords.words('english')),\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:15:55.007075Z",
     "start_time": "2020-03-14T14:14:44.203394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(doc_cleaner_pattern=\"('\\\\w+)|([^a-zA-Z0-9])\",\n",
       "                document_cleaner_func=None, dtype=<class 'numpy.float32'>,\n",
       "                min_word_counts=1, ngram_range=(1, 3),\n",
       "                stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...},\n",
       "                token_cleaner_func=<function <lambda> at 0x0000029D1D657E18>,\n",
       "                token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer_func=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:06:44.269836Z",
     "start_time": "2020-02-25T13:06:44.267043Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat])\n",
    "            \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:06:49.904359Z",
     "start_time": "2020-02-25T13:06:44.271429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((291088, 3524936), (291088, 6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,CountVectorizer)\n",
    "X_tr_q1q2.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T13:07:20.515518Z",
     "start_time": "2020-02-25T13:07:01.343252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, train_df[\"is_duplicate\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541938511772036"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train roc auc metrics\n",
    "sklearn.metrics.roc_auc_score(y_true = train_df[\"is_duplicate\"].values, y_score = logistic.predict(X_tr_q1q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32344, 6), (32344, 3524936))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te_q1q2  = get_features_from_df(test_df, CountVectorizer)\n",
    "test_df.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763361391823561"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test roc auc metrics\n",
    "sklearn.metrics.roc_auc_score(y_true = test_df[\"is_duplicate\"].values, y_score = logistic.predict(X_te_q1q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
