{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:09.447952Z",
     "start_time": "2020-03-14T14:45:09.441971Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from CountVectorizer_BagOfWords import CountVectorizer as cv\n",
    "from TfIdfVectorizer import TfIdfVectorizer as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:22.900000Z",
     "start_time": "2020-03-14T14:44:22.131056Z"
    }
   },
   "outputs": [],
   "source": [
    "#read the data\n",
    "available_data = pd.read_csv(\"quora_train_data.csv\")\n",
    "available_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:23.479283Z",
     "start_time": "2020-03-14T14:44:23.388271Z"
    }
   },
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(available_data, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUX FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used in order to create the feature matrices to feed the models. They are valid for both vectorizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_list_as_strings(mylist):\n",
    "    \"\"\"\n",
    "    return a list of strings\n",
    "    \"\"\"\n",
    "    assert isinstance(mylist, list), f\"the input mylist should be a list it is {type(mylist)}\"\n",
    "    \n",
    "    mylist_aux = []\n",
    "    \n",
    "    for i in mylist:\n",
    "        mylist_aux.append(str(i))\n",
    "        \n",
    "    return mylist_aux\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def get_features_from_df(df, count_vectorizer):\n",
    "    \"\"\"\n",
    "    returns a sparse matrix containing the features build by the count vectorizer.\n",
    "    Each row should contain features from question1 and question2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #list of questions where each element of the question is of type string\n",
    "    q1_casted =  cast_list_as_strings(list(df[\"question1\"]))\n",
    "    q2_casted =  cast_list_as_strings(list(df[\"question2\"]))    \n",
    "    \n",
    "    q1_mat = count_vectorizer.transform(q1_casted)\n",
    "    q2_mat = count_vectorizer.transform(q2_casted)\n",
    "    X_q1q2 = hstack([q1_mat,q2_mat])\n",
    "            \n",
    "    return X_q1q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is necessary in order to obtain a list of documents. This is the structure we usually want, at least for the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all elements of the documents into strings \n",
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "\n",
    "all_questions = q1_train + q2_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create the *CountVectorizer* class. It will have the following attributes and methods.\n",
    "\n",
    "ATTRIBUTES\n",
    "* **stop_words**: is a list (or set) of stop words. That is, these words will be ignored. By default, it is an empty list.\n",
    "* **ngram_range**: is the tuple giving the range of n-gram sizes to consider. By default it takes value (1,1).\n",
    "\n",
    "METHODS\n",
    "* **document_cleaner**: it defines the function to be used so as to perform the cleaning of the document. By default, such cleaning consists in lower casing the words, removing all characters after an apostrophe and removing all non alphanumeric characters.\n",
    "* **tokenizer**: defines the function to be used so as to convert the string into a list of tokens. By default, the tokens will be the sets of alphanumeric characters separated by white spaces. Notice that a token may be composed of a single character.\n",
    "* **token_cleaner**: defines the function to be used so as to perform the cleaning of the tokens (stemming, lemmatizing, doing nothing). By default, it returns the tokens as they are.\n",
    "* **fit**: it creates the vocabulary using the three above functions. It defines the attributes *self.vocabulary*, *self.n_features* and *self.word_to_ind* of the object.\n",
    "* **transform**: converts a document into a feature vector using the above methods.\n",
    "* **fit_transform**: performs the *fit* and *transform* methods in a single call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the doc_cleaner_pattern and token_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:15.753975Z",
     "start_time": "2020-03-14T14:10:15.748990Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_cleaner_pattern = r\"('\\w+)|([^a-zA-Z0-9])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:16.492004Z",
     "start_time": "2020-03-14T14:10:16.488012Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_doc_pattern = re.compile(doc_cleaner_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:19.097036Z",
     "start_time": "2020-03-14T14:10:19.091050Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = clean_doc_pattern.sub(\" \", \"I'll was', born'is Here in 1995 a)?\").lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:19.846030Z",
     "start_time": "2020-03-14T14:10:19.842043Z"
    }
   },
   "outputs": [],
   "source": [
    "token_pattern=r\"(?u)\\b\\w+\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:20.914175Z",
     "start_time": "2020-03-14T14:10:20.909188Z"
    }
   },
   "outputs": [],
   "source": [
    "token_pattern_aux = re.compile(token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:10:22.568749Z",
     "start_time": "2020-03-14T14:10:22.561767Z"
    }
   },
   "outputs": [],
   "source": [
    "auxi = token_pattern_aux.findall(doc)\n",
    "auxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT COUNTVECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:44:32.452092Z",
     "start_time": "2020-03-14T14:44:32.447068Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#How to call such stemmers and lemmatizer in the CountVectorizer object:\n",
    "#PorterStemmer(): token_cleaner_func = PorterStemmer().stem\n",
    "#LancasterStemmer(): token_cleaner_func = LancasterStemmer().stem\n",
    "#SnowballStemmer(language='english'): token_cleaner_func = SnowballStemmer(language='english').stem\n",
    "#WordNetLemmatizer(): token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:45:21.407870Z",
     "start_time": "2020-03-14T14:45:21.401885Z"
    }
   },
   "outputs": [],
   "source": [
    "#inicialize the CountVectorizer and define its parameters\n",
    "CountVectorizer = cv(token_cleaner_func = lambda doc: WordNetLemmatizer().lemmatize(doc,pos=\"v\"),\n",
    "                                 stop_words = set(stopwords.words('english')),\n",
    "                                 ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:46:44.793780Z",
     "start_time": "2020-03-14T14:45:42.528916Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit the CountVectorizer\n",
    "CountVectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:50:00.712106Z",
     "start_time": "2020-03-14T14:48:57.854063Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,CountVectorizer)\n",
    "X_tr_q1q2.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:51:15.476664Z",
     "start_time": "2020-03-14T14:50:00.714100Z"
    }
   },
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, train_df[\"is_duplicate\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:51:15.726099Z",
     "start_time": "2020-03-14T14:51:15.480654Z"
    }
   },
   "outputs": [],
   "source": [
    "#train roc auc metrics\n",
    "sklearn.metrics.roc_auc_score(y_true = train_df[\"is_duplicate\"].values, y_score = logistic.predict(X_tr_q1q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:51:23.752773Z",
     "start_time": "2020-03-14T14:51:15.728094Z"
    }
   },
   "outputs": [],
   "source": [
    "X_te_q1q2  = get_features_from_df(test_df, CountVectorizer)\n",
    "test_df.shape, X_te_q1q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:51:23.778942Z",
     "start_time": "2020-03-14T14:51:23.755765Z"
    }
   },
   "outputs": [],
   "source": [
    "#test roc auc metrics\n",
    "sklearn.metrics.roc_auc_score(y_true = test_df[\"is_duplicate\"].values, y_score = logistic.predict(X_te_q1q2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT TFIDF VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = tf()\n",
    "tfidf_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,tfidf_vectorizer)\n",
    "X_tr_q1q2.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "logistic.fit(X_tr_q1q2, train_df[\"is_duplicate\"].values)\n",
    "\n",
    "sklearn.metrics.roc_auc_score(y_true = train_df[\"is_duplicate\"].values, y_score = logistic.predict(X_tr_q1q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_q1q2  = get_features_from_df(test_df, tfidf_vectorizer)\n",
    "test_df.shape, X_te_q1q2.shape\n",
    "\n",
    "#test roc auc metrics\n",
    "sklearn.metrics.roc_auc_score(y_true = test_df[\"is_duplicate\"].values, y_score = logistic.predict(X_te_q1q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
